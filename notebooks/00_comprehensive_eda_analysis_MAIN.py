"""æ«»äº•å„ªè¡£ åŒ…æ‹¬çš„EDAåˆ†æï¼ˆYData Profiling + AutoViz + è©³ç´°Pandasåˆ†æï¼‰

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
1. YData Profilingã«ã‚ˆã‚‹åŒ…æ‹¬çš„HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
2. AutoVizã«ã‚ˆã‚‹è‡ªå‹•å¯è¦–åŒ–
3. è©³ç´°ãªPandasåˆ†æï¼ˆcrosstabã€pivot_tableã€groupbyç­‰ï¼‰
4. å…¨çµæœã‚’reports/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ç’°å¢ƒè¨­å®š
if "GOOGLE_APPLICATION_CREDENTIALS" in os.environ:
    gac_path = os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
    if not os.path.exists(gac_path):
        del os.environ["GOOGLE_APPLICATION_CREDENTIALS"]

root_dir = Path(__file__).parent.parent
if str(root_dir / "src") not in sys.path:
    sys.path.insert(0, str(root_dir / "src"))

from ai_data_lab.connectors.bigquery import BigQueryConnector

# BigQuery è¨­å®š
PROJECT_ID = "yoake-dev-analysis"
DATASET_ID = "dev_yoake_posts"
TABLE_ID = "æ«»äº•å„ªè¡£"

# ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
reports_dir = root_dir / "reports"
reports_dir.mkdir(exist_ok=True)

print("=" * 80)
print("ğŸ€ æ«»äº•å„ªè¡£ åŒ…æ‹¬çš„EDAåˆ†æå®Ÿè¡Œ")
print("=" * 80)
print()

# ================================================================================
# 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# ================================================================================
print("ğŸ“¥ 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
connector = BigQueryConnector(project_id=PROJECT_ID)

base_query = f"""
WITH deduplicated AS (
  SELECT
    keyword,
    talentId,
    post.xPostId as post_id,
    TIMESTAMP_SECONDS(post.xPostCreatedAt) as created_at,
    post.xPostUrl as post_url,
    post.xPostContent as content,
    post.xPostQuotedCount as quoted_count,
    post.xPostRepostedCount as repost_count,
    post.xPostRepliedCount as reply_count,
    post.xPostLikedCount as like_count,
    ARRAY_LENGTH(post.xPostMediaList) as media_count,
    user.xPostUserId as user_id,
    user.xPostUserName as user_name,
    user.xPostUserBadge as user_badge,
    user.xProfileImageUrl as user_profile_image,
    ROW_NUMBER() OVER (PARTITION BY post.xPostId ORDER BY _PARTITIONTIME DESC) as row_num
  FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
  WHERE _PARTITIONTIME IS NOT NULL
)
SELECT
    keyword,
    talentId,
    post_id,
    created_at,
    post_url,
    content,
    quoted_count,
    repost_count,
    reply_count,
    like_count,
    media_count,
    user_id,
    user_name,
    user_badge,
    user_profile_image
FROM deduplicated
WHERE row_num = 1
"""

df = connector.query(base_query)
print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ Ã— {len(df.columns)} åˆ—\n")

# ãƒ‡ãƒ¼ã‚¿åŠ å·¥
print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿åŠ å·¥ä¸­...")
df['created_at'] = pd.to_datetime(df['created_at'])
df['date'] = df['created_at'].dt.date
df['hour'] = df['created_at'].dt.hour
df['day_of_week'] = df['created_at'].dt.dayofweek
df['weekday_name'] = df['created_at'].dt.day_name()
df['has_media'] = df['media_count'] > 0
df['total_engagement'] = df['like_count'] + df['repost_count'] + df['reply_count'] + df['quoted_count']
df['content_length'] = df['content'].fillna('').str.len()
df['has_url'] = df['content'].fillna('').str.contains('http')
df['engagement_per_char'] = df['total_engagement'] / (df['content_length'] + 1)  # ã‚¼ãƒ­é™¤ç®—å›é¿
print("âœ… ãƒ‡ãƒ¼ã‚¿åŠ å·¥å®Œäº†\n")

# ================================================================================
# 2. YData Profiling ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ================================================================================
print("=" * 80)
print("ğŸ“Š 2. YData Profiling ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
print("=" * 80)

try:
    from ydata_profiling import ProfileReport
    
    # è»½é‡åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
    profile = ProfileReport(
        df,
        title="æ«»äº•å„ªè¡£ æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ",
        minimal=False,
        explorative=True,
        progress_bar=True
    )
    
    output_path = reports_dir / "æ«»äº•å„ªè¡£_ydata_profiling_report.html"
    profile.to_file(output_path)
    print(f"âœ… YData Profiling ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {output_path}")
    print(f"   ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã: open {output_path}")
    print()
except Exception as e:
    print(f"âš ï¸ YData Profiling ã‚¨ãƒ©ãƒ¼: {e}")
    print("   ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®åˆ†æã«é€²ã¿ã¾ã™...\n")

# ================================================================================
# 3. è©³ç´°Pandasåˆ†æ
# ================================================================================
print("=" * 80)
print("ğŸ“ˆ 3. è©³ç´°Pandasåˆ†æå®Ÿè¡Œä¸­...")
print("=" * 80)

# 3.1 æ›œæ—¥Ã—æ™‚é–“å¸¯ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ
print("\nğŸ“… 3.1 æ›œæ—¥ Ã— æ™‚é–“å¸¯ ã‚¯ãƒ­ã‚¹é›†è¨ˆ")
weekday_hour_crosstab = pd.crosstab(df['weekday_name'], df['hour'], margins=True)
print(weekday_hour_crosstab)
weekday_hour_crosstab.to_csv(reports_dir / "crosstab_weekday_hour.csv")
print(f"âœ… ä¿å­˜: crosstab_weekday_hour.csv")

# 3.2 ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡Ã—ãƒãƒƒã‚¸æœ‰ç„¡ã®ã‚¯ãƒ­ã‚¹é›†è¨ˆ
print("\nğŸ¬ 3.2 ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡ Ã— ãƒãƒƒã‚¸æœ‰ç„¡ ã‚¯ãƒ­ã‚¹é›†è¨ˆ")
media_badge_crosstab = pd.crosstab(
    df['has_media'], 
    df['user_badge'], 
    values=df['total_engagement'], 
    aggfunc='mean',
    margins=True
)
print(media_badge_crosstab)
media_badge_crosstab.to_csv(reports_dir / "crosstab_media_badge_engagement.csv")
print(f"âœ… ä¿å­˜: crosstab_media_badge_engagement.csv")

# 3.3 æ—¥åˆ¥Ã—ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡ã®ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
print("\nğŸ“Š 3.3 æ—¥åˆ¥ Ã— ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡ ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆæŠ•ç¨¿æ•°ãƒ»å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆï¼‰")
daily_media_pivot = df.pivot_table(
    index='date',
    columns='has_media',
    values=['post_id', 'total_engagement'],
    aggfunc={'post_id': 'count', 'total_engagement': 'mean'}
)
print(daily_media_pivot.head(10))
daily_media_pivot.to_csv(reports_dir / "pivot_daily_media.csv")
print(f"âœ… ä¿å­˜: pivot_daily_media.csv")

# 3.4 æ™‚é–“å¸¯åˆ¥ã®è©³ç´°çµ±è¨ˆ
print("\nğŸ• 3.4 æ™‚é–“å¸¯åˆ¥è©³ç´°çµ±è¨ˆ")
hourly_stats = df.groupby('hour').agg({
    'post_id': 'count',
    'like_count': ['mean', 'median', 'std', 'max'],
    'repost_count': ['mean', 'median', 'std', 'max'],
    'total_engagement': ['mean', 'median', 'std', 'max'],
    'content_length': ['mean', 'median'],
    'has_media': 'mean'
}).round(2)
hourly_stats.columns = ['_'.join(col).strip() for col in hourly_stats.columns.values]
print(hourly_stats)
hourly_stats.to_csv(reports_dir / "stats_hourly_detailed.csv")
print(f"âœ… ä¿å­˜: stats_hourly_detailed.csv")

# 3.5 æ›œæ—¥åˆ¥ã®è©³ç´°çµ±è¨ˆ
print("\nğŸ“† 3.5 æ›œæ—¥åˆ¥è©³ç´°çµ±è¨ˆ")
weekday_stats = df.groupby('weekday_name').agg({
    'post_id': 'count',
    'like_count': ['mean', 'median', 'std', 'max'],
    'repost_count': ['mean', 'median', 'std', 'max'],
    'total_engagement': ['mean', 'median', 'std', 'max'],
    'has_media': 'mean'
}).round(2)
weekday_stats.columns = ['_'.join(col).strip() for col in weekday_stats.columns.values]
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
weekday_stats = weekday_stats.reindex(weekday_order)
print(weekday_stats)
weekday_stats.to_csv(reports_dir / "stats_weekday_detailed.csv")
print(f"âœ… ä¿å­˜: stats_weekday_detailed.csv")

# 3.6 ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒã‚¸æœ‰ç„¡åˆ¥ã®è©³ç´°æ¯”è¼ƒ
print("\nğŸ‘¥ 3.6 ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒãƒƒã‚¸æœ‰ç„¡åˆ¥è©³ç´°æ¯”è¼ƒ")
badge_comparison = df.groupby('user_badge').agg({
    'post_id': 'count',
    'user_id': 'nunique',
    'like_count': ['mean', 'median', 'std', 'max'],
    'repost_count': ['mean', 'median', 'std', 'max'],
    'reply_count': ['mean', 'median', 'std', 'max'],
    'quoted_count': ['mean', 'median', 'std', 'max'],
    'total_engagement': ['mean', 'median', 'std', 'max'],
    'content_length': ['mean', 'median'],
    'has_media': 'mean'
}).round(2)
badge_comparison.columns = ['_'.join(col).strip() for col in badge_comparison.columns.values]
print(badge_comparison)
badge_comparison.to_csv(reports_dir / "stats_badge_comparison.csv")
print(f"âœ… ä¿å­˜: stats_badge_comparison.csv")

# 3.7 ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå››åˆ†ä½æ•°åˆ†æ
print("\nğŸ’¬ 3.7 ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå››åˆ†ä½æ•°åˆ†æ")
try:
    # é‡è¤‡å€¤ãŒå¤šã„å ´åˆã¯è‡ªå‹•ã§ãƒ“ãƒ³æ•°ã‚’èª¿æ•´
    df['engagement_quartile'] = pd.qcut(df['total_engagement'], q=4, labels=False, duplicates='drop')
    # ã‚«ãƒ†ã‚´ãƒªåã‚’è¿½åŠ 
    df['engagement_quartile'] = df['engagement_quartile'].map({0: 'Q1(ä½)', 1: 'Q2(ä¸­ä¸‹)', 2: 'Q3(ä¸­ä¸Š)', 3: 'Q4(é«˜)'})
    
    quartile_analysis = df.groupby('engagement_quartile').agg({
        'post_id': 'count',
        'like_count': ['min', 'mean', 'max'],
        'repost_count': ['min', 'mean', 'max'],
        'total_engagement': ['min', 'mean', 'max'],
        'has_media': 'mean',
        'content_length': 'mean',
        'user_badge': lambda x: x.sum()
    }).round(2)
    quartile_analysis.columns = ['_'.join(col).strip() for col in quartile_analysis.columns.values]
    print(quartile_analysis)
    quartile_analysis.to_csv(reports_dir / "stats_engagement_quartiles.csv")
    print("âœ… ä¿å­˜: stats_engagement_quartiles.csv")
except Exception as e:
    print(f"âš ï¸ å››åˆ†ä½æ•°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    print("   å¤šæ•°ã®ã‚¼ãƒ­å€¤ã®ãŸã‚ã€ä»£æ›¿åˆ†æã‚’å®Ÿæ–½...")
    # ä»£æ›¿: ã‚¼ãƒ­ã¨éã‚¼ãƒ­ã§åˆ†å‰²
    df['engagement_category'] = df['total_engagement'].apply(lambda x: 'ã‚¼ãƒ­' if x == 0 else ('ä½(1-10)' if x <= 10 else ('ä¸­(11-100)' if x <= 100 else 'é«˜(100+)')))
    alt_analysis = df.groupby('engagement_category').agg({
        'post_id': 'count',
        'like_count': ['min', 'mean', 'max'],
        'total_engagement': ['min', 'mean', 'max'],
        'has_media': 'mean'
    }).round(2)
    alt_analysis.columns = ['_'.join(col).strip() for col in alt_analysis.columns.values]
    print(alt_analysis)
    alt_analysis.to_csv(reports_dir / "stats_engagement_categories.csv")
    print("âœ… ä¿å­˜: stats_engagement_categories.csv")

# 3.8 ãƒ¡ãƒ‡ã‚£ã‚¢æ•°åˆ¥åˆ†æ
print("\nğŸ¬ 3.8 ãƒ¡ãƒ‡ã‚£ã‚¢æ•°åˆ¥åˆ†æ")
media_count_analysis = df.groupby('media_count').agg({
    'post_id': 'count',
    'like_count': ['mean', 'median'],
    'repost_count': ['mean', 'median'],
    'total_engagement': ['mean', 'median']
}).round(2)
media_count_analysis.columns = ['_'.join(col).strip() for col in media_count_analysis.columns.values]
print(media_count_analysis)
media_count_analysis.to_csv(reports_dir / "stats_media_count.csv")
print(f"âœ… ä¿å­˜: stats_media_count.csv")

# 3.9 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·åˆ¥åˆ†æï¼ˆãƒ“ãƒ³åˆ†å‰²ï¼‰
print("\nğŸ“ 3.9 ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·åˆ¥åˆ†æ")
df['content_length_bin'] = pd.cut(df['content_length'], bins=[0, 50, 100, 150, 200, 300], labels=['çŸ­(0-50)', 'ä¸­(51-100)', 'é•·(101-150)', 'è¶…é•·(151-200)', 'æ¥µé•·(200+)'])
content_length_analysis = df.groupby('content_length_bin').agg({
    'post_id': 'count',
    'total_engagement': ['mean', 'median'],
    'has_media': 'mean',
    'like_count': 'mean'
}).round(2)
content_length_analysis.columns = ['_'.join(col).strip() for col in content_length_analysis.columns.values]
print(content_length_analysis)
content_length_analysis.to_csv(reports_dir / "stats_content_length.csv")
print(f"âœ… ä¿å­˜: stats_content_length.csv")

# 3.10 ãƒˆãƒƒãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼è©³ç´°åˆ†æ
print("\nğŸ† 3.10 ãƒˆãƒƒãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼è©³ç´°åˆ†æï¼ˆTOP 20ï¼‰")
top_users_analysis = df.groupby(['user_id', 'user_name', 'user_badge']).agg({
    'post_id': 'count',
    'like_count': ['sum', 'mean', 'median', 'max'],
    'repost_count': ['sum', 'mean', 'max'],
    'total_engagement': ['sum', 'mean', 'max'],
    'has_media': 'mean',
    'content_length': 'mean'
}).round(2)
top_users_analysis.columns = ['_'.join(col).strip() for col in top_users_analysis.columns.values]
top_users_analysis = top_users_analysis.sort_values('post_id_count', ascending=False).head(20)
print(top_users_analysis)
top_users_analysis.to_csv(reports_dir / "stats_top_users_detailed.csv")
print(f"âœ… ä¿å­˜: stats_top_users_detailed.csv")

# 3.11 ç›¸é–¢è¡Œåˆ—ï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ï¼‰
print("\nğŸ”— 3.11 ç›¸é–¢è¡Œåˆ—")
numeric_cols = ['like_count', 'repost_count', 'reply_count', 'quoted_count', 'total_engagement', 'content_length', 'media_count']
correlation_matrix = df[numeric_cols].corr().round(3)
print(correlation_matrix)
correlation_matrix.to_csv(reports_dir / "correlation_matrix.csv")
print(f"âœ… ä¿å­˜: correlation_matrix.csv")

# 3.12 æ—¥åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨ç§»
print("\nğŸ“ˆ 3.12 æ—¥åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨ç§»")
daily_engagement = df.groupby('date').agg({
    'post_id': 'count',
    'like_count': ['sum', 'mean'],
    'repost_count': ['sum', 'mean'],
    'total_engagement': ['sum', 'mean'],
    'has_media': 'mean'
}).round(2)
daily_engagement.columns = ['_'.join(col).strip() for col in daily_engagement.columns.values]
print(daily_engagement.head(10))
print("...")
print(daily_engagement.tail(10))
daily_engagement.to_csv(reports_dir / "timeseries_daily_engagement.csv")
print(f"âœ… ä¿å­˜: timeseries_daily_engagement.csv")

print("\nâœ… è©³ç´°Pandasåˆ†æå®Œäº†ï¼\n")

# ================================================================================
# 4. AutoViz è‡ªå‹•å¯è¦–åŒ–
# ================================================================================
print("=" * 80)
print("ğŸ¨ 4. AutoViz è‡ªå‹•å¯è¦–åŒ–å®Ÿè¡Œä¸­...")
print("=" * 80)

try:
    from autoviz.AutoViz_Class import AutoViz_Class
    
    # AutoVizã¯æ•°å€¤ãƒ»ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•èªè­˜ã—ã¦å¯è¦–åŒ–
    # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    sample_size = min(5000, len(df))
    df_sample = df.sample(n=sample_size, random_state=42) if len(df) > sample_size else df
    
    # AutoVizç”¨ã«ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆdatetimeã‚’strã«å¤‰æ›ï¼‰
    df_autoviz = df_sample.copy()
    df_autoviz['created_at'] = df_autoviz['created_at'].astype(str)
    df_autoviz['date'] = df_autoviz['date'].astype(str)
    
    # ä¸€æ™‚CSVã«ä¿å­˜ï¼ˆAutoVizã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å¿…è¦ï¼‰
    temp_csv = reports_dir / "temp_autoviz_data.csv"
    df_autoviz.to_csv(temp_csv, index=False)
    
    AV = AutoViz_Class()
    
    # AutoVizå®Ÿè¡Œï¼ˆHTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼‰
    autoviz_output = reports_dir / "autoviz_visualizations"
    autoviz_output.mkdir(exist_ok=True)
    
    print(f"   ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {len(df_autoviz):,} è¡Œ")
    print(f"   å‡ºåŠ›å…ˆ: {autoviz_output}")
    
    dft = AV.AutoViz(
        filename=str(temp_csv),
        sep=',',
        depVar='total_engagement',
        dfte=None,
        header=0,
        verbose=1,
        lowess=False,
        chart_format='html',
        max_rows_analyzed=5000,
        max_cols_analyzed=30,
        save_plot_dir=str(autoviz_output)
    )
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    temp_csv.unlink()
    
    print(f"âœ… AutoViz å¯è¦–åŒ–å®Œäº†: {autoviz_output}")
    print(f"   HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèªã—ã¦ãã ã•ã„")
    print()
except Exception as e:
    print(f"âš ï¸ AutoViz ã‚¨ãƒ©ãƒ¼: {e}")
    print("   ã‚¹ã‚­ãƒƒãƒ—ã—ã¦æ¬¡ã®åˆ†æã«é€²ã¿ã¾ã™...\n")

# ================================================================================
# 5. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# ================================================================================
print("=" * 80)
print("ğŸ“„ 5. ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
print("=" * 80)

# ã‚¼ãƒ­é™¤ç®—ã‚¬ãƒ¼ãƒ‰ä»˜ãã§ãƒ¡ãƒ‡ã‚£ã‚¢å€ç‡ã¨ãƒãƒƒã‚¸å½±éŸ¿åŠ›ã‚’è¨ˆç®—
media_with_eng = df[df['has_media']]['total_engagement'].mean()
media_without_eng = df[~df['has_media']]['total_engagement'].mean()
media_ratio = media_with_eng / media_without_eng if media_without_eng != 0 else float('nan')

badge_with_eng = df[df['user_badge']]['total_engagement'].mean()
badge_without_eng = df[~df['user_badge']]['total_engagement'].mean()
badge_ratio = badge_with_eng / badge_without_eng if badge_without_eng != 0 else float('nan')

summary_report = f"""
# æ«»äº•å„ªè¡£ åŒ…æ‹¬çš„EDAåˆ†æã‚µãƒãƒªãƒ¼

**å®Ÿæ–½æ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ãƒ‡ãƒ¼ã‚¿ä»¶æ•°**: {len(df):,} ä»¶

## ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

### 1. YData Profiling ãƒ¬ãƒãƒ¼ãƒˆ
- `æ«»äº•å„ªè¡£_ydata_profiling_report.html` - åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### 2. AutoViz å¯è¦–åŒ–
- `autoviz_visualizations/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª - è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸå¯è¦–åŒ–HTML

### 3. è©³ç´°Pandasåˆ†æCSV

#### ã‚¯ãƒ­ã‚¹é›†è¨ˆ
- `crosstab_weekday_hour.csv` - æ›œæ—¥Ã—æ™‚é–“å¸¯ã®æŠ•ç¨¿æ•°
- `crosstab_media_badge_engagement.csv` - ãƒ¡ãƒ‡ã‚£ã‚¢Ã—ãƒãƒƒã‚¸åˆ¥å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ

#### ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
- `pivot_daily_media.csv` - æ—¥åˆ¥Ã—ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡ã®æŠ•ç¨¿æ•°ãƒ»ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ

#### çµ±è¨ˆåˆ†æ
- `stats_hourly_detailed.csv` - æ™‚é–“å¸¯åˆ¥è©³ç´°çµ±è¨ˆ
- `stats_weekday_detailed.csv` - æ›œæ—¥åˆ¥è©³ç´°çµ±è¨ˆ
- `stats_badge_comparison.csv` - ãƒãƒƒã‚¸æœ‰ç„¡åˆ¥æ¯”è¼ƒ
- `stats_engagement_quartiles.csv` - ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå››åˆ†ä½æ•°åˆ†æ
- `stats_media_count.csv` - ãƒ¡ãƒ‡ã‚£ã‚¢æ•°åˆ¥åˆ†æ
- `stats_content_length.csv` - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·åˆ¥åˆ†æ
- `stats_top_users_detailed.csv` - ãƒˆãƒƒãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼è©³ç´°ï¼ˆTOP 20ï¼‰

#### ç›¸é–¢ãƒ»æ™‚ç³»åˆ—
- `correlation_matrix.csv` - ç›¸é–¢è¡Œåˆ—
- `timeseries_daily_engagement.csv` - æ—¥åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæ¨ç§»

## ä¸»è¦ç™ºè¦‹ï¼ˆå†æ²ï¼‰

1. **ç·æŠ•ç¨¿æ•°**: {len(df):,} ä»¶ï¼ˆé‡è¤‡é™¤å»æ¸ˆã¿ï¼‰
2. **ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼**: {df['user_id'].nunique():,} äºº
3. **å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ**: {df['total_engagement'].mean():.2f}
4. **ãƒ¡ãƒ‡ã‚£ã‚¢å€ç‡**: {media_ratio:.2f}x
5. **ãƒãƒƒã‚¸ãƒ¦ãƒ¼ã‚¶ãƒ¼å½±éŸ¿åŠ›**: {badge_ratio:.2f}x

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. YData Profiling HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ã„ã¦ã€å…¨ä½“åƒã‚’æŠŠæ¡
2. AutoVizå¯è¦–åŒ–ã§ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦–è¦šçš„ã«ç¢ºèª
3. CSVåˆ†æçµæœã‚’Excelã‚„Pandasã§æ·±å €ã‚Š
4. ç‰¹å®šã®ä»®èª¬ï¼ˆä¾‹ï¼šæœ¨æ›œæ—¥ã®å½±éŸ¿ã€ãƒ¡ãƒ‡ã‚£ã‚¢ã‚¿ã‚¤ãƒ—åˆ¥åŠ¹æœï¼‰ã‚’è¿½åŠ æ¤œè¨¼

---
"""

summary_path = reports_dir / "åŒ…æ‹¬çš„EDAåˆ†æ_ã‚µãƒãƒªãƒ¼.md"
with open(summary_path, 'w', encoding='utf-8') as f:
    f.write(summary_report)

print(f"âœ… ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {summary_path}")
print()

print("=" * 80)
print("âœ… åŒ…æ‹¬çš„EDAåˆ†æå®Œäº†ï¼")
print("=" * 80)
print()
print("ğŸ“‚ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"   1. YData Profiling: {reports_dir}/æ«»äº•å„ªè¡£_ydata_profiling_report.html")
print(f"   2. AutoViz: {reports_dir}/autoviz_visualizations/")
print(f"   3. è©³ç´°CSV: {reports_dir}/*.csv (12ãƒ•ã‚¡ã‚¤ãƒ«)")
print(f"   4. ã‚µãƒãƒªãƒ¼: {reports_dir}/åŒ…æ‹¬çš„EDAåˆ†æ_ã‚µãƒãƒªãƒ¼.md")
print()
print("ğŸ‰ å…¨ã¦ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")


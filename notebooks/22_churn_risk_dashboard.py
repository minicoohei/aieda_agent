import marimo

__generated_with = "0.17.8"
app = marimo.App(width="full")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _(mo):
    mo.md("""
    # Churn Risk Dashboard with Intent

    ãƒãƒ£ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ã€GAåˆ©ç”¨çŠ¶æ³ã€ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼ˆç«¶åˆå«ã‚€ï¼‰ã‚’çµ±åˆã—ãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
    LLMã§å„ä¼šç¤¾ã®å±é™ºåº¦ã‚’åˆ¤å®šã—ã¾ã™ã€‚

    ## ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    - **ãƒãƒ£ãƒ¼ãƒ³æƒ…å ±**: `infobox_data.tsv`
    - **GAåˆ©ç”¨ãƒ­ã‚°**: BigQuery `analytics_400693944`
    - **ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆ**: BigQuery `gree-dionysus-infobox.production_infobox`
    - **çµ„ç¹”ãƒã‚¹ã‚¿**: Snowflake `USERORGANIZATION`, `BEEGLECOMPANY`

    ## ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å®šç¾©
    - å‚ç…§: `docs/semantic_questions_status.md`
    - ERå›³: `docs/snowflake_elt_entity.puml`
    """)
    return


@app.cell
def _():
    import json
    import os
    import re
    import sys
    import tomllib
    from datetime import date, timedelta
    from pathlib import Path

    import numpy as np
    import pandas as pd
    from dotenv import load_dotenv

    # Snowflake
    snowflake = None
    default_backend = None
    serialization = None
    snowflake_error = None
    try:
        import snowflake.connector
        from cryptography.hazmat.backends import default_backend
        from cryptography.hazmat.primitives import serialization
        import snowflake as snowflake
    except Exception as exc:
        snowflake_error = exc

    # BigQuery
    bq_error = None
    bigquery = None
    service_account = None
    try:
        from google.cloud import bigquery
        from google.oauth2 import service_account
    except Exception as exc:
        bq_error = exc

    # Gemini
    genai = None
    genai_types = None
    genai_error = None
    try:
        from google import genai
        from google.genai import types as genai_types
    except Exception as exc:
        genai_error = exc

    # .envèª­ã¿è¾¼ã¿
    possible_env_paths = [
        Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/.env"),
        Path(__file__).parent.parent / ".env",
        Path.cwd() / ".env",
        Path.cwd().parent / ".env",
    ]
    loaded_env_path = None
    for env_path in possible_env_paths:
        if env_path.exists():
            load_dotenv(env_path, override=True)
            loaded_env_path = str(env_path)
            break

    # APIã‚­ãƒ¼èª­ã¿è¾¼ã¿ç¢ºèª
    gemini_api_key_loaded = os.getenv("GEMINI_API_KEY")
    gemini_key_status = f"è¨­å®šæ¸ˆã¿ ({gemini_api_key_loaded[:8]}...)" if gemini_api_key_loaded else "æœªè¨­å®š"

    root_dir = Path("/Users/kou1904/githubactions_fordata/work/aieda_agent")
    if str(root_dir / "src") not in sys.path:
        sys.path.insert(0, str(root_dir / "src"))
    return (
        Path,
        bigquery,
        bq_error,
        default_backend,
        genai,
        genai_error,
        genai_types,
        gemini_key_status,
        json,
        loaded_env_path,
        np,
        os,
        pd,
        re,
        serialization,
        service_account,
        snowflake,
        snowflake_error,
        tomllib,
    )


@app.cell
def _(bq_error, genai_error, gemini_key_status, loaded_env_path, mo, snowflake_error):
    errors = []
    if snowflake_error:
        errors.append(f"Snowflake: `{snowflake_error}`")
    if bq_error:
        errors.append(f"BigQuery: `{bq_error}`")
    if genai_error:
        errors.append(f"Gemini: `{genai_error}`")
    
    status_lines = [
        f"- **Gemini API Key**: {gemini_key_status}",
        f"- **.envèª­ã¿è¾¼ã¿**: {loaded_env_path or 'æœªèª­ã¿è¾¼ã¿'}",
    ]
    
    output = "## ğŸ”§ åˆæœŸåŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n\n"
    if errors:
        output += "**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**:\n" + "\n".join([f"- {e}" for e in errors]) + "\n\n"
    output += "**è¨­å®šçŠ¶æ³**:\n" + "\n".join(status_lines)
    
    mo.md(output)
    return


@app.cell
def _(re):
    def normalize_company_name(name: str | None) -> str | None:
        if name is None:
            return None
        value = str(name)
        if not value.strip():
            return None
        value = re.sub(r"\s+", "", value)
        value = re.sub(
            r"(æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆåŒä¼šç¤¾|ä¸€èˆ¬ç¤¾å›£æ³•äºº|ä¸€èˆ¬è²¡å›£æ³•äºº|å…¬ç›Šç¤¾å›£æ³•äºº|å…¬ç›Šè²¡å›£æ³•äºº)",
            "",
            value,
        )
        value = re.sub(r"[ï¼ˆï¼‰()]", "", value)
        value = value.replace("æ§˜", "")
        return value or None

    def extract_company_name_from_deal(deal_name: str | None) -> str | None:
        if deal_name is None:
            return None
        raw = str(deal_name)
        if not raw.strip():
            return None
        patterns = [
            r"(?P<name>.+?)æ§˜å‘ã‘",
            r"(?P<name>.+?)å‘ã‘",
            r"(?P<name>.+?)æ§˜",
        ]
        for pattern in patterns:
            match = re.search(pattern, raw)
            if match:
                return match.group("name")
        return raw

    def extract_json_block(text: str) -> str:
        if not text:
            return ""
        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            return ""
        return text[start : end + 1]
    return (
        extract_company_name_from_deal,
        extract_json_block,
        normalize_company_name,
    )


@app.cell
def _(Path, bigquery, os, pd, service_account):
    # BigQueryæ¥ç¶šï¼ˆgree-dionysus-infoboxï¼‰
    BQ_PROJECT_ID = "gree-dionysus-infobox"
    BQ_DATASET_INTENT = "production_infobox"
    GA_DATASET_ID = "analytics_400693944"

    def get_bq_client():
        key_path = os.path.expanduser("~/.gcp/gree-dionysus-infobox.json")
        if Path(key_path).exists():
            credentials = service_account.Credentials.from_service_account_file(key_path)
            return bigquery.Client(project=BQ_PROJECT_ID, credentials=credentials)
        return bigquery.Client(project=BQ_PROJECT_ID)

    def query_bq(sql):
        """BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        client = get_bq_client()
        job = client.query(sql)
        results = job.result()
        rows = [dict(row) for row in results]
        return pd.DataFrame(rows) if rows else pd.DataFrame()
    return GA_DATASET_ID, query_bq


@app.cell
def _(
    Path,
    default_backend,
    os,
    serialization,
    snowflake,
    snowflake_error,
    tomllib,
):
    SF_SCHEMA = "ETL_S3_TRANSALES_DB.TRANSALES_DAILY_SCHEMA"

    def get_snowflake_connection():
        if snowflake is None:
            raise RuntimeError(f"Snowflake connector not available: {snowflake_error}")

        config_path = Path.home() / ".snowflake" / "connections.toml"
        config = {}
        if config_path.exists():
            with open(config_path, "rb") as f:
                config = tomllib.load(f).get("default", {})

        account = os.getenv("SNOWFLAKE_ACCOUNT") or config.get("account")
        user = os.getenv("SNOWFLAKE_USER") or config.get("user")
        password = os.getenv("SNOWFLAKE_PASSWORD") or config.get("password")
        private_key_path = os.getenv("SNOWFLAKE_PRIVATE_KEY_PATH") or config.get("private_key_path")
        authenticator = os.getenv("SNOWFLAKE_AUTHENTICATOR") or config.get("authenticator")
        role = os.getenv("SNOWFLAKE_ROLE") or config.get("role")
        warehouse = os.getenv("SNOWFLAKE_WAREHOUSE") or config.get("warehouse")
        database = os.getenv("SNOWFLAKE_DATABASE") or config.get("database")
        schema = os.getenv("SNOWFLAKE_SCHEMA") or config.get("schema")

        if not account or not user:
            raise ValueError("Snowflakeæ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

        connect_params = {"account": account, "user": user}

        if private_key_path:
            key_path = Path(private_key_path).expanduser()
            with open(key_path, "rb") as key_file:
                private_key = serialization.load_pem_private_key(
                    key_file.read(), password=None, backend=default_backend()
                )
            connect_params["private_key"] = private_key.private_bytes(
                encoding=serialization.Encoding.DER,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption(),
            )
        elif password:
            connect_params["password"] = password

        if authenticator and not private_key_path:
            connect_params["authenticator"] = authenticator
        if role:
            connect_params["role"] = role
        if warehouse:
            connect_params["warehouse"] = warehouse
        if database:
            connect_params["database"] = database
        if schema:
            connect_params["schema"] = schema

        return snowflake.connector.connect(**connect_params)

    def query_sf(sql):
        """Snowflakeã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        conn = get_snowflake_connection()
        cur = conn.cursor()
        try:
            cur.execute(sql)
            return cur.fetch_pandas_all()
        finally:
            cur.close()
            conn.close()
    return SF_SCHEMA, query_sf


@app.cell
def _(mo):
    mo.md("""
    ## 1. ãƒãƒ£ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    """)
    return


@app.cell
def _(Path, normalize_company_name, pd):
    tsv_path = Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/docs/assets/infobox_data.tsv")
    df_churn_raw = pd.read_csv(tsv_path, sep="\t")

    # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
    df_churn_raw.columns = [c.strip() for c in df_churn_raw.columns]

    # ã‚¨ãƒ©ãƒ¼è¡Œã‚’é™¤å¤–ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡ã‚«ãƒ©ãƒ ã‚’æ¢ã™ï¼‰
    active_rate_col = [c for c in df_churn_raw.columns if "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡" in c]
    if active_rate_col:
        df_churn = df_churn_raw[
            ~df_churn_raw[active_rate_col[0]].astype(str).str.contains("ã‚¨ãƒ©ãƒ¼", na=False)
        ].copy()
    else:
        # ã‚¨ãƒ©ãƒ¼è¡ŒãŒã‚ã‚‹åˆ—ã‚’æ¢ã™
        df_churn = df_churn_raw[
            ~df_churn_raw.apply(lambda row: row.astype(str).str.contains("ã‚¨ãƒ©ãƒ¼").any(), axis=1)
        ].copy()

    # statusåˆ—ã‚’ãƒã‚¤ãƒŠãƒªåŒ–ï¼ˆå¤§æ–‡å­—å°æ–‡å­—å¯¾å¿œï¼‰
    status_col = "STATUS" if "STATUS" in df_churn.columns else "status"
    df_churn["is_churned"] = (df_churn[status_col] == "è§£ç´„æ¸ˆã¿").astype(int)
    df_churn["COMPNO"] = df_churn["COMPNO"].astype(str)

    # åˆ©ç”¨ã—ã‚„ã™ã„ã‚«ãƒ©ãƒ åã«ãƒªãƒãƒ¼ãƒ 
    rename_map = {
        "STATUS": "status",
        "COMPANY_NAME": "CompanyName",
        "ACCOUT_COUNT": "account_count",
    }
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡ã‚«ãƒ©ãƒ 
    if active_rate_col:
        rename_map[active_rate_col[0]] = "active_rate"
    df_churn.rename(columns={k: v for k, v in rename_map.items() if k in df_churn.columns}, inplace=True)
    if "CompanyName" in df_churn.columns:
        df_churn["company_name_norm"] = df_churn["CompanyName"].apply(normalize_company_name)
    else:
        df_churn["company_name_norm"] = None
    return df_churn, df_churn_raw


@app.cell
def _(df_churn, df_churn_raw, mo):
    mo.md(f"""
    **ãƒãƒ£ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿**:
    - å…¨è¡Œæ•°: {len(df_churn_raw):,}
    - ã‚¨ãƒ©ãƒ¼é™¤å¤–å¾Œ: {len(df_churn):,}
    - è§£ç´„æ¸ˆã¿: {df_churn['is_churned'].sum():,}
    - å¥‘ç´„ä¸­: {(~df_churn['is_churned'].astype(bool)).sum():,}
    """)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 1-b. è§£ç´„ç†ç”±ãƒ‡ãƒ¼ã‚¿ï¼ˆCSï¼‰
    """)
    return


@app.cell
def _(Path, extract_company_name_from_deal, normalize_company_name, pd):
    churn_reason_path = Path(
        "/Users/kou1904/githubactions_fordata/work/aieda_agent/docs/assets/ã€CSã€‘è§£ç´„é¡§å®¢ä¸€è¦§_25å¹´1æœˆã€œ - ã‚·ãƒ¼ãƒˆ1.csv"
    )
    df_churn_reason_raw = pd.DataFrame()
    df_churn_reason_latest = pd.DataFrame()

    if churn_reason_path.exists():
        df_churn_reason_raw = pd.read_csv(churn_reason_path)
        df_churn_reason = df_churn_reason_raw.copy()

        if "å•†è«‡å" in df_churn_reason.columns:
            df_churn_reason["deal_company_name"] = df_churn_reason["å•†è«‡å"].apply(
                extract_company_name_from_deal
            )
        else:
            df_churn_reason["deal_company_name"] = None

        df_churn_reason["company_name_norm"] = df_churn_reason["deal_company_name"].apply(
            normalize_company_name
        )

        loss_date_col = "ç¾å¥‘ç´„çµ‚äº†æ—¥" if "ç¾å¥‘ç´„çµ‚äº†æ—¥" in df_churn_reason.columns else None
        if loss_date_col is None and "å®Œäº†äºˆå®šæ—¥" in df_churn_reason.columns:
            loss_date_col = "å®Œäº†äºˆå®šæ—¥"

        if loss_date_col:
            df_churn_reason["loss_end_date"] = pd.to_datetime(
                df_churn_reason[loss_date_col], errors="coerce"
            )
        else:
            df_churn_reason["loss_end_date"] = pd.NaT

        df_churn_reason_sorted = df_churn_reason.sort_values("loss_end_date")
        df_churn_reason_latest = (
            df_churn_reason_sorted.dropna(subset=["company_name_norm"])
            .groupby("company_name_norm")
            .tail(1)
            .copy()
        )

        churn_reason_rename_map = {
            "å•†è«‡å": "deal_name",
            "å¤±æ³¨ç¨®åˆ¥": "loss_type",
            "å¤±æ³¨ç†ç”±": "loss_reason",
            "å—æ³¨/å¤±æ³¨ç†ç”±è©³ç´°": "loss_detail",
            "å•†è«‡ æ‰€æœ‰è€…": "deal_owner",
            "ç¾å¥‘ç´„çµ‚äº†æ—¥": "current_contract_end_date",
            "å®Œäº†äºˆå®šæ—¥": "planned_close_date",
        }
        needed_cols = ["company_name_norm", "deal_company_name"] + [
            col for col in churn_reason_rename_map if col in df_churn_reason_latest.columns
        ]
        df_churn_reason_latest = df_churn_reason_latest[needed_cols].rename(
            columns=churn_reason_rename_map
        )
    return df_churn_reason_latest, df_churn_reason_raw


@app.cell
def _(df_churn_reason_latest, df_churn_reason_raw, mo):
    mo.md(f"""
    **è§£ç´„ç†ç”±ãƒ‡ãƒ¼ã‚¿**:
    - å…¨è¡Œæ•°: {len(df_churn_reason_raw):,}
    - æŠ½å‡ºä¼šç¤¾æ•°: {len(df_churn_reason_latest):,}
    """)
    return


@app.cell
def _(df_churn_reason_latest, mo):
    mo.ui.table(df_churn_reason_latest.head(20), pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 2. ã‚«ãƒ†ã‚´ãƒªæ¢ç´¢ï¼ˆå–¶æ¥­æ”¯æ´/CRM/SFAé–¢é€£ï¼‰
    """)
    return


@app.cell
def _(mo, query_bq):
    # ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å–å¾—
    category_query = """
    SELECT DISTINCT 
        original_category_name as category,
        COUNT(*) as count
    FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3`
    WHERE view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    GROUP BY 1
    ORDER BY count DESC
    LIMIT 100
    """
    df_categories = query_bq(category_query)
    mo.md(f"**ã‚«ãƒ†ã‚´ãƒªæ•°ï¼ˆç›´è¿‘30æ—¥ï¼‰**: {len(df_categories):,}")
    return (df_categories,)


@app.cell
def _(df_categories, mo):
    mo.ui.table(df_categories, pagination=True)
    return


@app.cell
def _(mo):
    # å–¶æ¥­æ”¯æ´é–¢é€£ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚£ãƒ«ã‚¿
    sales_keywords = ["å–¶æ¥­", "CRM", "SFA", "ã‚»ãƒ¼ãƒ«ã‚¹", "ãƒªãƒ¼ãƒ‰", "é¡§å®¢ç®¡ç†", "å•†è«‡", "MA", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°"]
    category_filter = mo.ui.text(
        label="ã‚«ãƒ†ã‚´ãƒªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
        value=",".join(sales_keywords),
    )
    category_filter
    return (category_filter,)


@app.cell
def _(category_filter, df_categories, mo, pd):
    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    keywords = [k.strip() for k in category_filter.value.split(",") if k.strip()]
    df_sales_categories = pd.DataFrame()

    if len(df_categories) > 0 and keywords:
        pattern = "|".join(keywords)
        df_sales_categories = df_categories[
            df_categories["category"].str.contains(pattern, case=False, na=False)
        ].copy()
        mo.md(f"**å–¶æ¥­æ”¯æ´é–¢é€£ã‚«ãƒ†ã‚´ãƒª**: {len(df_sales_categories):,} ä»¶")
    return (df_sales_categories,)


@app.cell
def _(df_sales_categories, mo):
    mo.ui.table(df_sales_categories, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 3. ç«¶åˆã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†æï¼ˆå–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªï¼‰
    """)
    return


@app.cell
def _(df_sales_categories, mo, pd, query_bq):
    # å–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆå¤‰å‹•ã‚’å–å¾—
    df_competitor_intent = pd.DataFrame()

    if len(df_sales_categories) > 0:
        categories_list = df_sales_categories["category"].tolist()[:10]  # ä¸Šä½10ã‚«ãƒ†ã‚´ãƒª
        categories_sql = ",".join([f"'{c}'" for c in categories_list])

        competitor_intent_query = f"""
        SELECT 
            c.original_category_name as category,
            s.intent_level,
            CASE s.intent_level 
                WHEN 1 THEN 'Low' 
                WHEN 2 THEN 'Middle' 
                WHEN 3 THEN 'High' 
            END as level_name,
            COUNT(DISTINCT s.corporate_id) as company_count
        FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3` c
        JOIN `gree-dionysus-infobox.production_infobox.first_party_score_company_latest` s
          ON CAST(c.corporate_id AS INT64) = s.corporate_id
        WHERE c.view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
          AND c.original_category_name IN ({categories_sql})
          AND s.intent_level IN (2, 3)
        GROUP BY 1, 2, 3
        ORDER BY 1, 2
        """
        df_competitor_intent = query_bq(competitor_intent_query)
    mo.md(f"**ç«¶åˆã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼ˆå–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªï¼‰**: {len(df_competitor_intent):,} ä»¶")
    return (df_competitor_intent,)


@app.cell
def _(df_competitor_intent, mo):
    mo.ui.table(df_competitor_intent, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 4. IDç´ã¥ã‘ï¼ˆCOMPNO â†’ ORGID â†’ corporate_idï¼‰
    """)
    return


@app.cell
def _(SF_SCHEMA, mo, pd, query_sf):
    # Snowflake: BEEGLECOMPANY + USERORGANIZATION ã§ORGIDã‚’å–å¾—
    # BeegleCompany.ID â†’ USERORGANIZATION.COMPANYID â†’ USERORGANIZATION.ORGIDï¼ˆGAã®org_idã¨ä¸€è‡´ï¼‰
    df_id_mapping = pd.DataFrame()

    try:
        mapping_query = f"""
        SELECT
            bc.COMPNO,
            bc.ID AS COMPANYID,
            u.ORGID,
            u.NAME AS ORG_NAME,
            bc.SHOGO AS BQ_COMPANY_NAME,
            bc.KANA AS BQ_COMPANY_KANA,
            bc.CEO,
            bc.SETURITU AS ESTABLISHMENT,
            bc.ZIP,
            bc.ADD AS ADDRESS,
            bc.PREFID,
            bc.CITYID,
            bc.TEL AS PHONE,
            bc.HPURL AS WEBSITE_URL,
            bc.MAIL,
            bc.GYOSHUSHOID AS INDUSTRY_ID,
            bc.EMPID AS EMPLOYEE_ID,
            bc.EMPCOUNT AS EMPLOYEE_COUNT,
            bc.REVENUEID AS REVENUE_ID,
            bc.SHIHONID AS CAPITAL_ID,
            bc.ISCLOSED,
            bc.CREATEDAT AS BQ_CREATED_AT,
            u.CREATEDAT AS ORG_CREATED_AT
        FROM {SF_SCHEMA}.BEEGLECOMPANY bc
        JOIN {SF_SCHEMA}.USERORGANIZATION u
            ON bc.ID = u.COMPANYID
        WHERE bc.COMPNO IS NOT NULL
        """
        df_id_mapping = query_sf(mapping_query)
        df_id_mapping["COMPNO"] = df_id_mapping["COMPNO"].astype(str)
        mo.md(f"**BeegleCompany + USERORGANIZATION ãƒãƒƒãƒ”ãƒ³ã‚°ä»¶æ•°**: {len(df_id_mapping):,}")
    except Exception as e:
        mo.md(f"**Snowflakeæ¥ç¶šã‚¨ãƒ©ãƒ¼**: `{e}`")
    return (df_id_mapping,)


@app.cell
def _(df_id_mapping, mo):
    mo.ui.table(df_id_mapping.head(20), pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5. GAåˆ©ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒšãƒ¼ã‚¸ç¾¤åˆ¥ï¼‰
    """)
    return


@app.cell
def _(GA_DATASET_ID, mo, query_bq):
    # GA4 org_idå˜ä½ã®åŸºæœ¬æŒ‡æ¨™ + ãƒšãƒ¼ã‚¸ç¾¤åˆ¥
    # å®Ÿéš›ã®URLæ§‹é€ :
    #   /companies (ä¸€è¦§=æ¤œç´¢), /companies/[id] (ä¼æ¥­è©³ç´°)
    #   /company-lists, /people-lists, /leads-lists (ãƒªã‚¹ãƒˆç³»)
    #   /analysis (ãƒˆãƒ¬ãƒ³ãƒ‰/1stParty/CRMé€£æº)
    #   /people (äººç‰©)
    #   /settings
    ga_query = f"""
    WITH base AS (
        SELECT
            user_pseudo_id,
            (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS ga_session_id,
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name,
            event_date
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            user_pseudo_id,
            ga_session_id,
            event_name,
            CASE
                WHEN REGEXP_CONTAINS(page_location, r'/company-lists|/people-lists|/leads-lists') THEN 'list'
                WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN 'company_detail'
                WHEN REGEXP_CONTAINS(page_location, r'/analysis') THEN 'analysis'
                WHEN REGEXP_CONTAINS(page_location, r'/people') THEN 'people'
                WHEN REGEXP_CONTAINS(page_location, r'/settings') THEN 'settings'
                WHEN REGEXP_CONTAINS(page_location, r'/sign-in|/sign-up|/org-selection') THEN 'auth'
                ELSE 'other'
            END AS page_group
        FROM base
        WHERE org_id IS NOT NULL
    )
    SELECT
        org_id,
        COUNT(DISTINCT user_pseudo_id) AS users,
        COUNT(DISTINCT CONCAT(user_pseudo_id, '-', CAST(ga_session_id AS STRING))) AS sessions,
        COUNTIF(event_name = 'page_view') AS page_views,
        COUNTIF(page_group = 'company_detail' AND event_name = 'page_view') AS pv_company_detail,
        COUNTIF(page_group = 'list' AND event_name = 'page_view') AS pv_list,
        COUNTIF(page_group = 'analysis' AND event_name = 'page_view') AS pv_analysis,
        COUNTIF(page_group = 'people' AND event_name = 'page_view') AS pv_people,
        COUNTIF(page_group = 'settings' AND event_name = 'page_view') AS pv_settings
    FROM classified
    GROUP BY org_id
    """
    df_ga = query_bq(ga_query)
    mo.md(f"**GAåˆ©ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆorg_idå˜ä½ï¼‰**: {len(df_ga):,} ä»¶")
    return (df_ga,)


@app.cell
def _(df_ga, mo, pd):
    import altair as _alt

    _ga_summary_outputs = []

    if len(df_ga) > 0:
        _total_orgs = len(df_ga)
        _pv_cols = {
            "pv_company_detail": "ä¼æ¥­è©³ç´°",
            "pv_list": "ãƒªã‚¹ãƒˆç³»",
            "pv_analysis": "åˆ†æ",
            "pv_people": "äººç‰©",
            "pv_settings": "è¨­å®š",
        }

        # å„ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãŸä¼æ¥­æ•°ã¨å‰²åˆ
        _summary_rows = []
        for _col, _label in _pv_cols.items():
            if _col in df_ga.columns:
                _org_count = int((df_ga[_col] > 0).sum())
                _pv_total = int(df_ga[_col].sum())
                _rate = round(_org_count / _total_orgs * 100, 1) if _total_orgs > 0 else 0
                _summary_rows.append({
                    "ãƒšãƒ¼ã‚¸ç¨®åˆ¥": _label,
                    "åˆ©ç”¨ä¼æ¥­æ•°": _org_count,
                    "åˆ©ç”¨ç‡(%)": _rate,
                    "ç·PV": _pv_total,
                    "ä¼æ¥­ã‚ãŸã‚Šå¹³å‡PV": round(_pv_total / _org_count, 1) if _org_count > 0 else 0,
                })

        _df_summary = pd.DataFrame(_summary_rows)
        _ga_summary_outputs.append(mo.md(f"### æ©Ÿèƒ½åˆ¥åˆ©ç”¨çŠ¶æ³ï¼ˆåˆ†æ¯: GAç™»éŒ²ä¼æ¥­ **{_total_orgs}** ç¤¾ï¼‰"))

        # æ£’ã‚°ãƒ©ãƒ•: åˆ©ç”¨ä¼æ¥­æ•° | åˆ©ç”¨ç‡
        _chart_count = (
            _alt.Chart(_df_summary)
            .mark_bar()
            .encode(
                x=_alt.X("åˆ©ç”¨ä¼æ¥­æ•°:Q", title="åˆ©ç”¨ä¼æ¥­æ•°"),
                y=_alt.Y("ãƒšãƒ¼ã‚¸ç¨®åˆ¥:N", title="", sort=list(_pv_cols.values())),
                color=_alt.Color("ãƒšãƒ¼ã‚¸ç¨®åˆ¥:N", legend=None),
                tooltip=["ãƒšãƒ¼ã‚¸ç¨®åˆ¥", "åˆ©ç”¨ä¼æ¥­æ•°", "åˆ©ç”¨ç‡(%)", "ç·PV"],
            )
            .properties(title="åˆ©ç”¨ä¼æ¥­æ•°", width=350, height=220)
        )
        _chart_rate = (
            _alt.Chart(_df_summary)
            .mark_bar()
            .encode(
                x=_alt.X("åˆ©ç”¨ç‡(%):Q", title="åˆ©ç”¨ç‡ (%)", scale=_alt.Scale(domain=[0, 100])),
                y=_alt.Y("ãƒšãƒ¼ã‚¸ç¨®åˆ¥:N", title="", sort=list(_pv_cols.values())),
                color=_alt.Color("ãƒšãƒ¼ã‚¸ç¨®åˆ¥:N", legend=None),
                tooltip=["ãƒšãƒ¼ã‚¸ç¨®åˆ¥", "åˆ©ç”¨ä¼æ¥­æ•°", "åˆ©ç”¨ç‡(%)", "ç·PV"],
            )
            .properties(title="åˆ©ç”¨ç‡ (%)", width=350, height=220)
        )
        _ga_summary_outputs.append(_chart_count | _chart_rate)
        _ga_summary_outputs.append(mo.ui.table(_df_summary, pagination=False))

        # å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­20ä»¶ï¼‰
        _ga_summary_outputs.append(mo.md("### org_idåˆ¥ è©³ç´°ãƒ‡ãƒ¼ã‚¿ï¼ˆå…ˆé ­20ä»¶ï¼‰"))
        _ga_summary_outputs.append(mo.ui.table(df_ga.head(20), pagination=True))
    else:
        _ga_summary_outputs.append(mo.md("*GAãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_ga_summary_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ### otherå†…è¨³ï¼ˆpage_location ä¸Šä½ï¼‰
    """)
    return


@app.cell
def _(GA_DATASET_ID, mo, query_bq):
    # otherå†…è¨³: æ­£ã—ã„URLåˆ†é¡ã«åŸºã¥ã
    _classify_case = """
            CASE
                WHEN REGEXP_CONTAINS(page_location, r'/company-lists|/people-lists|/leads-lists') THEN 'list'
                WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN 'company_detail'
                WHEN REGEXP_CONTAINS(page_location, r'/companies') THEN 'search'
                WHEN REGEXP_CONTAINS(page_location, r'/analysis') THEN 'analysis'
                WHEN REGEXP_CONTAINS(page_location, r'/people') THEN 'people'
                WHEN REGEXP_CONTAINS(page_location, r'/settings') THEN 'settings'
                WHEN REGEXP_CONTAINS(page_location, r'/sign-in|/sign-up|/org-selection') THEN 'auth'
                ELSE 'other'
            END
    """

    other_overall_query = f"""
    WITH base AS (
        SELECT
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            page_location,
            event_name,
            {_classify_case} AS page_group
        FROM base
        WHERE org_id IS NOT NULL
          AND page_location IS NOT NULL
    )
    SELECT
        page_location,
        COUNTIF(event_name = 'page_view') AS page_views
    FROM classified
    WHERE page_group = 'other'
    GROUP BY page_location
    ORDER BY page_views DESC
    LIMIT 50
    """

    other_by_org_query = f"""
    WITH base AS (
        SELECT
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            page_location,
            event_name,
            {_classify_case} AS page_group
        FROM base
        WHERE org_id IS NOT NULL
          AND page_location IS NOT NULL
    )
    SELECT
        org_id,
        page_location,
        COUNTIF(event_name = 'page_view') AS page_views
    FROM classified
    WHERE page_group = 'other'
    GROUP BY org_id, page_location
    ORDER BY page_views DESC
    LIMIT 200
    """

    df_other_pages = query_bq(other_overall_query)
    df_other_pages_by_org = query_bq(other_by_org_query)
    mo.md(
        f"**otherå†…è¨³**: å…¨ä½“ {len(df_other_pages):,} ä»¶ / org_idåˆ¥ {len(df_other_pages_by_org):,} ä»¶"
    )
    return df_other_pages, df_other_pages_by_org


@app.cell
def _(df_other_pages, df_other_pages_by_org, mo):
    mo.md("#### å…¨ä½“ä¸Šä½")
    mo.ui.table(df_other_pages, pagination=True)
    mo.md("#### org_idåˆ¥ ä¸Šä½")
    mo.ui.table(df_other_pages_by_org, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-b. GAæ¨ç§»ï¼ˆå…¨ä½“ãƒãƒ£ãƒ¼ãƒˆï¼‰

    å…¨çµ„ç¹”ã®GAåˆ©ç”¨æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰ã‚’PVãƒ»UUã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(GA_DATASET_ID, query_bq):
    _ga_overall_query = f"""
    WITH base AS (
        SELECT
            FORMAT_DATE('%Y-%m', PARSE_DATE('%Y%m%d', event_date)) AS month,
            user_pseudo_id,
            event_name
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))
    )
    SELECT
        month,
        COUNT(DISTINCT user_pseudo_id) AS uu,
        COUNTIF(event_name = 'page_view') AS pv
    FROM base
    GROUP BY month
    ORDER BY month
    """
    df_ga_overall_trend = query_bq(_ga_overall_query)
    return (df_ga_overall_trend,)


@app.cell
def _(df_ga_overall_trend, mo):
    import altair as _alt

    _ga_chart_outputs = []
    if len(df_ga_overall_trend) > 0:
        _ga_chart_outputs.append(mo.md(f"**å…¨ä½“GAæ¨ç§»**: {len(df_ga_overall_trend)} ãƒ¶æœˆåˆ†"))

        _chart_pv = (
            _alt.Chart(df_ga_overall_trend)
            .mark_line(point=True)
            .encode(
                x=_alt.X("month:N", title="æœˆ"),
                y=_alt.Y("pv:Q", title="PV", scale=_alt.Scale(zero=False)),
                tooltip=["month", "pv"],
            )
            .properties(title="PVæ¨ç§»ï¼ˆå…¨ä½“ï¼‰", width=400, height=280)
        )

        _chart_uu = (
            _alt.Chart(df_ga_overall_trend)
            .mark_line(point=True, color="orange")
            .encode(
                x=_alt.X("month:N", title="æœˆ"),
                y=_alt.Y("uu:Q", title="UU", scale=_alt.Scale(zero=False)),
                tooltip=["month", "uu"],
            )
            .properties(title="UUæ¨ç§»ï¼ˆå…¨ä½“ï¼‰", width=400, height=280)
        )

        _ga_chart_outputs.append(_chart_pv | _chart_uu)
        _ga_chart_outputs.append(mo.ui.table(df_ga_overall_trend, pagination=False))
    else:
        _ga_chart_outputs.append(mo.md("*GAãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_ga_chart_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-c. ãƒªã‚¹ãƒˆè¿½åŠ æ¨ç§»ï¼ˆå…¨ä½“ãƒãƒ£ãƒ¼ãƒˆï¼‰

    CompanyListãƒ»PeopleListã®æœˆæ¬¡è¿½åŠ æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰ã‚’Useræ•°ãƒ»å›æ•°ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(SF_SCHEMA, query_sf):
    _cl_trend_query = f"""
    SELECT
        DATE_TRUNC('month', cl.CREATEDAT) AS MONTH,
        COUNT(DISTINCT cl.ID) AS LIST_COUNT,
        COUNT(DISTINCT cl.USERORGRELATIONID) AS USER_COUNT
    FROM {SF_SCHEMA}.COMPANYLIST cl
    WHERE cl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
    GROUP BY MONTH
    ORDER BY MONTH
    """
    df_companylist_trend_all = query_sf(_cl_trend_query)

    _pl_trend_query = f"""
    SELECT
        DATE_TRUNC('month', pl.CREATEDAT) AS MONTH,
        COUNT(DISTINCT pl.ID) AS LIST_COUNT,
        COUNT(DISTINCT pl.USERORGRELATIONID) AS USER_COUNT
    FROM {SF_SCHEMA}.PEOPLELIST pl
    WHERE pl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
    GROUP BY MONTH
    ORDER BY MONTH
    """
    df_peoplelist_trend_all = query_sf(_pl_trend_query)
    return df_companylist_trend_all, df_peoplelist_trend_all


@app.cell
def _(df_companylist_trend_all, df_peoplelist_trend_all, mo):
    import altair as _alt

    _list_chart_outputs = []

    # --- CompanyList ---
    _list_chart_outputs.append(mo.md("### CompanyListè¿½åŠ æ¨ç§»ï¼ˆå…¨ä½“ï¼‰"))
    if len(df_companylist_trend_all) > 0:
        _df_cl = df_companylist_trend_all.copy()
        _df_cl.columns = [c.upper() for c in _df_cl.columns]
        _df_cl["MONTH"] = _df_cl["MONTH"].astype(str).str[:7]

        _chart_cl_count = (
            _alt.Chart(_df_cl)
            .mark_bar(color="steelblue", opacity=0.7)
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("LIST_COUNT:Q", title="ãƒªã‚¹ãƒˆä½œæˆæ•°"),
                tooltip=["MONTH", "LIST_COUNT", "USER_COUNT"],
            )
            .properties(title="CompanyListä½œæˆæ•°", width=400, height=280)
        )
        _chart_cl_user = (
            _alt.Chart(_df_cl)
            .mark_line(point=True, color="red")
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("USER_COUNT:Q", title="ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", scale=_alt.Scale(zero=False)),
                tooltip=["MONTH", "LIST_COUNT", "USER_COUNT"],
            )
            .properties(title="CompanyListä½œæˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", width=400, height=280)
        )
        _list_chart_outputs.append(_chart_cl_count | _chart_cl_user)
        _list_chart_outputs.append(mo.ui.table(_df_cl, pagination=False))
    else:
        _list_chart_outputs.append(mo.md("*CompanyListãƒ‡ãƒ¼ã‚¿ãªã—*"))

    # --- PeopleList ---
    _list_chart_outputs.append(mo.md("### PeopleListè¿½åŠ æ¨ç§»ï¼ˆå…¨ä½“ï¼‰"))
    if len(df_peoplelist_trend_all) > 0:
        _df_pl = df_peoplelist_trend_all.copy()
        _df_pl.columns = [c.upper() for c in _df_pl.columns]
        _df_pl["MONTH"] = _df_pl["MONTH"].astype(str).str[:7]

        _chart_pl_count = (
            _alt.Chart(_df_pl)
            .mark_bar(color="teal", opacity=0.7)
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("LIST_COUNT:Q", title="ãƒªã‚¹ãƒˆä½œæˆæ•°"),
                tooltip=["MONTH", "LIST_COUNT", "USER_COUNT"],
            )
            .properties(title="PeopleListä½œæˆæ•°", width=400, height=280)
        )
        _chart_pl_user = (
            _alt.Chart(_df_pl)
            .mark_line(point=True, color="purple")
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("USER_COUNT:Q", title="ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", scale=_alt.Scale(zero=False)),
                tooltip=["MONTH", "LIST_COUNT", "USER_COUNT"],
            )
            .properties(title="PeopleListä½œæˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", width=400, height=280)
        )
        _list_chart_outputs.append(_chart_pl_count | _chart_pl_user)
        _list_chart_outputs.append(mo.ui.table(_df_pl, pagination=False))
    else:
        _list_chart_outputs.append(mo.md("*PeopleListãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_list_chart_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-b2. ãƒšãƒ¼ã‚¸åˆ¥ã‚¢ã‚¯ã‚»ã‚¹ç‡æ¨ç§»

    å„æ©Ÿèƒ½ãƒšãƒ¼ã‚¸ã®ã€Œã‚¢ã‚¯ã‚»ã‚¹UU / ç·ãƒ­ã‚°ã‚¤ãƒ³UUã€æ¯”ç‡ã‚’æœˆæ¬¡ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(GA_DATASET_ID, query_bq):
    # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½: æœˆåˆ¥ Ã— ã‚«ãƒ†ã‚´ãƒªåˆ¥ UUæ•° + ç‡ ---
    # å®Ÿéš›ã®URLæ§‹é€ :
    #   /companies(?:[?#]|$) â†’ ä¼æ¥­æ¤œç´¢(ä¸€è¦§)
    #   /companies/[a-z0-9]  â†’ ä¼æ¥­è©³ç´°
    #   /company-lists|/people-lists|/leads-lists â†’ ãƒªã‚¹ãƒˆç³»
    #   -lists/import        â†’ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    #   /analysis/trends     â†’ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    #   /analysis/intent-settings â†’ 1stPartyã‚¹ã‚³ã‚¢è¨­å®š
    #   /analysis/crm-integration â†’ CRMé€£æº
    #   /people              â†’ äººç‰©
    _user_query = f"""
    WITH base AS (
        SELECT
            FORMAT_DATE('%Y-%m', PARSE_DATE('%Y%m%d', event_date)) AS month,
            user_pseudo_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))
          AND event_name = 'page_view'
    ),
    monthly_total AS (
        SELECT month, COUNT(DISTINCT user_pseudo_id) AS total_uu
        FROM base GROUP BY month
    ),
    monthly_page AS (
        SELECT month,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/companies(?:[?#]|$)') THEN user_pseudo_id END) AS uu_search,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN user_pseudo_id END) AS uu_company_detail,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/company-lists|/people-lists|/leads-lists') THEN user_pseudo_id END) AS uu_lists,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'-lists/import') THEN user_pseudo_id END) AS uu_import,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/trends') THEN user_pseudo_id END) AS uu_trends,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/intent-settings') THEN user_pseudo_id END) AS uu_intent,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/crm-integration') THEN user_pseudo_id END) AS uu_crm,
            COUNT(DISTINCT CASE WHEN REGEXP_CONTAINS(page_location, r'/people') THEN user_pseudo_id END) AS uu_people
        FROM base GROUP BY month
    )
    SELECT p.month, t.total_uu,
           p.uu_search,          ROUND(SAFE_DIVIDE(p.uu_search, t.total_uu) * 100, 1)          AS rate_search,
           p.uu_company_detail,  ROUND(SAFE_DIVIDE(p.uu_company_detail, t.total_uu) * 100, 1)  AS rate_company_detail,
           p.uu_lists,           ROUND(SAFE_DIVIDE(p.uu_lists, t.total_uu) * 100, 1)           AS rate_lists,
           p.uu_import,          ROUND(SAFE_DIVIDE(p.uu_import, t.total_uu) * 100, 1)          AS rate_import,
           p.uu_trends,          ROUND(SAFE_DIVIDE(p.uu_trends, t.total_uu) * 100, 1)          AS rate_trends,
           p.uu_intent,          ROUND(SAFE_DIVIDE(p.uu_intent, t.total_uu) * 100, 1)          AS rate_intent,
           p.uu_crm,             ROUND(SAFE_DIVIDE(p.uu_crm, t.total_uu) * 100, 1)             AS rate_crm,
           p.uu_people,          ROUND(SAFE_DIVIDE(p.uu_people, t.total_uu) * 100, 1)          AS rate_people
    FROM monthly_page p JOIN monthly_total t ON p.month = t.month
    ORDER BY p.month
    """
    df_ga_page_rate_user = query_bq(_user_query)
    return (df_ga_page_rate_user,)


@app.cell
def _(GA_DATASET_ID, query_bq):
    # --- ä¼æ¥­(org_id)å˜ä½: æœˆÃ—org_id ã”ã¨ã®å„ã‚«ãƒ†ã‚´ãƒªåˆ©ç”¨æœ‰ç„¡ (0/1) ---
    _org_raw_query = f"""
    SELECT
        month,
        org_id,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies(?:[?#]|$)') THEN 1 ELSE 0 END) AS has_search,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN 1 ELSE 0 END) AS has_company_detail,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/company-lists|/people-lists|/leads-lists') THEN 1 ELSE 0 END) AS has_lists,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'-lists/import') THEN 1 ELSE 0 END) AS has_import,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/trends') THEN 1 ELSE 0 END) AS has_trends,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/intent-settings') THEN 1 ELSE 0 END) AS has_intent,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/analysis/crm-integration') THEN 1 ELSE 0 END) AS has_crm,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/people') THEN 1 ELSE 0 END) AS has_people
    FROM (
        SELECT
            FORMAT_DATE('%Y-%m', PARSE_DATE('%Y%m%d', event_date)) AS month,
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))
          AND event_name = 'page_view'
    )
    WHERE org_id IS NOT NULL
    GROUP BY month, org_id
    """
    df_ga_page_org_raw = query_bq(_org_raw_query)
    return (df_ga_page_org_raw,)


@app.cell
def _(df_churn, df_ga_page_org_raw, df_id_mapping, pd):
    # --- ä¼æ¥­å˜ä½ã®æœˆæ¬¡é›†ç´„ + è§£ç´„/å¥‘ç´„ä¸­ã®ç´ã¥ã‘ ---
    df_ga_page_rate_org = pd.DataFrame()
    df_ga_page_rate_churn = pd.DataFrame()

    _has_cols = [c for c in df_ga_page_org_raw.columns if c.startswith("has_")]

    if len(df_ga_page_org_raw) > 0 and _has_cols:
        # å…¨ä½“é›†ç´„: æœˆåˆ¥ã«å„ã‚«ãƒ†ã‚´ãƒªã®ä¼æ¥­æ•°ã¨ç·ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ä¼æ¥­æ•°
        _total_org = df_ga_page_org_raw.groupby("month")["org_id"].nunique().reset_index(name="total_orgs")
        _page_org = df_ga_page_org_raw.groupby("month")[_has_cols].sum().astype(int).reset_index()
        df_ga_page_rate_org = _page_org.merge(_total_org, on="month")
        for _hc in _has_cols:
            _rc = _hc.replace("has_", "rate_")
            _oc = _hc.replace("has_", "org_")
            df_ga_page_rate_org[_oc] = df_ga_page_rate_org[_hc]
            df_ga_page_rate_org[_rc] = (df_ga_page_rate_org[_hc] / df_ga_page_rate_org["total_orgs"] * 100).round(1)

        # è§£ç´„/å¥‘ç´„ä¸­ã®ç´ã¥ã‘
        _org_churn = df_ga_page_org_raw.copy()
        if len(df_id_mapping) > 0 and "ORGID" in df_id_mapping.columns:
            _map = df_id_mapping[["ORGID", "COMPNO"]].drop_duplicates(subset=["ORGID"])
            _org_churn = _org_churn.merge(_map, left_on="org_id", right_on="ORGID", how="left")
        if len(df_churn) > 0 and "COMPNO" in _org_churn.columns and "COMPNO" in df_churn.columns:
            _churn_map = df_churn[["COMPNO", "status"]].drop_duplicates(subset=["COMPNO"])
            _org_churn = _org_churn.merge(_churn_map, on="COMPNO", how="left")
        if "status" not in _org_churn.columns:
            _org_churn["status"] = "ä¸æ˜"
        _org_churn["status"] = _org_churn["status"].fillna("ä¸æ˜")

        # è§£ç´„/å¥‘ç´„ä¸­åˆ¥ã®æœˆæ¬¡é›†ç´„
        _churn_total = _org_churn.groupby(["month", "status"])["org_id"].nunique().reset_index(name="total_orgs")
        _churn_page = _org_churn.groupby(["month", "status"])[_has_cols].sum().astype(int).reset_index()
        df_ga_page_rate_churn = _churn_page.merge(_churn_total, on=["month", "status"])
        for _hc in _has_cols:
            _rc = _hc.replace("has_", "rate_")
            _oc = _hc.replace("has_", "org_")
            df_ga_page_rate_churn[_oc] = df_ga_page_rate_churn[_hc]
            df_ga_page_rate_churn[_rc] = (df_ga_page_rate_churn[_hc] / df_ga_page_rate_churn["total_orgs"] * 100).round(1)

    return df_ga_page_rate_churn, df_ga_page_rate_org


@app.cell
def _(df_ga_page_rate_churn, df_ga_page_rate_org, df_ga_page_rate_user, mo, pd):
    import altair as _alt

    _outputs = []

    # ========== ã‚«ãƒ†ã‚´ãƒªå®šç¾© ==========
    _main_labels = {
        "search": "ä¼æ¥­æ¤œç´¢(ä¸€è¦§)",
        "company_detail": "ä¼æ¥­è©³ç´°",
        "lists": "ãƒªã‚¹ãƒˆç³»",
        "people": "äººç‰©",
    }
    _sub_labels = {
        "trends": "ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ",
        "intent": "1stPartyã‚¹ã‚³ã‚¢è¨­å®š",
        "crm": "CRMé€£æº",
        "import": "ãƒªã‚¹ãƒˆã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
    }
    _all_labels = {**_main_labels, **_sub_labels}

    # ========== ãƒ˜ãƒ«ãƒ‘ãƒ¼: melt ã—ã¦ãƒãƒ£ãƒ¼ãƒˆã‚’ä½œã‚‹ ==========
    def _make_line_chart(df, cols_map, id_col, value_col, title, y_title, width=480, height=280):
        _available = [c for c in cols_map if c in df.columns]
        if not _available:
            return None
        _df = df[[id_col] + _available].copy()
        _melted = _df.melt(id_vars=id_col, var_name="page_type", value_name=value_col)
        _melted["page_type"] = _melted["page_type"].map(cols_map)
        return (
            _alt.Chart(_melted)
            .mark_line(point=True)
            .encode(
                x=_alt.X(f"{id_col}:N", title="æœˆ"),
                y=_alt.Y(f"{value_col}:Q", title=y_title, scale=_alt.Scale(zero=True)),
                color=_alt.Color("page_type:N", title="ãƒšãƒ¼ã‚¸ç¨®åˆ¥"),
                tooltip=[id_col, "page_type", value_col],
            )
            .properties(title=title, width=width, height=height)
        )

    # ========== 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ ==========
    _outputs.append(mo.md("### ãƒ¦ãƒ¼ã‚¶ãƒ¼(UU)å˜ä½"))
    if len(df_ga_page_rate_user) > 0:
        # UUæ•° + ç‡ã®åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°
        _uu_main = {f"uu_{k}": v for k, v in _main_labels.items()}
        _rate_main = {f"rate_{k}": v for k, v in _main_labels.items()}
        _uu_sub = {f"uu_{k}": v for k, v in _sub_labels.items()}
        _rate_sub = {f"rate_{k}": v for k, v in _sub_labels.items()}

        # ãƒ¡ã‚¤ãƒ³: æ•° | ç‡
        _c_uu_main = _make_line_chart(df_ga_page_rate_user, _uu_main, "month", "uu", "ä¸»è¦æ©Ÿèƒ½ UUæ•°æ¨ç§»", "UUæ•°")
        _c_rate_main = _make_line_chart(df_ga_page_rate_user, _rate_main, "month", "rate", "ä¸»è¦æ©Ÿèƒ½ UUç‡æ¨ç§» (%)", "ã‚¢ã‚¯ã‚»ã‚¹ç‡ (%)")
        if _c_uu_main and _c_rate_main:
            _outputs.append(_c_uu_main | _c_rate_main)

        # ã‚µãƒ–: æ•° | ç‡
        _c_uu_sub = _make_line_chart(df_ga_page_rate_user, _uu_sub, "month", "uu", "åˆ†æãƒ»è¨­å®šç³» UUæ•°æ¨ç§»", "UUæ•°")
        _c_rate_sub = _make_line_chart(df_ga_page_rate_user, _rate_sub, "month", "rate", "åˆ†æãƒ»è¨­å®šç³» UUç‡æ¨ç§» (%)", "ã‚¢ã‚¯ã‚»ã‚¹ç‡ (%)")
        if _c_uu_sub and _c_rate_sub:
            _outputs.append(_c_uu_sub | _c_rate_sub)

        _outputs.append(mo.ui.table(df_ga_page_rate_user, pagination=False))
    else:
        _outputs.append(mo.md("*ãƒ¦ãƒ¼ã‚¶ãƒ¼å˜ä½ GAãƒ‡ãƒ¼ã‚¿ãªã—*"))

    # ========== 2. ä¼æ¥­(org_id)å˜ä½ ==========
    _outputs.append(mo.md("### ä¼æ¥­(org_id)å˜ä½"))
    if len(df_ga_page_rate_org) > 0:
        _org_main = {f"org_{k}": v for k, v in _main_labels.items()}
        _org_rate_main = {f"rate_{k}": v for k, v in _main_labels.items()}
        _org_sub = {f"org_{k}": v for k, v in _sub_labels.items()}
        _org_rate_sub = {f"rate_{k}": v for k, v in _sub_labels.items()}

        _c_org_main = _make_line_chart(df_ga_page_rate_org, _org_main, "month", "count", "ä¸»è¦æ©Ÿèƒ½ ä¼æ¥­æ•°æ¨ç§»", "ä¼æ¥­æ•°")
        _c_org_rate = _make_line_chart(df_ga_page_rate_org, _org_rate_main, "month", "rate", "ä¸»è¦æ©Ÿèƒ½ ä¼æ¥­ç‡æ¨ç§» (%)", "ã‚¢ã‚¯ã‚»ã‚¹ä¼æ¥­ç‡ (%)")
        if _c_org_main and _c_org_rate:
            _outputs.append(_c_org_main | _c_org_rate)

        _c_org_sub = _make_line_chart(df_ga_page_rate_org, _org_sub, "month", "count", "åˆ†æãƒ»è¨­å®šç³» ä¼æ¥­æ•°æ¨ç§»", "ä¼æ¥­æ•°")
        _c_org_rate_sub = _make_line_chart(df_ga_page_rate_org, _org_rate_sub, "month", "rate", "åˆ†æãƒ»è¨­å®šç³» ä¼æ¥­ç‡æ¨ç§» (%)", "ã‚¢ã‚¯ã‚»ã‚¹ä¼æ¥­ç‡ (%)")
        if _c_org_sub and _c_org_rate_sub:
            _outputs.append(_c_org_sub | _c_org_rate_sub)

        _outputs.append(mo.ui.table(df_ga_page_rate_org, pagination=False))
    else:
        _outputs.append(mo.md("*ä¼æ¥­å˜ä½ GAãƒ‡ãƒ¼ã‚¿ãªã—*"))

    # ========== 3. è§£ç´„ vs å¥‘ç´„ä¸­ æ¯”è¼ƒ ==========
    _outputs.append(mo.md("### è§£ç´„ä¼æ¥­ vs å¥‘ç´„ä¸­ä¼æ¥­"))
    if len(df_ga_page_rate_churn) > 0:
        # è§£ç´„æ¸ˆã¿ãƒ»å¥‘ç´„ä¸­ã®ã¿ã«çµã‚‹ï¼ˆã€Œä¸æ˜ã€ã¯é™¤å¤–ï¼‰
        _df_churn_viz = df_ga_page_rate_churn[
            df_ga_page_rate_churn["status"].isin(["è§£ç´„æ¸ˆã¿", "å¥‘ç´„ä¸­"])
        ].copy()

        if len(_df_churn_viz) > 0:
            _outputs.append(mo.md(f"è§£ç´„æ¸ˆã¿: {_df_churn_viz[_df_churn_viz['status']=='è§£ç´„æ¸ˆã¿']['total_orgs'].max() if len(_df_churn_viz[_df_churn_viz['status']=='è§£ç´„æ¸ˆã¿']) > 0 else 0} ç¤¾, "
                                  f"å¥‘ç´„ä¸­: {_df_churn_viz[_df_churn_viz['status']=='å¥‘ç´„ä¸­']['total_orgs'].max() if len(_df_churn_viz[_df_churn_viz['status']=='å¥‘ç´„ä¸­']) > 0 else 0} ç¤¾"))

            # ä¸»è¦4ã‚«ãƒ†ã‚´ãƒªã®ã¿: ç‡æ¯”è¼ƒ
            _rate_churn_cols = [f"rate_{k}" for k in _main_labels]
            _rate_churn_available = [c for c in _rate_churn_cols if c in _df_churn_viz.columns]
            if _rate_churn_available:
                _df_cr = _df_churn_viz[["month", "status"] + _rate_churn_available].copy()
                _label_map = {f"rate_{k}": v for k, v in _main_labels.items()}
                _df_cr_melted = _df_cr.melt(id_vars=["month", "status"], var_name="page_type", value_name="rate")
                _df_cr_melted["page_type"] = _df_cr_melted["page_type"].map(_label_map)

                _chart_churn_rate = (
                    _alt.Chart(_df_cr_melted)
                    .mark_line(point=True)
                    .encode(
                        x=_alt.X("month:N", title="æœˆ"),
                        y=_alt.Y("rate:Q", title="ã‚¢ã‚¯ã‚»ã‚¹ç‡ (%)", scale=_alt.Scale(zero=True)),
                        color=_alt.Color("page_type:N", title="ãƒšãƒ¼ã‚¸ç¨®åˆ¥"),
                        strokeDash=_alt.StrokeDash("status:N", title="å¥‘ç´„çŠ¶æ…‹"),
                        tooltip=["month", "status", "page_type", "rate"],
                    )
                    .properties(title="è§£ç´„ vs å¥‘ç´„ä¸­: ã‚¢ã‚¯ã‚»ã‚¹ç‡æ¯”è¼ƒ", width=480, height=300)
                )

                # ä¸»è¦4ã‚«ãƒ†ã‚´ãƒª: æ•°æ¯”è¼ƒ
                _org_churn_cols = [f"org_{k}" for k in _main_labels]
                _org_churn_available = [c for c in _org_churn_cols if c in _df_churn_viz.columns]
                _df_cc = _df_churn_viz[["month", "status"] + _org_churn_available].copy()
                _count_label_map = {f"org_{k}": v for k, v in _main_labels.items()}
                _df_cc_melted = _df_cc.melt(id_vars=["month", "status"], var_name="page_type", value_name="count")
                _df_cc_melted["page_type"] = _df_cc_melted["page_type"].map(_count_label_map)

                _chart_churn_count = (
                    _alt.Chart(_df_cc_melted)
                    .mark_line(point=True)
                    .encode(
                        x=_alt.X("month:N", title="æœˆ"),
                        y=_alt.Y("count:Q", title="ä¼æ¥­æ•°"),
                        color=_alt.Color("page_type:N", title="ãƒšãƒ¼ã‚¸ç¨®åˆ¥"),
                        strokeDash=_alt.StrokeDash("status:N", title="å¥‘ç´„çŠ¶æ…‹"),
                        tooltip=["month", "status", "page_type", "count"],
                    )
                    .properties(title="è§£ç´„ vs å¥‘ç´„ä¸­: ä¼æ¥­æ•°æ¯”è¼ƒ", width=480, height=300)
                )
                _outputs.append(_chart_churn_rate | _chart_churn_count)

            _outputs.append(mo.ui.table(_df_churn_viz, pagination=False))
        else:
            _outputs.append(mo.md("*è§£ç´„/å¥‘ç´„ä¸­ã®ç´ã¥ã‘ä¼æ¥­ãŒä¸è¶³*"))
    else:
        _outputs.append(mo.md("*ä¼æ¥­Ã—è§£ç´„ãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-d. CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¨ç§»ï¼ˆå…¨ä½“ï¼‰
    """)
    return


@app.cell
def _(SF_SCHEMA, query_sf):
    _csv_dl_query = f"""
    SELECT
        DATE_TRUNC('month', dl.CREATEDAT) AS MONTH,
        COUNT(DISTINCT dl.ID) AS DOWNLOAD_COUNT,
        COUNT(DISTINCT dl.USERORGRELATIONID) AS USER_COUNT
    FROM {SF_SCHEMA}.CSVDOWNLOADLOG dl
    WHERE dl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
    GROUP BY MONTH
    ORDER BY MONTH
    """
    df_csv_trend_all = query_sf(_csv_dl_query)
    return (df_csv_trend_all,)


@app.cell
def _(df_csv_trend_all, mo):
    import altair as _alt

    _csv_outputs = []
    _csv_outputs.append(mo.md("### CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¨ç§»ï¼ˆå…¨ä½“ï¼‰"))
    if len(df_csv_trend_all) > 0:
        _df_csv = df_csv_trend_all.copy()
        _df_csv.columns = [c.upper() for c in _df_csv.columns]
        _df_csv["MONTH"] = _df_csv["MONTH"].astype(str).str[:7]

        _chart_csv_count = (
            _alt.Chart(_df_csv)
            .mark_bar(color="darkorange", opacity=0.7)
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("DOWNLOAD_COUNT:Q", title="DLæ•°"),
                tooltip=["MONTH", "DOWNLOAD_COUNT", "USER_COUNT"],
            )
            .properties(title="CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ•°", width=400, height=280)
        )
        _chart_csv_user = (
            _alt.Chart(_df_csv)
            .mark_line(point=True, color="crimson")
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("USER_COUNT:Q", title="ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", scale=_alt.Scale(zero=False)),
                tooltip=["MONTH", "DOWNLOAD_COUNT", "USER_COUNT"],
            )
            .properties(title="CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", width=400, height=280)
        )
        _csv_outputs.append(_chart_csv_count | _chart_csv_user)
        _csv_outputs.append(mo.ui.table(_df_csv, pagination=False))
    else:
        _csv_outputs.append(mo.md("*CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_csv_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-e. Memoæ´»å‹•æ¨ç§»ï¼ˆå…¨ä½“ï¼‰
    """)
    return


@app.cell
def _(SF_SCHEMA, query_sf):
    _memo_trend_query = f"""
    SELECT
        DATE_TRUNC('month', m.CREATEDAT) AS MONTH,
        COUNT(*) AS MEMO_COUNT,
        COUNT(DISTINCT m.USERORGRELATIONID) AS USER_COUNT
    FROM {SF_SCHEMA}.MEMO m
    WHERE m.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
    GROUP BY MONTH
    ORDER BY MONTH
    """
    df_memo_trend_all = query_sf(_memo_trend_query)

    _memo_priority_query = f"""
    SELECT
        COALESCE(PRIORITY, 'ãªã—') AS PRIORITY,
        COUNT(*) AS COUNT
    FROM {SF_SCHEMA}.MEMO
    WHERE CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
    GROUP BY PRIORITY
    ORDER BY COUNT DESC
    """
    df_memo_priority = query_sf(_memo_priority_query)
    return df_memo_priority, df_memo_trend_all


@app.cell
def _(df_memo_priority, df_memo_trend_all, mo):
    import altair as _alt

    _memo_outputs = []
    _memo_outputs.append(mo.md("### Memoæ´»å‹•æ¨ç§»ï¼ˆå…¨ä½“ï¼‰"))
    if len(df_memo_trend_all) > 0:
        _df_memo = df_memo_trend_all.copy()
        _df_memo.columns = [c.upper() for c in _df_memo.columns]
        _df_memo["MONTH"] = _df_memo["MONTH"].astype(str).str[:7]

        _chart_memo_count = (
            _alt.Chart(_df_memo)
            .mark_bar(color="mediumpurple", opacity=0.7)
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("MEMO_COUNT:Q", title="Memoä»¶æ•°"),
                tooltip=["MONTH", "MEMO_COUNT", "USER_COUNT"],
            )
            .properties(title="Memoä½œæˆæ•°", width=400, height=280)
        )
        _chart_memo_user = (
            _alt.Chart(_df_memo)
            .mark_line(point=True, color="darkviolet")
            .encode(
                x=_alt.X("MONTH:N", title="æœˆ"),
                y=_alt.Y("USER_COUNT:Q", title="ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", scale=_alt.Scale(zero=False)),
                tooltip=["MONTH", "MEMO_COUNT", "USER_COUNT"],
            )
            .properties(title="Memoä½œæˆãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°", width=400, height=280)
        )
        _memo_outputs.append(_chart_memo_count | _chart_memo_user)
        _memo_outputs.append(mo.ui.table(_df_memo, pagination=False))
    else:
        _memo_outputs.append(mo.md("*Memoãƒ‡ãƒ¼ã‚¿ãªã—*"))

    if len(df_memo_priority) > 0:
        _memo_outputs.append(mo.md("**Priorityåˆ†å¸ƒ**"))
        _df_pri = df_memo_priority.copy()
        _df_pri.columns = [c.upper() for c in _df_pri.columns]
        _memo_outputs.append(mo.ui.table(_df_pri, pagination=False))

    mo.vstack(_memo_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5-f. ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ãƒãƒ«åˆ†æ

    ç™»éŒ² â†’ ä¼æ¥­æ¤œç´¢ â†’ ä¼æ¥­è©³ç´° â†’ ãƒªã‚¹ãƒˆ â†’ ãƒ¡ãƒ¢ â†’ CRMæ´»ç”¨(Negotiation+Lead+LeadImport) ã®6ã‚¹ãƒ†ãƒƒãƒ—ãƒ•ã‚¡ãƒãƒ«ã€‚
    åˆ†æ¯: GAã§ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒ1ä»¶ä»¥ä¸Šã‚ã‚‹ä¼æ¥­/ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ã¿ã€‚è§£ç´„ vs å¥‘ç´„ä¸­ã®æ¯”è¼ƒã‚ã‚Šã€‚
    """)
    return


@app.cell
def _(GA_DATASET_ID, query_bq):
    # GA: ä¼æ¥­(org_id)å˜ä½ ã‚¹ãƒ†ãƒƒãƒ—1-3
    # å®Ÿéš›ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³: /companies(ä¸€è¦§=æ¤œç´¢), /companies/[id](ä¼æ¥­è©³ç´°)
    _funnel_org_query = f"""
    SELECT
        org_id,
        1 AS step_login,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies') THEN 1 ELSE 0 END) AS step_search,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN 1 ELSE 0 END) AS step_company
    FROM (
        SELECT
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
        FROM `{GA_DATASET_ID}.events_*`
        WHERE _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY))
          AND event_name = 'page_view'
    )
    WHERE org_id IS NOT NULL
    GROUP BY org_id
    """
    df_funnel_ga = query_bq(_funnel_org_query)

    # GA: ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ(user)å˜ä½ ã‚¹ãƒ†ãƒƒãƒ—1-3
    _funnel_user_query = f"""
    SELECT
        user_pseudo_id,
        org_id,
        1 AS step_login,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies') THEN 1 ELSE 0 END) AS step_search,
        MAX(CASE WHEN REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') THEN 1 ELSE 0 END) AS step_company
    FROM (
        SELECT
            user_pseudo_id,
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
        FROM `{GA_DATASET_ID}.events_*`
        WHERE _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 90 DAY))
          AND event_name = 'page_view'
    )
    WHERE org_id IS NOT NULL
    GROUP BY user_pseudo_id, org_id
    """
    df_funnel_ga_user = query_bq(_funnel_user_query)
    return df_funnel_ga, df_funnel_ga_user


@app.cell
def _(SF_SCHEMA, df_funnel_ga, pd, query_sf):
    # Snowflake: ã‚¹ãƒ†ãƒƒãƒ—4(ãƒªã‚¹ãƒˆ) + 5(Memo) + 6(CRM: Negotiation/Lead/LeadImport) + ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•°
    df_funnel_sf = pd.DataFrame()
    df_account_count_per_org = pd.DataFrame()

    _funnel_orgids = df_funnel_ga["org_id"].dropna().unique().tolist() if len(df_funnel_ga) > 0 else []

    if _funnel_orgids:
        _orgids_sql = ",".join([f"'{o}'" for o in _funnel_orgids[:1000]])

        _funnel_sf_query = f"""
        WITH base_uorid AS (
            SELECT CAST(u.ORGID AS STRING) AS ORGID, ur.ID AS UORID
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
            WHERE CAST(u.ORGID AS STRING) IN ({_orgids_sql})
        ),
        list_check AS (
            SELECT DISTINCT b.ORGID
            FROM base_uorid b
            WHERE EXISTS (SELECT 1 FROM {SF_SCHEMA}.COMPANYLIST cl WHERE cl.USERORGRELATIONID = b.UORID)
               OR EXISTS (SELECT 1 FROM {SF_SCHEMA}.PEOPLELIST pl WHERE pl.USERORGRELATIONID = b.UORID)
        ),
        memo_check AS (
            SELECT DISTINCT b.ORGID
            FROM base_uorid b
            WHERE EXISTS (SELECT 1 FROM {SF_SCHEMA}.MEMO m WHERE m.USERORGRELATIONID = b.UORID)
        ),
        crm_check AS (
            SELECT DISTINCT d.ORGID
            FROM (SELECT DISTINCT ORGID FROM base_uorid) d
            WHERE EXISTS (SELECT 1 FROM {SF_SCHEMA}.NEGOTIATION n WHERE n.ORGANIZATIONID = d.ORGID)
               OR EXISTS (SELECT 1 FROM {SF_SCHEMA}.LEAD l WHERE l.ORGANIZATIONID = d.ORGID)
               OR EXISTS (
                   SELECT 1 FROM base_uorid b
                   JOIN {SF_SCHEMA}.LEADIMPORTEVENT lie ON lie.USERORGRELATIONID = b.UORID
                   WHERE b.ORGID = d.ORGID
               )
        )
        SELECT
            d.ORGID,
            CASE WHEN lc.ORGID IS NOT NULL THEN 1 ELSE 0 END AS STEP_LIST,
            CASE WHEN mc.ORGID IS NOT NULL THEN 1 ELSE 0 END AS STEP_MEMO,
            CASE WHEN cc.ORGID IS NOT NULL THEN 1 ELSE 0 END AS STEP_CRM
        FROM (SELECT DISTINCT ORGID FROM base_uorid) d
        LEFT JOIN list_check lc ON d.ORGID = lc.ORGID
        LEFT JOIN memo_check mc ON d.ORGID = mc.ORGID
        LEFT JOIN crm_check cc ON d.ORGID = cc.ORGID
        """
        try:
            df_funnel_sf = query_sf(_funnel_sf_query)
        except Exception:
            df_funnel_sf = pd.DataFrame()

        _account_count_query = f"""
        SELECT
            CAST(ORGANIZATIONID AS STRING) AS ORGID,
            COUNT(DISTINCT ID) AS ACCOUNT_COUNT
        FROM {SF_SCHEMA}.USERORGRELATION
        WHERE CAST(ORGANIZATIONID AS STRING) IN ({_orgids_sql})
        GROUP BY ORGANIZATIONID
        """
        try:
            df_account_count_per_org = query_sf(_account_count_query)
        except Exception:
            df_account_count_per_org = pd.DataFrame()

    return df_account_count_per_org, df_funnel_sf


@app.cell
def _(df_account_count_per_org, df_churn, df_funnel_ga, df_funnel_sf, df_id_mapping, pd):
    # ãƒ•ã‚¡ãƒãƒ«ãƒ‡ãƒ¼ã‚¿çµ±åˆ + ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå±æ€§ä»˜ä¸
    df_funnel_org = pd.DataFrame()

    if len(df_funnel_ga) > 0:
        df_funnel_org = df_funnel_ga.copy()

        # SF steps join
        if len(df_funnel_sf) > 0:
            _sf = df_funnel_sf.copy()
            _sf.columns = [c.upper() for c in _sf.columns]
            df_funnel_org = df_funnel_org.merge(_sf, left_on="org_id", right_on="ORGID", how="left")

        for _col in ["STEP_LIST", "STEP_MEMO", "STEP_CRM"]:
            _lc = _col.lower()
            _src = _col if _col in df_funnel_org.columns else _lc
            if _src in df_funnel_org.columns:
                df_funnel_org[_lc] = df_funnel_org[_src].fillna(0).astype(int)
            else:
                df_funnel_org[_lc] = 0

        # ä¼æ¥­å±æ€§ join
        if len(df_id_mapping) > 0:
            _attr_cols = [c for c in ["ORGID", "EMPLOYEE_COUNT", "INDUSTRY_ID", "PREFID", "COMPNO"] if c in df_id_mapping.columns]
            _df_attr = df_id_mapping[_attr_cols].drop_duplicates(subset=["ORGID"])
            df_funnel_org = df_funnel_org.merge(_df_attr, left_on="org_id", right_on="ORGID", how="left", suffixes=("", "_attr"))

        # ãƒãƒ£ãƒ¼ãƒ³ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ join
        if len(df_churn) > 0 and "COMPNO" in df_funnel_org.columns:
            _churn_cols = [c for c in ["COMPNO", "status"] if c in df_churn.columns]
            if _churn_cols:
                df_funnel_org = df_funnel_org.merge(
                    df_churn[_churn_cols].drop_duplicates(subset=["COMPNO"]),
                    on="COMPNO", how="left",
                )

        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆæ•° join
        if len(df_account_count_per_org) > 0:
            _acc = df_account_count_per_org.copy()
            _acc.columns = [c.upper() for c in _acc.columns]
            df_funnel_org = df_funnel_org.merge(
                _acc[["ORGID", "ACCOUNT_COUNT"]], left_on="org_id", right_on="ORGID",
                how="left", suffixes=("", "_acc"),
            )

        # --- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ—è¿½åŠ  ---
        def _emp_bucket(val):
            if pd.isna(val) or val <= 0: return "ä¸æ˜"
            if val <= 10: return "1-10äºº"
            if val <= 50: return "11-50äºº"
            if val <= 200: return "51-200äºº"
            if val <= 1000: return "201-1000äºº"
            return "1001äºº+"

        def _region(val):
            try: v = int(val)
            except (TypeError, ValueError): return "ä¸æ˜"
            if v in (11, 12, 13, 14): return "é¦–éƒ½åœ"
            if v in (26, 27, 28): return "é–¢è¥¿"
            if v in (22, 23): return "ä¸­éƒ¨"
            return "ãã®ä»–åœ°æ–¹"

        def _acc_bucket(val):
            if pd.isna(val) or val <= 0: return "ä¸æ˜"
            if val == 1: return "1"
            if val <= 5: return "2-5"
            if val <= 10: return "6-10"
            return "11+"

        df_funnel_org["emp_bucket"] = df_funnel_org["EMPLOYEE_COUNT"].apply(_emp_bucket) if "EMPLOYEE_COUNT" in df_funnel_org.columns else "ä¸æ˜"
        df_funnel_org["region"] = df_funnel_org["PREFID"].apply(_region) if "PREFID" in df_funnel_org.columns else "ä¸æ˜"
        df_funnel_org["account_bucket"] = df_funnel_org["ACCOUNT_COUNT"].apply(_acc_bucket) if "ACCOUNT_COUNT" in df_funnel_org.columns else "ä¸æ˜"

        if "INDUSTRY_ID" in df_funnel_org.columns:
            _top_ind = df_funnel_org["INDUSTRY_ID"].value_counts().head(10).index.tolist()
            df_funnel_org["industry_group"] = df_funnel_org["INDUSTRY_ID"].apply(lambda x: str(x) if x in _top_ind else "ãã®ä»–")
        else:
            df_funnel_org["industry_group"] = "ä¸æ˜"

        if "status" not in df_funnel_org.columns:
            df_funnel_org["status"] = "ä¸æ˜"
        df_funnel_org["status"] = df_funnel_org["status"].fillna("ä¸æ˜")

    return (df_funnel_org,)


@app.cell
def _(df_funnel_ga_user, df_funnel_org, mo, pd):
    import altair as _alt

    _funnel_outputs = []

    # ã‚¹ãƒ†ãƒƒãƒ—åã®å®šç¾©ï¼ˆ6ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    _step_keys = [
        "1. ç™»éŒ²", "2. ä¼æ¥­æ¤œç´¢", "3. ä¼æ¥­è©³ç´°",
        "4. ãƒªã‚¹ãƒˆ", "5. ãƒ¡ãƒ¢", "6. CRMæ´»ç”¨",
    ]

    def _get_step_col(df, name):
        return name if name in df.columns else name.upper()

    def _sum_step(df, col_name):
        _c = _get_step_col(df, col_name)
        return int(df[_c].sum()) if _c in df.columns else 0

    if len(df_funnel_org) > 0:
        # --- ä¼æ¥­å˜ä½ãƒ•ã‚¡ãƒãƒ«ï¼ˆå…¨ä½“ï¼‰ ---
        _total_org = len(df_funnel_org)
        _steps_org = {
            "1. ç™»éŒ²": int(df_funnel_org["step_login"].sum()),
            "2. ä¼æ¥­æ¤œç´¢": int(df_funnel_org["step_search"].sum()),
            "3. ä¼æ¥­è©³ç´°": int(df_funnel_org["step_company"].sum()),
            "4. ãƒªã‚¹ãƒˆ": _sum_step(df_funnel_org, "step_list"),
            "5. ãƒ¡ãƒ¢": _sum_step(df_funnel_org, "step_memo"),
            "6. CRMæ´»ç”¨": _sum_step(df_funnel_org, "step_crm"),
        }
        _df_funnel_chart = pd.DataFrame([
            {"step": k, "count": v, "rate": round(v / _total_org * 100, 1)}
            for k, v in _steps_org.items()
        ])

        _funnel_outputs.append(mo.md(f"### ä¼æ¥­å˜ä½ãƒ•ã‚¡ãƒãƒ«ï¼ˆåˆ†æ¯: {_total_org:,} ç¤¾ï¼‰"))
        _chart_funnel_org = (
            _alt.Chart(_df_funnel_chart)
            .mark_bar()
            .encode(
                x=_alt.X("rate:Q", title="é€šéç‡ (%)", scale=_alt.Scale(domain=[0, 100])),
                y=_alt.Y("step:N", title="ã‚¹ãƒ†ãƒƒãƒ—", sort=_step_keys),
                color=_alt.Color("step:N", legend=None),
                tooltip=["step", "count", "rate"],
            )
            .properties(title="ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ãƒãƒ«ï¼ˆä¼æ¥­å˜ä½ãƒ»å…¨ä½“ï¼‰", width=600, height=280)
        )
        _funnel_outputs.append(_chart_funnel_org)
        _funnel_outputs.append(mo.ui.table(_df_funnel_chart, pagination=False))

        # --- è§£ç´„ vs å¥‘ç´„ä¸­ æ¯”è¼ƒãƒ•ã‚¡ãƒãƒ« ---
        if "status" in df_funnel_org.columns:
            _churn_compare_data = []
            for _status_val in ["è§£ç´„æ¸ˆã¿", "å¥‘ç´„ä¸­"]:
                _grp = df_funnel_org[df_funnel_org["status"] == _status_val]
                _n = len(_grp)
                if _n == 0:
                    continue
                _grp_steps = {
                    "1. ç™»éŒ²": int(_grp["step_login"].sum()),
                    "2. ä¼æ¥­æ¤œç´¢": int(_grp["step_search"].sum()),
                    "3. ä¼æ¥­è©³ç´°": int(_grp["step_company"].sum()),
                    "4. ãƒªã‚¹ãƒˆ": _sum_step(_grp, "step_list"),
                    "5. ãƒ¡ãƒ¢": _sum_step(_grp, "step_memo"),
                    "6. CRMæ´»ç”¨": _sum_step(_grp, "step_crm"),
                }
                for _sk, _sv in _grp_steps.items():
                    _churn_compare_data.append({
                        "status": _status_val,
                        "step": _sk,
                        "count": _sv,
                        "total": _n,
                        "rate": round(_sv / _n * 100, 1),
                    })

            if _churn_compare_data:
                _df_cc = pd.DataFrame(_churn_compare_data)
                _funnel_outputs.append(mo.md("### è§£ç´„ vs å¥‘ç´„ä¸­ ãƒ•ã‚¡ãƒãƒ«æ¯”è¼ƒ"))

                _chart_cc = (
                    _alt.Chart(_df_cc)
                    .mark_bar()
                    .encode(
                        x=_alt.X("rate:Q", title="é€šéç‡ (%)", scale=_alt.Scale(domain=[0, 100])),
                        y=_alt.Y("step:N", title="ã‚¹ãƒ†ãƒƒãƒ—", sort=_step_keys),
                        color=_alt.Color("status:N", title="ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
                            scale=_alt.Scale(domain=["å¥‘ç´„ä¸­", "è§£ç´„æ¸ˆã¿"], range=["#4c78a8", "#e45756"])),
                        tooltip=["status", "step", "count", "total", "rate"],
                        xOffset="status:N",
                    )
                    .properties(title="è§£ç´„ vs å¥‘ç´„ä¸­ ãƒ•ã‚¡ãƒãƒ«æ¯”è¼ƒ", width=700, height=300)
                )
                _funnel_outputs.append(_chart_cc)

                # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
                _df_cc_pivot = _df_cc.pivot_table(
                    index="status", columns="step", values="rate", aggfunc="first"
                )
                _ordered = [s for s in _step_keys if s in _df_cc_pivot.columns]
                _df_cc_pivot = _df_cc_pivot[_ordered].reset_index()
                _funnel_outputs.append(mo.md("**é€šéç‡æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ« (%)**"))
                _funnel_outputs.append(mo.ui.table(_df_cc_pivot, pagination=False))

                # ä»¶æ•°ãƒ†ãƒ¼ãƒ–ãƒ«
                _df_cc_count = _df_cc.pivot_table(
                    index="status", columns="step", values="count", aggfunc="first"
                )
                _df_cc_count = _df_cc_count[_ordered].reset_index()
                _df_cc_count.insert(1, "ä¼æ¥­æ•°", _df_cc.groupby("status")["total"].first().values)
                _funnel_outputs.append(mo.md("**ä»¶æ•°ãƒ†ãƒ¼ãƒ–ãƒ«**"))
                _funnel_outputs.append(mo.ui.table(_df_cc_count, pagination=False))

        # --- ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå˜ä½ãƒ•ã‚¡ãƒãƒ« (GA Step 1-3) ---
        if len(df_funnel_ga_user) > 0:
            _total_user = len(df_funnel_ga_user)
            _user_keys = ["1. ç™»éŒ²", "2. ä¼æ¥­æ¤œç´¢", "3. ä¼æ¥­è©³ç´°"]
            _steps_user = {
                "1. ç™»éŒ²": int(df_funnel_ga_user["step_login"].sum()),
                "2. ä¼æ¥­æ¤œç´¢": int(df_funnel_ga_user["step_search"].sum()),
                "3. ä¼æ¥­è©³ç´°": int(df_funnel_ga_user["step_company"].sum()),
            }
            _df_funnel_user = pd.DataFrame([
                {"step": k, "count": v, "rate": round(v / _total_user * 100, 1)}
                for k, v in _steps_user.items()
            ])
            _funnel_outputs.append(mo.md(f"### ã‚¢ã‚«ã‚¦ãƒ³ãƒˆå˜ä½ãƒ•ã‚¡ãƒãƒ«ï¼ˆåˆ†æ¯: {_total_user:,} ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€GA Step 1-3ï¼‰"))
            _chart_funnel_user = (
                _alt.Chart(_df_funnel_user)
                .mark_bar(color="coral")
                .encode(
                    x=_alt.X("rate:Q", title="é€šéç‡ (%)", scale=_alt.Scale(domain=[0, 100])),
                    y=_alt.Y("step:N", title="ã‚¹ãƒ†ãƒƒãƒ—", sort=_user_keys),
                    tooltip=["step", "count", "rate"],
                )
                .properties(title="ãƒãƒªãƒ¥ãƒ¼ãƒ•ã‚¡ãƒãƒ«ï¼ˆã‚¢ã‚«ã‚¦ãƒ³ãƒˆå˜ä½ï¼‰", width=600, height=180)
            )
            _funnel_outputs.append(_chart_funnel_user)
            _funnel_outputs.append(mo.ui.table(_df_funnel_user, pagination=False))
    else:
        _funnel_outputs.append(mo.md("*ãƒ•ã‚¡ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ãªã—*"))

    mo.vstack(_funnel_outputs)
    return


@app.cell
def _(mo):
    funnel_segment_selector = mo.ui.dropdown(
        options={
            "å¾“æ¥­å“¡è¦æ¨¡åˆ¥": "emp_bucket",
            "æ¥­ç¨®åˆ¥": "industry_group",
            "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆäººæ•°åˆ¥": "account_bucket",
            "åœ°åŸŸåˆ¥": "region",
            "å¥‘ç´„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¥": "status",
        },
        label="ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ‡ã‚Šæ›¿ãˆ",
        value="emp_bucket",
    )
    funnel_segment_selector
    return (funnel_segment_selector,)


@app.cell
def _(df_funnel_org, funnel_segment_selector, mo, pd):
    import altair as _alt

    _seg_outputs = []

    if len(df_funnel_org) > 0 and funnel_segment_selector.value:
        _seg_col = funnel_segment_selector.value
        _seg_label = {
            "emp_bucket": "å¾“æ¥­å“¡è¦æ¨¡",
            "industry_group": "æ¥­ç¨®",
            "account_bucket": "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆäººæ•°",
            "region": "åœ°åŸŸ",
            "status": "å¥‘ç´„ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
        }.get(_seg_col, _seg_col)

        _seg_outputs.append(mo.md(f"### ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ•ã‚¡ãƒãƒ«: {_seg_label}"))

        _step_list_col = "step_list" if "step_list" in df_funnel_org.columns else "STEP_LIST"
        _step_memo_col = "step_memo" if "step_memo" in df_funnel_org.columns else "STEP_MEMO"
        _step_crm_col = "step_crm" if "step_crm" in df_funnel_org.columns else "STEP_CRM"
        _step_cols = ["step_login", "step_search", "step_company", _step_list_col, _step_memo_col, _step_crm_col]
        _step_cols = [c for c in _step_cols if c in df_funnel_org.columns]
        _step_labels = {
            "step_login": "1.ç™»éŒ²",
            "step_search": "2.ä¼æ¥­æ¤œç´¢",
            "step_company": "3.ä¼æ¥­è©³ç´°",
            _step_list_col: "4.ãƒªã‚¹ãƒˆ",
            _step_memo_col: "5.ãƒ¡ãƒ¢",
            _step_crm_col: "6.CRMæ´»ç”¨",
        }

        _seg_data = []
        for _seg_val, _grp in df_funnel_org.groupby(_seg_col):
            _n = len(_grp)
            if _n < 3:
                continue
            for _sc in _step_cols:
                _cnt = int(_grp[_sc].sum())
                _seg_data.append({
                    "segment": str(_seg_val),
                    "step": _step_labels.get(_sc, _sc),
                    "count": _cnt,
                    "total": _n,
                    "rate": round(_cnt / _n * 100, 1),
                })

        if _seg_data:
            _df_seg = pd.DataFrame(_seg_data)

            _chart_seg = (
                _alt.Chart(_df_seg)
                .mark_bar()
                .encode(
                    x=_alt.X("rate:Q", title="é€šéç‡ (%)", scale=_alt.Scale(domain=[0, 100])),
                    y=_alt.Y("step:N", title="ã‚¹ãƒ†ãƒƒãƒ—", sort=[v for v in _step_labels.values()]),
                    color=_alt.Color("segment:N", title=_seg_label),
                    tooltip=["segment", "step", "count", "total", "rate"],
                    xOffset="segment:N",
                )
                .properties(title=f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ•ã‚¡ãƒãƒ«: {_seg_label}", width=700, height=350)
            )
            _seg_outputs.append(_chart_seg)

            # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
            _df_pivot = _df_seg.pivot_table(
                index="segment", columns="step", values="rate", aggfunc="first"
            )
            _ordered_steps = [v for v in _step_labels.values() if v in _df_pivot.columns]
            _df_pivot = _df_pivot[_ordered_steps].reset_index()
            _seg_outputs.append(mo.md("**é€šéç‡ãƒ†ãƒ¼ãƒ–ãƒ« (%)**"))
            _seg_outputs.append(mo.ui.table(_df_pivot, pagination=False))

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ä¼æ¥­æ•°
            _df_n = df_funnel_org.groupby(_seg_col).size().reset_index(name="ä¼æ¥­æ•°")
            _df_n.columns = [_seg_label, "ä¼æ¥­æ•°"]
            _seg_outputs.append(mo.md("**ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ä¼æ¥­æ•°**"))
            _seg_outputs.append(mo.ui.table(_df_n, pagination=False))
        else:
            _seg_outputs.append(mo.md("*ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“*"))
    else:
        _seg_outputs.append(mo.md("*ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„*"))

    mo.vstack(_seg_outputs)
    return


@app.cell
def _(df_list_summary, mo):
    # Listé›†è¨ˆçµæœï¼ˆè»½é‡ãƒ»1è¡Œã‚µãƒãƒªãƒ¼è¡¨ç¤ºã®ã¿ã€è©³ç´°åˆ†æã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³10ã§ï¼‰
    _cl = (df_list_summary['companylist_count'] > 0).sum() if len(df_list_summary) > 0 else 0
    _pl = (df_list_summary['peoplelist_count'] > 0).sum() if len(df_list_summary) > 0 else 0
    mo.md(f"**Listä¿æœ‰çŠ¶æ³**: CompanyListä¿æœ‰ {_cl}ç¤¾ / PeopleListä¿æœ‰ {_pl}ç¤¾ï¼ˆè©³ç´°ã¯ã‚»ã‚¯ã‚·ãƒ§ãƒ³10å‚ç…§ï¼‰")
    return


@app.cell
def _(SF_SCHEMA, df_id_mapping, mo, pd, query_sf):
    # Listé›†è¨ˆï¼ˆè»½é‡ç‰ˆï¼‰: ORGIDå˜ä½ã®CompanyList/PeopleListä»¶æ•°ã®ã¿
    df_list_summary = pd.DataFrame()

    _list_orgids = (
        df_id_mapping["ORGID"].dropna().astype(str).unique().tolist()
        if len(df_id_mapping) > 0 and "ORGID" in df_id_mapping.columns
        else []
    )

    if _list_orgids:
        _orgids_sql = ",".join([f"'{o}'" for o in _list_orgids[:500]])
        try:
            _q = f"""
            SELECT
                u.ORGID,
                COUNT(DISTINCT cl.ID) AS companylist_count,
                COUNT(DISTINCT pl.ID) AS peoplelist_count
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
            LEFT JOIN {SF_SCHEMA}.COMPANYLIST cl ON cl.USERORGRELATIONID = ur.ID
            LEFT JOIN {SF_SCHEMA}.PEOPLELIST pl ON pl.USERORGRELATIONID = ur.ID
            WHERE u.ORGID IN ({_orgids_sql})
            GROUP BY u.ORGID
            """
            df_list_summary = query_sf(_q)
            # ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€
            _rename = {}
            for _c in df_list_summary.columns:
                if _c.upper() == "COMPANYLIST_COUNT":
                    _rename[_c] = "companylist_count"
                elif _c.upper() == "PEOPLELIST_COUNT":
                    _rename[_c] = "peoplelist_count"
                elif _c.upper() == "ORGID":
                    _rename[_c] = "ORGID"
            df_list_summary = df_list_summary.rename(columns=_rename)
            df_list_summary["companylist_count"] = df_list_summary["companylist_count"].fillna(0).astype(int)
            df_list_summary["peoplelist_count"] = df_list_summary["peoplelist_count"].fillna(0).astype(int)
        except Exception as _e:
            mo.md(f"Listé›†è¨ˆã‚¨ãƒ©ãƒ¼: {_e}")
            df_list_summary = pd.DataFrame()

    mo.md(f"**Listé›†è¨ˆ**: {len(df_list_summary)}ä¼æ¥­ (CompanyListä¿æœ‰: {(df_list_summary['companylist_count'] > 0).sum() if len(df_list_summary) > 0 else 0}, PeopleListä¿æœ‰: {(df_list_summary['peoplelist_count'] > 0).sum() if len(df_list_summary) > 0 else 0})")
    return (df_list_summary,)




@app.cell
def _(mo):
    mo.md("""
    ## 6. ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆãƒãƒ£ãƒ¼ãƒ³ + GA + ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼‰
    """)
    return


@app.cell
def _(
    df_churn,
    df_churn_reason_latest,
    df_ga,
    df_id_mapping,
    df_list_summary,
    mo,
    np,
    pd,
    query_bq,
):
    df_merged = pd.DataFrame()

    if len(df_id_mapping) > 0 and len(df_ga) > 0:
        # 1. ãƒãƒ£ãƒ¼ãƒ³ï¼ˆTSVï¼‰+ BeegleCompany + USERORGANIZATIONï¼ˆCOMPNO ã§ç´ã¥ã‘ï¼‰
        bq_cols = [
            "COMPNO", "COMPANYID", "ORGID", "ORG_NAME", "BQ_COMPANY_NAME", "BQ_COMPANY_KANA",
            "CEO", "ESTABLISHMENT", "ADDRESS", "PHONE", "WEBSITE_URL", "MAIL",
            "INDUSTRY_ID", "EMPLOYEE_ID", "EMPLOYEE_COUNT", "REVENUE_ID", "CAPITAL_ID",
            "ISCLOSED", "BQ_CREATED_AT", "ORG_CREATED_AT"
        ]
        bq_cols_exist = [c for c in bq_cols if c in df_id_mapping.columns]
        df_merged = df_churn.merge(
            df_id_mapping[bq_cols_exist],
            on="COMPNO",
            how="left",
        )

        # 2. è§£ç´„ç†ç”±ï¼ˆCSVï¼‰ã‚’çµåˆ
        if len(df_churn_reason_latest) > 0 and "company_name_norm" in df_merged.columns:
            df_merged = df_merged.merge(
                df_churn_reason_latest,
                on="company_name_norm",
                how="left",
                suffixes=("", "_reason"),
            )

        # 3. GAæŒ‡æ¨™ã‚’çµåˆ
        df_merged = df_merged.merge(
            df_ga,
            left_on="ORGID",
            right_on="org_id",
            how="left",
        )

        # 4. Liståˆ†æï¼ˆSnowflakeï¼‰
        if len(df_list_summary) > 0:
            df_merged = df_merged.merge(
                df_list_summary,
                on="ORGID",
                how="left",
                suffixes=("", "_list"),
            )

        # 5. æ´¾ç”ŸæŒ‡æ¨™
        df_merged["sessions_per_user"] = df_merged["sessions"] / df_merged["users"].replace(0, np.nan)
        df_merged["page_views_per_user"] = df_merged["page_views"] / df_merged["users"].replace(0, np.nan)

        # 6. ãƒšãƒ¼ã‚¸åˆ©ç”¨ç‡
        total_pv = df_merged["page_views"].replace(0, np.nan)
        df_merged["list_rate"] = df_merged["pv_list"] / total_pv * 100
        df_merged["company_rate"] = df_merged["pv_company"] / total_pv * 100
        df_merged["download_rate"] = df_merged["pv_download"] / total_pv * 100
        # pv_search ã¯é™¤å¤–ï¼ˆInfoBoxãƒ‰ãƒ¡ã‚¤ãƒ³ã«è©²å½“ãƒšãƒ¼ã‚¸ãªã—ï¼‰

        # 7. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚¹ã‚³ã‚¢å–å¾—ï¼ˆBigQuery first_party_scoreï¼‰
        # ORGIDã‚’corporate_idã¨ã—ã¦ä½¿ç”¨ï¼ˆè¦ç¢ºèªï¼‰
        orgids = df_merged["ORGID"].dropna().unique().tolist()
        if orgids:
            orgids_sql = ",".join([f"'{o}'" for o in orgids[:500]])  # ä¸Šé™500
            intent_query = f"""
            SELECT 
                CAST(first_party_corporate_id AS STRING) AS org_id,
                AVG(CASE WHEN intent_level = 3 THEN 1 ELSE 0 END) * 100 AS high_intent_rate,
                AVG(CASE WHEN intent_level = 2 THEN 1 ELSE 0 END) * 100 AS middle_intent_rate,
                COUNT(DISTINCT corporate_id) AS intent_company_count,
                MAX(change_date) AS latest_intent_change_date,
                MAX(CASE WHEN intent_level = 3 THEN 1 ELSE 0 END) AS intent_activation_flag,
                MAX(
                    CASE
                        WHEN change_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
                        THEN 1
                        ELSE 0
                    END
                ) AS intent_recent_change_flag
            FROM `gree-dionysus-infobox.production_infobox.first_party_score_company_latest`
            WHERE CAST(first_party_corporate_id AS STRING) IN ({orgids_sql})
            GROUP BY 1
            """
            try:
                df_intent = query_bq(intent_query)
                if len(df_intent) > 0:
                    df_merged = df_merged.merge(
                        df_intent,
                        left_on="ORGID",
                        right_on="org_id",
                        how="left",
                        suffixes=("", "_intent"),
                    )
            except Exception:
                pass

            sfa_intent_query = f"""
            SELECT 
                CAST(s.first_party_corporate_id AS STRING) AS org_id,
                MAX(
                    CASE
                        WHEN REGEXP_CONTAINS(c.original_category_name, r'(SFA|å–¶æ¥­æ”¯æ´|CRM|ã‚»ãƒ¼ãƒ«ã‚¹)') THEN 1
                        ELSE 0
                    END
                ) AS sfa_intent_flag,
                COUNT(DISTINCT CASE
                    WHEN REGEXP_CONTAINS(c.original_category_name, r'(SFA|å–¶æ¥­æ”¯æ´|CRM|ã‚»ãƒ¼ãƒ«ã‚¹)')
                    THEN s.corporate_id
                    ELSE NULL
                END) AS sfa_intent_company_count
            FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3` c
            JOIN `gree-dionysus-infobox.production_infobox.first_party_score_company_latest` s
              ON CAST(c.corporate_id AS INT64) = s.corporate_id
            WHERE c.view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
              AND s.change_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
              AND s.intent_level IN (2, 3)
              AND CAST(s.first_party_corporate_id AS STRING) IN ({orgids_sql})
            GROUP BY 1
            """
            try:
                df_sfa_intent = query_bq(sfa_intent_query)
                if len(df_sfa_intent) > 0:
                    df_merged = df_merged.merge(
                        df_sfa_intent,
                        left_on="ORGID",
                        right_on="org_id",
                        how="left",
                        suffixes=("", "_sfa"),
                    )
            except Exception:
                pass

        if "sfa_intent_flag" in df_merged.columns:
            df_merged["sfa_intent_flag"] = (
                df_merged["sfa_intent_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["sfa_intent_flag"] = 0

        if "sfa_intent_company_count" in df_merged.columns:
            df_merged["sfa_intent_company_count"] = (
                df_merged["sfa_intent_company_count"].fillna(0).astype(int)
            )
        else:
            df_merged["sfa_intent_company_count"] = 0

        if "intent_activation_flag" in df_merged.columns:
            df_merged["intent_activation_flag"] = (
                df_merged["intent_activation_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["intent_activation_flag"] = 0

        if "intent_recent_change_flag" in df_merged.columns:
            df_merged["intent_recent_change_flag"] = (
                df_merged["intent_recent_change_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["intent_recent_change_flag"] = 0

        high_series = (
            df_merged["high_intent_rate"]
            if "high_intent_rate" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        middle_series = (
            df_merged["middle_intent_rate"]
            if "middle_intent_rate" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        sfa_series = (
            df_merged["sfa_intent_flag"]
            if "sfa_intent_flag" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        df_merged["intent_interest_flag"] = (
            (high_series.fillna(0) > 0)
            | (middle_series.fillna(0) > 0)
            | (sfa_series.fillna(0) > 0)
        ).astype(int)

        df_merged["intent_recent_change_activation_flag"] = (
            (df_merged["intent_recent_change_flag"] > 0)
            & (df_merged["intent_activation_flag"] > 0)
        ).astype(int)

        mo.md(
            f"""
            **çµ±åˆçµæœ**:
            - çµ±åˆå¾Œè¡Œæ•°: {len(df_merged):,}
            - ORGIDçµåˆæˆåŠŸ: {df_merged['ORGID'].notna().sum():,}
            - GAçµåˆæˆåŠŸ: {df_merged['sessions'].notna().sum():,}
            """
        )
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã›ã‚“*")
    return (df_merged,)


@app.cell
def _(df_merged, mo):
    # ä¼šç¤¾åã‚«ãƒ©ãƒ ã‚’æ¢ã™
    company_col = next((c for c in ["BQ_COMPANY_NAME", "CompanyName", "COMPANY_NAME"] if c in df_merged.columns), None) if len(df_merged) > 0 else None
    display_cols = [
        company_col, "INDUSTRY_ID", "EMPLOYEE_COUNT", "ADDRESS",
        "status", "is_churned", "ORGID",
        "sessions", "page_views", "users",
        "list_rate", "download_rate",
        "sfa_intent_flag", "sfa_intent_company_count", "intent_interest_flag",
        "intent_activation_flag",
        "intent_recent_change_flag",
        "intent_recent_change_activation_flag",
        "latest_intent_change_date",
        "companylist_count", "peoplelist_count",
        "companylist_company_count", "peoplelist_keyman_count",
        "companylist_top_companies", "peoplelist_top_keymen",
        "loss_type", "loss_reason", "loss_detail",
    ]
    cols_exist = [c for c in display_cols if c and c in df_merged.columns] if len(df_merged) > 0 else []
    df_display = df_merged[cols_exist].head(30) if cols_exist else df_merged.head(30)
    mo.ui.table(df_display, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 7. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆãƒãƒ£ãƒ¼ãƒ³å¯„ä¸åº¦ï¼‰
    """)
    return


@app.cell
def _(df_merged, mo, pd):
    df_importance = pd.DataFrame()

    if len(df_merged) > 0:
        try:
            from sklearn.linear_model import LogisticRegression
            from sklearn.preprocessing import StandardScaler

            # ç‰¹å¾´é‡é¸æŠ
            feature_cols = [
                "sessions", "page_views", "users",
                "sessions_per_user", "page_views_per_user",
                "pv_list", "pv_company", "pv_download",
                "list_rate", "download_rate",
            ]
            # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆæŒ‡æ¨™ãŒã‚ã‚Œã°è¿½åŠ 
            if "high_intent_rate" in df_merged.columns:
                feature_cols.extend(["high_intent_rate", "middle_intent_rate", "intent_company_count"])

            feature_cols = [c for c in feature_cols if c in df_merged.columns]

            # æ¬ æå€¤ã‚’é™¤å¤–
            df_ml = df_merged[["is_churned"] + feature_cols].dropna()

            if len(df_ml) > 10:
                X = df_ml[feature_cols]
                y = df_ml["is_churned"]

                # æ¨™æº–åŒ–
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
                model_logit = LogisticRegression(max_iter=1000, random_state=42)
                model_logit.fit(X_scaled, y)

                importance_data = []
                for feat_col, coef in zip(feature_cols, model_logit.coef_[0]):
                    importance_data.append({
                        "ç‰¹å¾´é‡": feat_col,
                        "ä¿‚æ•°": round(coef, 4),
                        "åŠ¹æœæ–¹å‘": "è§£ç´„ä¿ƒé€²" if coef > 0 else "ç¶™ç¶šä¿ƒé€²",
                        "é‡è¦åº¦": round(abs(coef), 4),
                    })
                df_importance = pd.DataFrame(importance_data).sort_values("é‡è¦åº¦", ascending=False)

                mo.md(f"**å­¦ç¿’å®Œäº†**: ã‚µãƒ³ãƒ—ãƒ«æ•° {len(df_ml)}, ç‰¹å¾´é‡æ•° {len(feature_cols)}")
            else:
                mo.md("*ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ10ä»¶ä»¥ä¸Šå¿…è¦ï¼‰*")

        except ImportError as e:
            mo.md(f"**scikit-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: `{e}`")
        except Exception as e:
            mo.md(f"**MLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼**: `{e}`")
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿çµ±åˆãŒå®Œäº†ã™ã‚‹ã¾ã§ãŠå¾…ã¡ãã ã•ã„*")
    return (df_importance,)


@app.cell
def _(df_importance, mo):
    mo.md("### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° å¯„ä¸åº¦")
    mo.ui.table(df_importance, pagination=False)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 8. LLMå±é™ºåº¦åˆ¤å®šï¼ˆGeminiï¼‰
    """)
    return


@app.cell
def _(df_merged, mo):
    llm_company_options = {}
    if len(df_merged) > 0:
        llm_name_cols = [c for c in ["BQ_COMPANY_NAME", "ORG_NAME", "CompanyName"] if c in df_merged.columns]
        df_llm_company_base = df_merged.dropna(subset=["ORGID"]).copy()
        if llm_name_cols:
            df_llm_company_base["company_label"] = df_llm_company_base[llm_name_cols[0]].astype(str)
        else:
            df_llm_company_base["company_label"] = df_llm_company_base["ORGID"].astype(str)

        for _, llm_company_row in df_llm_company_base.drop_duplicates(subset=["ORGID"]).iterrows():
            llm_label = f"{llm_company_row['company_label']} ({llm_company_row['ORGID']})"
            llm_company_options[llm_label] = llm_company_row["ORGID"]

    llm_company_default_value = next(iter(llm_company_options.keys()), None) if llm_company_options else None
    company_selector = mo.ui.dropdown(
        options=llm_company_options,
        label="ä¼šç¤¾ã‚’é¸æŠ",
        value=llm_company_default_value,
    )
    company_selector
    return company_selector, llm_company_options


@app.cell
def _(company_selector, df_merged, pd):
    df_company_detail = pd.DataFrame()

    # company_selector.value ã¯æ—¢ã«ORGIDï¼ˆdropdown ã® dict ã§ã¯ .value ãŒå€¤ã‚’ç›´æ¥è¿”ã™ï¼‰
    selected_company_orgid = company_selector.value

    if len(df_merged) > 0 and selected_company_orgid:
        df_company_detail = df_merged[
            df_merged["ORGID"] == selected_company_orgid
        ].copy()
    return (df_company_detail,)


@app.cell
def _(company_selector, df_company_detail, mo):
    _company_outputs = []
    
    if company_selector.value and len(df_company_detail) > 0:
        _company_outputs.append(mo.md(f"**é¸æŠä¼æ¥­**: {company_selector.value} ({len(df_company_detail)} ä»¶)"))
        _company_outputs.append(mo.ui.table(df_company_detail.head(10), pagination=True))
    else:
        _company_outputs.append(mo.md("*ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))
    
    return mo.vstack(_company_outputs)


@app.cell
def _(mo):
    run_llm_button = mo.ui.run_button(label="LLMå±é™ºåº¦åˆ¤å®šã‚’å®Ÿè¡Œ")
    run_llm_button
    return (run_llm_button,)


@app.cell
def _(
    GA_DATASET_ID,
    SF_SCHEMA,
    company_selector,
    df_company_detail,
    extract_json_block,
    genai,
    genai_error,
    genai_types,
    json,
    mo,
    os,
    query_bq,
    query_sf,
    run_llm_button,
):
    _llm_outputs = []
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§é¸æŠã•ã‚ŒãŸä¼æ¥­ã‚’ä½¿ç”¨
    if len(df_company_detail) > 0:
        sample_company = df_company_detail.iloc[0:1].copy()
        row = sample_company.iloc[0]
        
        # è¤‡æ•°ã‚«ãƒ©ãƒ ã‹ã‚‰NaNã§ãªã„æœ€åˆã®å€¤ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ï¼‰
        import pandas as _pd_check
        company_name = (
            row.get("CompanyName") 
            if _pd_check.notna(row.get("CompanyName")) 
            else row.get("BQ_COMPANY_NAME") 
            if _pd_check.notna(row.get("BQ_COMPANY_NAME"))
            else row.get("ORG_NAME")
            if _pd_check.notna(row.get("ORG_NAME"))
            else "N/A"
        )
        orgid = row.get("ORGID") if _pd_check.notna(row.get("ORGID")) else "N/A"
        
        _llm_outputs.append(mo.md(f"### é¸æŠä¼æ¥­: {company_name}"))
        _llm_outputs.append(mo.md(f"ORGID: `{orgid}`"))
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒœã‚¿ãƒ³ã¨genaiã®çŠ¶æ…‹
        _llm_outputs.append(mo.md(f"*Debug: button={run_llm_button.value}, genai={genai is not None}, genai_error={genai_error}*"))
        
        if run_llm_button.value and genai is not None:
            # ã‚¹ãƒ”ãƒŠãƒ¼ä»˜ãã§LLMå‡¦ç†å®Ÿè¡Œ
            with mo.status.spinner(title="åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾— + LLMå‡¦ç†ä¸­...") as _status:
                try:
                    # === 1. GA 6ãƒ¶æœˆæ¨ç§»ã‚’å–å¾— ===
                    _status.update("GAåˆ©ç”¨æ¨ç§»ã‚’å–å¾—ä¸­ï¼ˆ6ãƒ¶æœˆï¼‰...")
                    ga_trend_query = f"""
                    WITH base AS (
                        SELECT
                            FORMAT_DATE('%Y-%m', PARSE_DATE('%Y%m%d', event_date)) AS month,
                            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
                            user_pseudo_id,
                            (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS session_id,
                            event_name,
                            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
                        FROM `{GA_DATASET_ID}.events_*`
                        WHERE _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))
                          AND (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') = '{orgid}'
                    )
                    SELECT
                        month,
                        COUNT(DISTINCT user_pseudo_id) AS users,
                        COUNT(DISTINCT CONCAT(user_pseudo_id, '-', CAST(session_id AS STRING))) AS sessions,
                        COUNTIF(event_name = 'page_view') AS page_views,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/companies/[a-z0-9]') AND event_name = 'page_view') AS pv_company_detail,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/company-lists|/people-lists|/leads-lists') AND event_name = 'page_view') AS pv_list,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/analysis') AND event_name = 'page_view') AS pv_analysis,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/people') AND event_name = 'page_view') AS pv_people
                    FROM base
                    GROUP BY month
                    ORDER BY month
                    """
                    try:
                        df_ga_trend = query_bq(ga_trend_query)
                        ga_trend_json = df_ga_trend.to_json(orient="records", force_ascii=False) if len(df_ga_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… GAæ¨ç§»å–å¾—: {len(df_ga_trend)}è¡Œ"))
                    except Exception as ga_err:
                        df_ga_trend = None
                        ga_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ GAæ¨ç§»å–å¾—ã‚¨ãƒ©ãƒ¼: `{ga_err}`"))
                    
                    # === 2. Snowflake ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ã‚’å–å¾— ===
                    _status.update("Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ã‚’å–å¾—ä¸­...")
                    
                    # CompanyListä½œæˆæ¨ç§»
                    list_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', cl.CREATEDAT) AS MONTH,
                        COUNT(DISTINCT cl.ID) AS COMPANYLIST_CREATED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.COMPANYLIST cl ON cl.USERORGRELATIONID = ur.ID
                    WHERE u.ORGID = '{orgid}'
                      AND cl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_list_trend = query_sf(list_trend_query)
                        list_trend_json = df_list_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_list_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… CompanyListæ¨ç§»: {len(df_list_trend)}è¡Œ"))
                    except Exception as cl_err:
                        df_list_trend = None
                        list_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ CompanyListæ¨ç§»ã‚¨ãƒ©ãƒ¼: `{cl_err}`"))
                    
                    # PeopleListä½œæˆæ¨ç§»ï¼ˆã‚­ãƒ¼ãƒãƒ³ç™»éŒ²æ•°ï¼‰
                    peoplelist_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', pl.CREATEDAT) AS MONTH,
                        COUNT(DISTINCT pl.ID) AS PEOPLELIST_CREATED,
                        COUNT(DISTINCT k.ID) AS KEYMAN_REGISTERED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.PEOPLELIST pl ON pl.USERORGRELATIONID = ur.ID
                    LEFT JOIN {SF_SCHEMA}._KEYMANTOPEOPLELIST rel ON rel.B = pl.ID
                    LEFT JOIN {SF_SCHEMA}.KEYMAN k ON rel.A = k.ID
                    WHERE u.ORGID = '{orgid}'
                      AND pl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_peoplelist_trend = query_sf(peoplelist_trend_query)
                        peoplelist_trend_json = df_peoplelist_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_peoplelist_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… PeopleListæ¨ç§»: {len(df_peoplelist_trend)}è¡Œ"))
                    except Exception as pl_err:
                        df_peoplelist_trend = None
                        peoplelist_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ PeopleListæ¨ç§»ã‚¨ãƒ©ãƒ¼: `{pl_err}`"))
                    
                    # Memoä½œæˆæ¨ç§»
                    memo_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', m.CREATEDAT) AS MONTH,
                        COUNT(*) AS MEMO_CREATED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.MEMO m ON m.USERORGRELATIONID = ur.ID
                    WHERE u.ORGID = '{orgid}'
                      AND m.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_memo_trend = query_sf(memo_trend_query)
                        memo_trend_json = df_memo_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_memo_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… ãƒ¡ãƒ¢æ¨ç§»: {len(df_memo_trend)}è¡Œ"))
                    except Exception as memo_err:
                        df_memo_trend = None
                        memo_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ ãƒ¡ãƒ¢æ¨ç§»ã‚¨ãƒ©ãƒ¼: `{memo_err}`"))
                    
                    # === 3. æ¨ç§»ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º ===
                    _llm_outputs.append(mo.md("### GAåˆ©ç”¨æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰"))
                    if df_ga_trend is not None and len(df_ga_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_ga_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*GAãƒ‡ãƒ¼ã‚¿ãªã—*"))
                    
                    _llm_outputs.append(mo.md("### Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»"))
                    _llm_outputs.append(mo.md("**CompanyListä½œæˆæ¨ç§»**"))
                    if df_list_trend is not None and len(df_list_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_list_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*CompanyListä½œæˆãªã—*"))
                    
                    _llm_outputs.append(mo.md("**PeopleListä½œæˆæ¨ç§»ï¼ˆã‚­ãƒ¼ãƒãƒ³ç™»éŒ²å«ã‚€ï¼‰**"))
                    if df_peoplelist_trend is not None and len(df_peoplelist_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_peoplelist_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*PeopleListä½œæˆãªã—*"))
                    
                    _llm_outputs.append(mo.md("**ãƒ¡ãƒ¢ä½œæˆæ¨ç§»**"))
                    if df_memo_trend is not None and len(df_memo_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_memo_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*ãƒ¡ãƒ¢ä½œæˆãªã—*"))
                    
                    # === 4. LLMå‡¦ç† ===
                    _status.update("ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ä¸­...")
                    
                    # .envã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
                    import dotenv as _dotenv_mod
                    import pathlib as _pathlib_mod
                    _llm_env_path = _pathlib_mod.Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/.env")
                    if _llm_env_path.exists():
                        _dotenv_mod.load_dotenv(_llm_env_path, override=True)
                    
                    # Gemini 3 Pro Preview
                    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
                    if not api_key:
                        raise ValueError("GEMINI_API_KEY ã¾ãŸã¯ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    
                    _status.update("Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
                    client = genai.Client(api_key=api_key)

                    # ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
                    company_data = sample_company.iloc[0].to_dict()
                    company_json = json.dumps({k: str(v) for k, v in company_data.items() if v is not None}, ensure_ascii=False, indent=2)

                    prompt = f"""
    ã‚ãªãŸã¯å–¶æ¥­æ”¯æ´SaaSã€ŒInfoBoxã€ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µã‚¯ã‚»ã‚¹æ‹…å½“ã§ã™ã€‚
    ä»¥ä¸‹ã®ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ã¨åˆ©ç”¨æ¨ç§»ã‚’åˆ†æã—ã€ãƒãƒ£ãƒ¼ãƒ³ï¼ˆè§£ç´„ï¼‰ãƒªã‚¹ã‚¯ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

    ## ä¼šç¤¾åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    ```json
    {company_json}
    ```

    ## GAåˆ©ç”¨æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰
    - users: ãƒ­ã‚°ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
    - sessions: ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
    - page_views: PVæ•°
    - pv_list: ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
    - pv_company: ä¼æ¥­è©³ç´°ã‚¢ã‚¯ã‚»ã‚¹
    - pv_download: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
    ```json
    {ga_trend_json}
    ```

    ## Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰
    ### CompanyListä½œæˆæ•°ï¼ˆä¼æ¥­ãƒªã‚¹ãƒˆï¼‰
    ```json
    {list_trend_json}
    ```
    ### PeopleListä½œæˆæ•°ï¼ˆã‚­ãƒ¼ãƒãƒ³ãƒªã‚¹ãƒˆï¼‰
    - PEOPLELIST_CREATED: ä½œæˆã•ã‚ŒãŸPeopleListæ•°
    - KEYMAN_REGISTERED: ç™»éŒ²ã•ã‚ŒãŸã‚­ãƒ¼ãƒãƒ³æ•°
    ```json
    {peoplelist_trend_json}
    ```
    ### ãƒ¡ãƒ¢ä½œæˆæ•°
    ```json
    {memo_trend_json}
    ```

    ## åˆ¤å®šåŸºæº–
    - **Highï¼ˆ70-100ç‚¹ï¼‰**: è§£ç´„ãƒªã‚¹ã‚¯ãŒé«˜ã„
      - åˆ©ç”¨ç‡ãŒæ¸›å°‘å‚¾å‘
      - CompanyList/PeopleList/ãƒ¡ãƒ¢ä½œæˆãŒåœæ»
      - ç«¶åˆæ¤œè¨ã®å…†å€™
    - **Mediumï¼ˆ40-69ç‚¹ï¼‰**: æ³¨æ„ãŒå¿…è¦
      - åˆ©ç”¨é »åº¦ã«æ³¢ãŒã‚ã‚‹
      - ä¸€éƒ¨æ©Ÿèƒ½ã®ã¿æ´»ç”¨ï¼ˆä¾‹: CompanyListã¯ä½¿ã†ãŒPeopleListã¯æœªä½¿ç”¨ï¼‰
    - **Lowï¼ˆ0-39ç‚¹ï¼‰**: å®‰å®š
      - ç¶™ç¶šçš„ãªåˆ©ç”¨
      - è¤‡æ•°æ©Ÿèƒ½ã‚’æ´»ç”¨ï¼ˆCompanyList + PeopleList + ãƒ¡ãƒ¢ï¼‰
      - ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒç¶­æŒ/å¢—åŠ 

    ## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
    ```json
    {{
      "risk_level": "High" | "Medium" | "Low",
      "risk_score": 0-100,
      "reason": "åˆ¤å®šç†ç”±ï¼ˆæ—¥æœ¬èªã€3æ–‡ä»¥å†…ï¼‰",
      "key_signals": ["ã‚·ã‚°ãƒŠãƒ«1", "ã‚·ã‚°ãƒŠãƒ«2"],
      "recommended_actions": ["ã‚¢ã‚¯ã‚·ãƒ§ãƒ³1", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³2"],
      "trend_analysis": "æ¨ç§»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®åˆ†æï¼ˆæ—¥æœ¬èªã€2æ–‡ä»¥å†…ï¼‰"
    }}
    ```
    - JSONã®ã¿è¿”ã™ï¼ˆä½™è¨ˆãªèª¬æ˜ã‚„ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ä¸è¦ï¼‰
    """

                    _status.update("Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆ10-30ç§’ã‹ã‹ã‚Šã¾ã™ï¼‰...")
                    
                    response = client.models.generate_content(
                        model="gemini-3-pro-preview",
                        contents=prompt,
                        config=genai_types.GenerateContentConfig(
                            temperature=0.1,
                            max_output_tokens=2000,
                        ),
                    )

                    _status.update("å¿œç­”è§£æä¸­...")
                    llm_result = response.text
                    llm_json = extract_json_block(llm_result) or llm_result
                    
                except Exception as e:
                    _llm_outputs.append(mo.md(f"âŒ **LLMå®Ÿè¡Œã‚¨ãƒ©ãƒ¼**: `{e}`"))
                    llm_json = None
            
            # å‡¦ç†å®Œäº†å¾Œã®çµæœè¡¨ç¤º
            if llm_json:
                _llm_outputs.append(mo.md("âœ… **LLMå‡¦ç†å®Œäº†**"))
                _llm_outputs.append(mo.md("### LLMåˆ¤å®šçµæœ"))
                _llm_outputs.append(mo.md(f"```json\n{llm_json}\n```"))

                # JSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’å¯è¦–åŒ–
                try:
                    llm_parsed = json.loads(llm_json)
                    risk_level = llm_parsed.get("risk_level", "Unknown")
                    risk_score = llm_parsed.get("risk_score", 0)
                    reason = llm_parsed.get("reason", "")
                    key_signals = llm_parsed.get("key_signals", [])
                    actions = llm_parsed.get("recommended_actions", [])
                    trend_analysis = llm_parsed.get("trend_analysis", "")
                    
                    color = {"High": "red", "Medium": "orange", "Low": "green"}.get(risk_level, "gray")
                    
                    _llm_outputs.append(mo.md(f"""
### ãƒªã‚¹ã‚¯åˆ¤å®šã‚µãƒãƒªãƒ¼

| é …ç›® | å€¤ |
|------|-----|
| **ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«** | <span style="color:{color};font-weight:bold">{risk_level}</span> |
| **ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢** | {risk_score}/100 |
| **åˆ¤å®šç†ç”±** | {reason} |
| **æ¨ç§»åˆ†æ** | {trend_analysis} |
| **ä¸»è¦ã‚·ã‚°ãƒŠãƒ«** | {', '.join(key_signals) if key_signals else '-'} |
| **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** | {', '.join(actions) if actions else '-'} |
"""))
                except Exception:
                    pass
                    
        elif genai_error:
            _llm_outputs.append(mo.md(f"**Geminiæœªè¨­å®š**: `{genai_error}`"))
        else:
            _llm_outputs.append(mo.md("*ã€ŒLLMå±é™ºåº¦åˆ¤å®šã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„*"))
    else:
        _llm_outputs.append(mo.md("*ä¸Šã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))
    
    mo.vstack(_llm_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 9. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å…¨ä½“ã‚µãƒãƒªãƒ¼
    """)
    return


@app.cell
def _(df_competitor_intent, df_importance, df_merged, mo):
    summary_parts = []

    if len(df_merged) > 0:
        churned = df_merged["is_churned"].sum()
        active = len(df_merged) - churned
        summary_parts.append(f"- **ä¼æ¥­æ•°**: è§£ç´„ {churned}, å¥‘ç´„ä¸­ {active}")

        if "sessions" in df_merged.columns:
            avg_sessions = df_merged[df_merged["is_churned"] == 0]["sessions"].mean()
            avg_sessions_churned = df_merged[df_merged["is_churned"] == 1]["sessions"].mean()
            summary_parts.append(f"- **å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³**: å¥‘ç´„ä¸­ {avg_sessions:.1f}, è§£ç´„ {avg_sessions_churned:.1f}")

    if len(df_importance) > 0:
        top_factors = df_importance.head(3)["ç‰¹å¾´é‡"].tolist()
        summary_parts.append(f"- **ãƒãƒ£ãƒ¼ãƒ³å¯„ä¸åº¦TOP3**: {', '.join(top_factors)}")

    if len(df_competitor_intent) > 0:
        high_intent = df_competitor_intent[df_competitor_intent["level_name"] == "High"]["company_count"].sum()
        summary_parts.append(f"- **ç«¶åˆã‚«ãƒ†ã‚´ãƒª High ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆ**: {high_intent:,} ç¤¾")

    if summary_parts:
        mo.md("### ã‚µãƒãƒªãƒ¼\n" + "\n".join(summary_parts))
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„*")
    summary_parts
    return


@app.cell
def _(mo):
    mo.md("""
    ## 10. Liståˆ†æï¼ˆCompanyList / PeopleListï¼‰
    ä¼æ¥­ã‚’é¸æŠã™ã‚‹ã¨ã€ãã®ä¼æ¥­ã®ãƒªã‚¹ãƒˆè©³ç´°ã‚’ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§å–å¾—ãƒ»è¡¨ç¤ºã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(df_id_mapping, df_list_summary, mo):
    _list_options = {}
    if len(df_id_mapping) > 0 and len(df_list_summary) > 0:
        _name_cols = [c for c in ["BQ_COMPANY_NAME", "ORG_NAME"] if c in df_id_mapping.columns]
        _base = df_id_mapping.dropna(subset=["ORGID"]).copy()
        if _name_cols:
            _base["_label"] = _base[_name_cols[0]].astype(str)
        else:
            _base["_label"] = _base["ORGID"].astype(str)

        _orgids_with_lists = set(
            df_list_summary[
                (df_list_summary["companylist_count"].fillna(0) > 0)
                | (df_list_summary["peoplelist_count"].fillna(0) > 0)
            ]["ORGID"].astype(str).tolist()
        )

        for _, _r in _base.drop_duplicates(subset=["ORGID"]).iterrows():
            if str(_r["ORGID"]) in _orgids_with_lists:
                _list_options[f"{_r['_label']} ({_r['ORGID']})"] = _r["ORGID"]

    _default = next(iter(_list_options.keys()), None) if _list_options else None
    list_org_selector = mo.ui.dropdown(
        options=_list_options,
        label="Liståˆ†æ: ä¼æ¥­ã‚’é¸æŠï¼ˆãƒªã‚¹ãƒˆ1ä»¶ä»¥ä¸Šï¼‰",
        value=_default,
    )
    list_org_selector
    return (list_org_selector,)


@app.cell
def _(
    SF_SCHEMA,
    df_list_summary,
    list_org_selector,
    mo,
    pd,
    query_sf,
):
    _out = []
    _orgid = list_org_selector.value if list_org_selector.value else None

    if _orgid:
        # ã‚µãƒãƒªãƒ¼ï¼ˆäº‹å‰é›†è¨ˆæ¸ˆã¿ï¼‰
        _sum = df_list_summary[df_list_summary["ORGID"] == _orgid] if len(df_list_summary) > 0 else pd.DataFrame()
        _cl_count = int(_sum.iloc[0]["companylist_count"]) if len(_sum) > 0 else 0
        _pl_count = int(_sum.iloc[0]["peoplelist_count"]) if len(_sum) > 0 else 0
        _out.append(mo.md(f"### {list_org_selector.value}\nCompanyList: **{_cl_count}ä»¶** / PeopleList: **{_pl_count}ä»¶**"))

        # ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§è©³ç´°å–å¾—
        with mo.status.spinner(title="Listè©³ç´°å–å¾—ä¸­..."):
            _df_cl = pd.DataFrame()
            _df_pl = pd.DataFrame()
            try:
                if _cl_count > 0:
                    _df_cl = query_sf(f"""
                    SELECT bc.SHOGO AS COMPANY_NAME, bc.GYOSHUSHOID AS INDUSTRY_ID,
                           bc.PREFID, bc.EMPCOUNT AS EMPLOYEE_COUNT, cl.CREATEDAT
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.COMPANYLIST cl ON cl.USERORGRELATIONID = ur.ID
                    JOIN {SF_SCHEMA}._BEEGLECOMPANYTOCOMPANYLIST rel ON rel.B = cl.ID
                    JOIN {SF_SCHEMA}.BEEGLECOMPANY bc ON rel.A = bc.ID
                    WHERE u.ORGID = '{_orgid}' LIMIT 500
                    """)
            except Exception:
                pass
            try:
                if _pl_count > 0:
                    _df_pl = query_sf(f"""
                    SELECT km.ID AS KEYMAN_ID, km.NAME AS KEYMAN_NAME, pl.CREATEDAT
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.PEOPLELIST pl ON pl.USERORGRELATIONID = ur.ID
                    JOIN {SF_SCHEMA}._KEYMANTOPEOPLELIST rel ON rel.B = pl.ID
                    JOIN {SF_SCHEMA}.KEYMAN km ON rel.A = km.ID
                    WHERE u.ORGID = '{_orgid}' LIMIT 500
                    """)
            except Exception:
                pass

        # CompanyList
        if len(_df_cl) > 0:
            _out.append(mo.md(f"#### CompanyListè©³ç´° ({len(_df_cl)}ä»¶)"))
            if "INDUSTRY_ID" in _df_cl.columns:
                _ind = _df_cl["INDUSTRY_ID"].dropna().value_counts().head(5).reset_index()
                _ind.columns = ["æ¥­ç¨®ID", "ä»¶æ•°"]
                _out.append(mo.md("**æ¥­ç¨®åˆ†å¸ƒï¼ˆä¸Šä½5ä»¶ï¼‰**"))
                _out.append(mo.ui.table(_ind, pagination=False))
            if "PREFID" in _df_cl.columns:
                _reg = _df_cl["PREFID"].dropna().value_counts().head(5).reset_index()
                _reg.columns = ["éƒ½é“åºœçœŒID", "ä»¶æ•°"]
                _out.append(mo.md("**åœ°åŸŸåˆ†å¸ƒï¼ˆä¸Šä½5ä»¶ï¼‰**"))
                _out.append(mo.ui.table(_reg, pagination=False))
            _out.append(mo.ui.table(_df_cl.head(50), pagination=True))

        # PeopleList
        if len(_df_pl) > 0:
            _out.append(mo.md(f"#### PeopleListè©³ç´° ({len(_df_pl)}ä»¶)"))
            _out.append(mo.ui.table(_df_pl.head(50), pagination=True))

        if len(_df_cl) == 0 and len(_df_pl) == 0:
            _out.append(mo.md("*ã“ã®ä¼æ¥­ã«ã¯ãƒªã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“*"))
    else:
        _out.append(mo.md("*ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))

    mo.vstack(_out)
    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()

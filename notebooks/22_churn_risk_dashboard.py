import marimo

__generated_with = "0.17.8"
app = marimo.App(width="full")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _(mo):
    mo.md("""
    # Churn Risk Dashboard with Intent

    ãƒãƒ£ãƒ¼ãƒ³ãƒªã‚¹ã‚¯ã€GAåˆ©ç”¨çŠ¶æ³ã€ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼ˆç«¶åˆå«ã‚€ï¼‰ã‚’çµ±åˆã—ãŸãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã™ã€‚
    LLMã§å„ä¼šç¤¾ã®å±é™ºåº¦ã‚’åˆ¤å®šã—ã¾ã™ã€‚

    ## ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    - **ãƒãƒ£ãƒ¼ãƒ³æƒ…å ±**: `infobox_data.tsv`
    - **GAåˆ©ç”¨ãƒ­ã‚°**: BigQuery `analytics_400693944`
    - **ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆ**: BigQuery `gree-dionysus-infobox.production_infobox`
    - **çµ„ç¹”ãƒã‚¹ã‚¿**: Snowflake `USERORGANIZATION`, `BEEGLECOMPANY`

    ## ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯å®šç¾©
    - å‚ç…§: `docs/semantic_questions_status.md`
    - ERå›³: `docs/snowflake_elt_entity.puml`
    """)
    return


@app.cell
def _():
    import json
    import os
    import re
    import sys
    import tomllib
    from datetime import date, timedelta
    from pathlib import Path

    import numpy as np
    import pandas as pd
    from dotenv import load_dotenv

    # Snowflake
    snowflake = None
    default_backend = None
    serialization = None
    snowflake_error = None
    try:
        import snowflake.connector
        from cryptography.hazmat.backends import default_backend
        from cryptography.hazmat.primitives import serialization
        import snowflake as snowflake
    except Exception as exc:
        snowflake_error = exc

    # BigQuery
    bq_error = None
    bigquery = None
    service_account = None
    try:
        from google.cloud import bigquery
        from google.oauth2 import service_account
    except Exception as exc:
        bq_error = exc

    # Gemini
    genai = None
    genai_types = None
    genai_error = None
    try:
        from google import genai
        from google.genai import types as genai_types
    except Exception as exc:
        genai_error = exc

    # .envèª­ã¿è¾¼ã¿
    possible_env_paths = [
        Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/.env"),
        Path(__file__).parent.parent / ".env",
        Path.cwd() / ".env",
        Path.cwd().parent / ".env",
    ]
    loaded_env_path = None
    for env_path in possible_env_paths:
        if env_path.exists():
            load_dotenv(env_path, override=True)
            loaded_env_path = str(env_path)
            break

    # APIã‚­ãƒ¼èª­ã¿è¾¼ã¿ç¢ºèª
    gemini_api_key_loaded = os.getenv("GEMINI_API_KEY")
    gemini_key_status = f"è¨­å®šæ¸ˆã¿ ({gemini_api_key_loaded[:8]}...)" if gemini_api_key_loaded else "æœªè¨­å®š"

    root_dir = Path("/Users/kou1904/githubactions_fordata/work/aieda_agent")
    if str(root_dir / "src") not in sys.path:
        sys.path.insert(0, str(root_dir / "src"))
    return (
        Path,
        bigquery,
        bq_error,
        default_backend,
        genai,
        genai_error,
        genai_types,
        gemini_key_status,
        json,
        loaded_env_path,
        np,
        os,
        pd,
        re,
        serialization,
        service_account,
        snowflake,
        snowflake_error,
        tomllib,
    )


@app.cell
def _(bq_error, genai_error, gemini_key_status, loaded_env_path, mo, snowflake_error):
    errors = []
    if snowflake_error:
        errors.append(f"Snowflake: `{snowflake_error}`")
    if bq_error:
        errors.append(f"BigQuery: `{bq_error}`")
    if genai_error:
        errors.append(f"Gemini: `{genai_error}`")
    
    status_lines = [
        f"- **Gemini API Key**: {gemini_key_status}",
        f"- **.envèª­ã¿è¾¼ã¿**: {loaded_env_path or 'æœªèª­ã¿è¾¼ã¿'}",
    ]
    
    output = "## ğŸ”§ åˆæœŸåŒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\n\n"
    if errors:
        output += "**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼**:\n" + "\n".join([f"- {e}" for e in errors]) + "\n\n"
    output += "**è¨­å®šçŠ¶æ³**:\n" + "\n".join(status_lines)
    
    mo.md(output)
    return


@app.cell
def _(re):
    def normalize_company_name(name: str | None) -> str | None:
        if name is None:
            return None
        value = str(name)
        if not value.strip():
            return None
        value = re.sub(r"\s+", "", value)
        value = re.sub(
            r"(æ ªå¼ä¼šç¤¾|æœ‰é™ä¼šç¤¾|åˆåŒä¼šç¤¾|ä¸€èˆ¬ç¤¾å›£æ³•äºº|ä¸€èˆ¬è²¡å›£æ³•äºº|å…¬ç›Šç¤¾å›£æ³•äºº|å…¬ç›Šè²¡å›£æ³•äºº)",
            "",
            value,
        )
        value = re.sub(r"[ï¼ˆï¼‰()]", "", value)
        value = value.replace("æ§˜", "")
        return value or None

    def extract_company_name_from_deal(deal_name: str | None) -> str | None:
        if deal_name is None:
            return None
        raw = str(deal_name)
        if not raw.strip():
            return None
        patterns = [
            r"(?P<name>.+?)æ§˜å‘ã‘",
            r"(?P<name>.+?)å‘ã‘",
            r"(?P<name>.+?)æ§˜",
        ]
        for pattern in patterns:
            match = re.search(pattern, raw)
            if match:
                return match.group("name")
        return raw

    def extract_json_block(text: str) -> str:
        if not text:
            return ""
        start = text.find("{")
        end = text.rfind("}")
        if start == -1 or end == -1 or end <= start:
            return ""
        return text[start : end + 1]
    return (
        extract_company_name_from_deal,
        extract_json_block,
        normalize_company_name,
    )


@app.cell
def _(Path, bigquery, os, pd, service_account):
    # BigQueryæ¥ç¶šï¼ˆgree-dionysus-infoboxï¼‰
    BQ_PROJECT_ID = "gree-dionysus-infobox"
    BQ_DATASET_INTENT = "production_infobox"
    GA_DATASET_ID = "analytics_400693944"

    def get_bq_client():
        key_path = os.path.expanduser("~/.gcp/gree-dionysus-infobox.json")
        if Path(key_path).exists():
            credentials = service_account.Credentials.from_service_account_file(key_path)
            return bigquery.Client(project=BQ_PROJECT_ID, credentials=credentials)
        return bigquery.Client(project=BQ_PROJECT_ID)

    def query_bq(sql):
        """BigQueryã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        client = get_bq_client()
        job = client.query(sql)
        results = job.result()
        rows = [dict(row) for row in results]
        return pd.DataFrame(rows) if rows else pd.DataFrame()
    return GA_DATASET_ID, query_bq


@app.cell
def _(
    Path,
    default_backend,
    os,
    serialization,
    snowflake,
    snowflake_error,
    tomllib,
):
    SF_SCHEMA = "ETL_S3_TRANSALES_DB.TRANSALES_DAILY_SCHEMA"

    def get_snowflake_connection():
        if snowflake is None:
            raise RuntimeError(f"Snowflake connector not available: {snowflake_error}")

        config_path = Path.home() / ".snowflake" / "connections.toml"
        config = {}
        if config_path.exists():
            with open(config_path, "rb") as f:
                config = tomllib.load(f).get("default", {})

        account = os.getenv("SNOWFLAKE_ACCOUNT") or config.get("account")
        user = os.getenv("SNOWFLAKE_USER") or config.get("user")
        password = os.getenv("SNOWFLAKE_PASSWORD") or config.get("password")
        private_key_path = os.getenv("SNOWFLAKE_PRIVATE_KEY_PATH") or config.get("private_key_path")
        authenticator = os.getenv("SNOWFLAKE_AUTHENTICATOR") or config.get("authenticator")
        role = os.getenv("SNOWFLAKE_ROLE") or config.get("role")
        warehouse = os.getenv("SNOWFLAKE_WAREHOUSE") or config.get("warehouse")
        database = os.getenv("SNOWFLAKE_DATABASE") or config.get("database")
        schema = os.getenv("SNOWFLAKE_SCHEMA") or config.get("schema")

        if not account or not user:
            raise ValueError("Snowflakeæ¥ç¶šæƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™")

        connect_params = {"account": account, "user": user}

        if private_key_path:
            key_path = Path(private_key_path).expanduser()
            with open(key_path, "rb") as key_file:
                private_key = serialization.load_pem_private_key(
                    key_file.read(), password=None, backend=default_backend()
                )
            connect_params["private_key"] = private_key.private_bytes(
                encoding=serialization.Encoding.DER,
                format=serialization.PrivateFormat.PKCS8,
                encryption_algorithm=serialization.NoEncryption(),
            )
        elif password:
            connect_params["password"] = password

        if authenticator and not private_key_path:
            connect_params["authenticator"] = authenticator
        if role:
            connect_params["role"] = role
        if warehouse:
            connect_params["warehouse"] = warehouse
        if database:
            connect_params["database"] = database
        if schema:
            connect_params["schema"] = schema

        return snowflake.connector.connect(**connect_params)

    def query_sf(sql):
        """Snowflakeã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        conn = get_snowflake_connection()
        cur = conn.cursor()
        try:
            cur.execute(sql)
            return cur.fetch_pandas_all()
        finally:
            cur.close()
            conn.close()
    return SF_SCHEMA, query_sf


@app.cell
def _(mo):
    mo.md("""
    ## 1. ãƒãƒ£ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    """)
    return


@app.cell
def _(Path, normalize_company_name, pd):
    tsv_path = Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/docs/assets/infobox_data.tsv")
    df_churn_raw = pd.read_csv(tsv_path, sep="\t")

    # ã‚«ãƒ©ãƒ åã‚’æ­£è¦åŒ–
    df_churn_raw.columns = [c.strip() for c in df_churn_raw.columns]

    # ã‚¨ãƒ©ãƒ¼è¡Œã‚’é™¤å¤–ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡ã‚«ãƒ©ãƒ ã‚’æ¢ã™ï¼‰
    active_rate_col = [c for c in df_churn_raw.columns if "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡" in c]
    if active_rate_col:
        df_churn = df_churn_raw[
            ~df_churn_raw[active_rate_col[0]].astype(str).str.contains("ã‚¨ãƒ©ãƒ¼", na=False)
        ].copy()
    else:
        # ã‚¨ãƒ©ãƒ¼è¡ŒãŒã‚ã‚‹åˆ—ã‚’æ¢ã™
        df_churn = df_churn_raw[
            ~df_churn_raw.apply(lambda row: row.astype(str).str.contains("ã‚¨ãƒ©ãƒ¼").any(), axis=1)
        ].copy()

    # statusåˆ—ã‚’ãƒã‚¤ãƒŠãƒªåŒ–ï¼ˆå¤§æ–‡å­—å°æ–‡å­—å¯¾å¿œï¼‰
    status_col = "STATUS" if "STATUS" in df_churn.columns else "status"
    df_churn["is_churned"] = (df_churn[status_col] == "è§£ç´„æ¸ˆã¿").astype(int)
    df_churn["COMPNO"] = df_churn["COMPNO"].astype(str)

    # åˆ©ç”¨ã—ã‚„ã™ã„ã‚«ãƒ©ãƒ åã«ãƒªãƒãƒ¼ãƒ 
    rename_map = {
        "STATUS": "status",
        "COMPANY_NAME": "CompanyName",
        "ACCOUT_COUNT": "account_count",
    }
    # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ç‡ã‚«ãƒ©ãƒ 
    if active_rate_col:
        rename_map[active_rate_col[0]] = "active_rate"
    df_churn.rename(columns={k: v for k, v in rename_map.items() if k in df_churn.columns}, inplace=True)
    if "CompanyName" in df_churn.columns:
        df_churn["company_name_norm"] = df_churn["CompanyName"].apply(normalize_company_name)
    else:
        df_churn["company_name_norm"] = None
    return df_churn, df_churn_raw


@app.cell
def _(df_churn, df_churn_raw, mo):
    mo.md(f"""
    **ãƒãƒ£ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿**:
    - å…¨è¡Œæ•°: {len(df_churn_raw):,}
    - ã‚¨ãƒ©ãƒ¼é™¤å¤–å¾Œ: {len(df_churn):,}
    - è§£ç´„æ¸ˆã¿: {df_churn['is_churned'].sum():,}
    - å¥‘ç´„ä¸­: {(~df_churn['is_churned'].astype(bool)).sum():,}
    """)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 1-b. è§£ç´„ç†ç”±ãƒ‡ãƒ¼ã‚¿ï¼ˆCSï¼‰
    """)
    return


@app.cell
def _(Path, extract_company_name_from_deal, normalize_company_name, pd):
    churn_reason_path = Path(
        "/Users/kou1904/githubactions_fordata/work/aieda_agent/docs/assets/ã€CSã€‘è§£ç´„é¡§å®¢ä¸€è¦§_25å¹´1æœˆã€œ - ã‚·ãƒ¼ãƒˆ1.csv"
    )
    df_churn_reason_raw = pd.DataFrame()
    df_churn_reason_latest = pd.DataFrame()

    if churn_reason_path.exists():
        df_churn_reason_raw = pd.read_csv(churn_reason_path)
        df_churn_reason = df_churn_reason_raw.copy()

        if "å•†è«‡å" in df_churn_reason.columns:
            df_churn_reason["deal_company_name"] = df_churn_reason["å•†è«‡å"].apply(
                extract_company_name_from_deal
            )
        else:
            df_churn_reason["deal_company_name"] = None

        df_churn_reason["company_name_norm"] = df_churn_reason["deal_company_name"].apply(
            normalize_company_name
        )

        loss_date_col = "ç¾å¥‘ç´„çµ‚äº†æ—¥" if "ç¾å¥‘ç´„çµ‚äº†æ—¥" in df_churn_reason.columns else None
        if loss_date_col is None and "å®Œäº†äºˆå®šæ—¥" in df_churn_reason.columns:
            loss_date_col = "å®Œäº†äºˆå®šæ—¥"

        if loss_date_col:
            df_churn_reason["loss_end_date"] = pd.to_datetime(
                df_churn_reason[loss_date_col], errors="coerce"
            )
        else:
            df_churn_reason["loss_end_date"] = pd.NaT

        df_churn_reason_sorted = df_churn_reason.sort_values("loss_end_date")
        df_churn_reason_latest = (
            df_churn_reason_sorted.dropna(subset=["company_name_norm"])
            .groupby("company_name_norm")
            .tail(1)
            .copy()
        )

        churn_reason_rename_map = {
            "å•†è«‡å": "deal_name",
            "å¤±æ³¨ç¨®åˆ¥": "loss_type",
            "å¤±æ³¨ç†ç”±": "loss_reason",
            "å—æ³¨/å¤±æ³¨ç†ç”±è©³ç´°": "loss_detail",
            "å•†è«‡ æ‰€æœ‰è€…": "deal_owner",
            "ç¾å¥‘ç´„çµ‚äº†æ—¥": "current_contract_end_date",
            "å®Œäº†äºˆå®šæ—¥": "planned_close_date",
        }
        needed_cols = ["company_name_norm", "deal_company_name"] + [
            col for col in churn_reason_rename_map if col in df_churn_reason_latest.columns
        ]
        df_churn_reason_latest = df_churn_reason_latest[needed_cols].rename(
            columns=churn_reason_rename_map
        )
    return df_churn_reason_latest, df_churn_reason_raw


@app.cell
def _(df_churn_reason_latest, df_churn_reason_raw, mo):
    mo.md(f"""
    **è§£ç´„ç†ç”±ãƒ‡ãƒ¼ã‚¿**:
    - å…¨è¡Œæ•°: {len(df_churn_reason_raw):,}
    - æŠ½å‡ºä¼šç¤¾æ•°: {len(df_churn_reason_latest):,}
    """)
    return


@app.cell
def _(df_churn_reason_latest, mo):
    mo.ui.table(df_churn_reason_latest.head(20), pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 2. ã‚«ãƒ†ã‚´ãƒªæ¢ç´¢ï¼ˆå–¶æ¥­æ”¯æ´/CRM/SFAé–¢é€£ï¼‰
    """)
    return


@app.cell
def _(mo, query_bq):
    # ã‚«ãƒ†ã‚´ãƒªä¸€è¦§å–å¾—
    category_query = """
    SELECT DISTINCT 
        original_category_name as category,
        COUNT(*) as count
    FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3`
    WHERE view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
    GROUP BY 1
    ORDER BY count DESC
    LIMIT 100
    """
    df_categories = query_bq(category_query)
    mo.md(f"**ã‚«ãƒ†ã‚´ãƒªæ•°ï¼ˆç›´è¿‘30æ—¥ï¼‰**: {len(df_categories):,}")
    return (df_categories,)


@app.cell
def _(df_categories, mo):
    mo.ui.table(df_categories, pagination=True)
    return


@app.cell
def _(mo):
    # å–¶æ¥­æ”¯æ´é–¢é€£ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚£ãƒ«ã‚¿
    sales_keywords = ["å–¶æ¥­", "CRM", "SFA", "ã‚»ãƒ¼ãƒ«ã‚¹", "ãƒªãƒ¼ãƒ‰", "é¡§å®¢ç®¡ç†", "å•†è«‡", "MA", "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°"]
    category_filter = mo.ui.text(
        label="ã‚«ãƒ†ã‚´ãƒªæ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
        value=",".join(sales_keywords),
    )
    category_filter
    return (category_filter,)


@app.cell
def _(category_filter, df_categories, mo, pd):
    # ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    keywords = [k.strip() for k in category_filter.value.split(",") if k.strip()]
    df_sales_categories = pd.DataFrame()

    if len(df_categories) > 0 and keywords:
        pattern = "|".join(keywords)
        df_sales_categories = df_categories[
            df_categories["category"].str.contains(pattern, case=False, na=False)
        ].copy()
        mo.md(f"**å–¶æ¥­æ”¯æ´é–¢é€£ã‚«ãƒ†ã‚´ãƒª**: {len(df_sales_categories):,} ä»¶")
    return (df_sales_categories,)


@app.cell
def _(df_sales_categories, mo):
    mo.ui.table(df_sales_categories, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 3. ç«¶åˆã‚¤ãƒ³ãƒ†ãƒ³ãƒˆåˆ†æï¼ˆå–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªï¼‰
    """)
    return


@app.cell
def _(df_sales_categories, mo, pd, query_bq):
    # å–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªã®ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆå¤‰å‹•ã‚’å–å¾—
    df_competitor_intent = pd.DataFrame()

    if len(df_sales_categories) > 0:
        categories_list = df_sales_categories["category"].tolist()[:10]  # ä¸Šä½10ã‚«ãƒ†ã‚´ãƒª
        categories_sql = ",".join([f"'{c}'" for c in categories_list])

        competitor_intent_query = f"""
        SELECT 
            c.original_category_name as category,
            s.intent_level,
            CASE s.intent_level 
                WHEN 1 THEN 'Low' 
                WHEN 2 THEN 'Middle' 
                WHEN 3 THEN 'High' 
            END as level_name,
            COUNT(DISTINCT s.corporate_id) as company_count
        FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3` c
        JOIN `gree-dionysus-infobox.production_infobox.first_party_score_company_latest` s
          ON CAST(c.corporate_id AS INT64) = s.corporate_id
        WHERE c.view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 7 DAY)
          AND c.original_category_name IN ({categories_sql})
          AND s.intent_level IN (2, 3)
        GROUP BY 1, 2, 3
        ORDER BY 1, 2
        """
        df_competitor_intent = query_bq(competitor_intent_query)
    mo.md(f"**ç«¶åˆã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼ˆå–¶æ¥­æ”¯æ´ã‚«ãƒ†ã‚´ãƒªï¼‰**: {len(df_competitor_intent):,} ä»¶")
    return (df_competitor_intent,)


@app.cell
def _(df_competitor_intent, mo):
    mo.ui.table(df_competitor_intent, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 4. IDç´ã¥ã‘ï¼ˆCOMPNO â†’ ORGID â†’ corporate_idï¼‰
    """)
    return


@app.cell
def _(SF_SCHEMA, mo, pd, query_sf):
    # Snowflake: BEEGLECOMPANY + USERORGANIZATION ã§ORGIDã‚’å–å¾—
    # BeegleCompany.ID â†’ USERORGANIZATION.COMPANYID â†’ USERORGANIZATION.ORGIDï¼ˆGAã®org_idã¨ä¸€è‡´ï¼‰
    df_id_mapping = pd.DataFrame()

    try:
        mapping_query = f"""
        SELECT
            bc.COMPNO,
            bc.ID AS COMPANYID,
            u.ORGID,
            u.NAME AS ORG_NAME,
            bc.SHOGO AS BQ_COMPANY_NAME,
            bc.KANA AS BQ_COMPANY_KANA,
            bc.CEO,
            bc.SETURITU AS ESTABLISHMENT,
            bc.ZIP,
            bc.ADD AS ADDRESS,
            bc.PREFID,
            bc.CITYID,
            bc.TEL AS PHONE,
            bc.HPURL AS WEBSITE_URL,
            bc.MAIL,
            bc.GYOSHUSHOID AS INDUSTRY_ID,
            bc.EMPID AS EMPLOYEE_ID,
            bc.EMPCOUNT AS EMPLOYEE_COUNT,
            bc.REVENUEID AS REVENUE_ID,
            bc.SHIHONID AS CAPITAL_ID,
            bc.ISCLOSED,
            bc.CREATEDAT AS BQ_CREATED_AT,
            u.CREATEDAT AS ORG_CREATED_AT
        FROM {SF_SCHEMA}.BEEGLECOMPANY bc
        JOIN {SF_SCHEMA}.USERORGANIZATION u
            ON bc.ID = u.COMPANYID
        WHERE bc.COMPNO IS NOT NULL
        """
        df_id_mapping = query_sf(mapping_query)
        df_id_mapping["COMPNO"] = df_id_mapping["COMPNO"].astype(str)
        mo.md(f"**BeegleCompany + USERORGANIZATION ãƒãƒƒãƒ”ãƒ³ã‚°ä»¶æ•°**: {len(df_id_mapping):,}")
    except Exception as e:
        mo.md(f"**Snowflakeæ¥ç¶šã‚¨ãƒ©ãƒ¼**: `{e}`")
    return (df_id_mapping,)


@app.cell
def _(df_id_mapping, mo):
    mo.ui.table(df_id_mapping.head(20), pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 5. GAåˆ©ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒšãƒ¼ã‚¸ç¾¤åˆ¥ï¼‰
    """)
    return


@app.cell
def _(GA_DATASET_ID, mo, query_bq):
    # GA4 org_idå˜ä½ã®åŸºæœ¬æŒ‡æ¨™ + ãƒšãƒ¼ã‚¸ç¾¤åˆ¥
    ga_query = f"""
    WITH base AS (
        SELECT
            user_pseudo_id,
            (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS ga_session_id,
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name,
            event_date
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            user_pseudo_id,
            ga_session_id,
            event_name,
            CASE
                WHEN REGEXP_CONTAINS(page_location, r'/list') THEN 'list'
                WHEN REGEXP_CONTAINS(page_location, r'/company') THEN 'company'
                WHEN REGEXP_CONTAINS(page_location, r'/download|csv') THEN 'download'
                WHEN REGEXP_CONTAINS(page_location, r'/search') THEN 'search'
                WHEN REGEXP_CONTAINS(page_location, r'/settings') THEN 'settings'
                ELSE 'other'
            END AS page_group
        FROM base
        WHERE org_id IS NOT NULL
    )
    SELECT
        org_id,
        COUNT(DISTINCT user_pseudo_id) AS users,
        COUNT(DISTINCT CONCAT(user_pseudo_id, '-', CAST(ga_session_id AS STRING))) AS sessions,
        COUNTIF(event_name = 'page_view') AS page_views,
        COUNTIF(page_group = 'list' AND event_name = 'page_view') AS pv_list,
        COUNTIF(page_group = 'company' AND event_name = 'page_view') AS pv_company,
        COUNTIF(page_group = 'download' AND event_name = 'page_view') AS pv_download,
        COUNTIF(page_group = 'search' AND event_name = 'page_view') AS pv_search,
        COUNTIF(page_group = 'settings' AND event_name = 'page_view') AS pv_settings
    FROM classified
    GROUP BY org_id
    """
    df_ga = query_bq(ga_query)
    mo.md(f"**GAåˆ©ç”¨ãƒ‡ãƒ¼ã‚¿ï¼ˆorg_idå˜ä½ï¼‰**: {len(df_ga):,} ä»¶")
    return (df_ga,)


@app.cell
def _(df_ga, mo):
    mo.ui.table(df_ga.head(20), pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ### otherå†…è¨³ï¼ˆpage_location ä¸Šä½ï¼‰
    """)
    return


@app.cell
def _(GA_DATASET_ID, mo, query_bq):
    other_overall_query = f"""
    WITH base AS (
        SELECT
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            page_location,
            event_name,
            CASE
                WHEN REGEXP_CONTAINS(page_location, r'/list') THEN 'list'
                WHEN REGEXP_CONTAINS(page_location, r'/company') THEN 'company'
                WHEN REGEXP_CONTAINS(page_location, r'/download|csv') THEN 'download'
                WHEN REGEXP_CONTAINS(page_location, r'/search') THEN 'search'
                WHEN REGEXP_CONTAINS(page_location, r'/settings') THEN 'settings'
                ELSE 'other'
            END AS page_group
        FROM base
        WHERE org_id IS NOT NULL
          AND page_location IS NOT NULL
    )
    SELECT
        page_location,
        COUNTIF(event_name = 'page_view') AS page_views
    FROM classified
    WHERE page_group = 'other'
    GROUP BY page_location
    ORDER BY page_views DESC
    LIMIT 50
    """

    other_by_org_query = f"""
    WITH base AS (
        SELECT
            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location,
            event_name
        FROM `{GA_DATASET_ID}.events_*`
        WHERE NOT STARTS_WITH(_TABLE_SUFFIX, 'intraday_')
          AND _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY))
    ),
    classified AS (
        SELECT
            org_id,
            page_location,
            event_name,
            CASE
                WHEN REGEXP_CONTAINS(page_location, r'/list') THEN 'list'
                WHEN REGEXP_CONTAINS(page_location, r'/company') THEN 'company'
                WHEN REGEXP_CONTAINS(page_location, r'/download|csv') THEN 'download'
                WHEN REGEXP_CONTAINS(page_location, r'/search') THEN 'search'
                WHEN REGEXP_CONTAINS(page_location, r'/settings') THEN 'settings'
                ELSE 'other'
            END AS page_group
        FROM base
        WHERE org_id IS NOT NULL
          AND page_location IS NOT NULL
    )
    SELECT
        org_id,
        page_location,
        COUNTIF(event_name = 'page_view') AS page_views
    FROM classified
    WHERE page_group = 'other'
    GROUP BY org_id, page_location
    ORDER BY page_views DESC
    LIMIT 200
    """

    df_other_pages = query_bq(other_overall_query)
    df_other_pages_by_org = query_bq(other_by_org_query)
    mo.md(
        f"**otherå†…è¨³**: å…¨ä½“ {len(df_other_pages):,} ä»¶ / org_idåˆ¥ {len(df_other_pages_by_org):,} ä»¶"
    )
    return df_other_pages, df_other_pages_by_org


@app.cell
def _(df_other_pages, df_other_pages_by_org, mo):
    mo.md("#### å…¨ä½“ä¸Šä½")
    mo.ui.table(df_other_pages, pagination=True)
    mo.md("#### org_idåˆ¥ ä¸Šä½")
    mo.ui.table(df_other_pages_by_org, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 6. Liståˆ†æï¼ˆCompanyList / PeopleListï¼‰
    """)
    return


@app.cell
def _(SF_SCHEMA, df_id_mapping, mo, pd, query_sf):
    debug_outputs = []  # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’åé›†
    df_list_summary = pd.DataFrame()
    df_companylist_detail = pd.DataFrame()
    df_peoplelist_detail = pd.DataFrame()
    list_orgid_truncated = False

    list_orgids = (
        df_id_mapping["ORGID"].dropna().astype(str).unique().tolist()
        if len(df_id_mapping) > 0 and "ORGID" in df_id_mapping.columns
        else []
    )
    
    # ãƒ‡ãƒãƒƒã‚°: list_orgids ã®çŠ¶æ…‹ã‚’å¸¸ã«è¡¨ç¤º
    debug_outputs.append(mo.md(f"**Liståˆ†æãƒ‡ãƒãƒƒã‚°**: df_id_mapping={len(df_id_mapping)}è¡Œ, list_orgids={len(list_orgids)}ä»¶"))
    if list_orgids:
        debug_outputs.append(mo.md(f"**ORGIDã‚µãƒ³ãƒ—ãƒ«ï¼ˆdf_id_mappingï¼‰**: {list_orgids[:3]}"))
    
    if list_orgids:
        list_orgid_limit = 500
        if len(list_orgids) > list_orgid_limit:
            list_orgid_truncated = True
        list_orgids_sql = ",".join([f"'{orgid}'" for orgid in list_orgids[:list_orgid_limit]])

        try:
            # ãƒ‡ãƒãƒƒã‚°1: å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®å…¨ä½“åƒ
            debug_counts_query = f"""
            SELECT
                (SELECT COUNT(*) FROM {SF_SCHEMA}.USERORGANIZATION) AS total_userorg,
                (SELECT COUNT(*) FROM {SF_SCHEMA}.USERORGRELATION) AS total_userorgrel,
                (SELECT COUNT(*) FROM {SF_SCHEMA}.COMPANYLIST) AS total_companylist,
                (SELECT COUNT(*) FROM {SF_SCHEMA}.PEOPLELIST) AS total_peoplelist
            """
            df_debug_counts = query_sf(debug_counts_query)
            if len(df_debug_counts) > 0:
                dc = df_debug_counts.iloc[0].to_dict()
                debug_outputs.append(mo.md(
                    "**ãƒ†ãƒ¼ãƒ–ãƒ«å…¨ä½“ä»¶æ•°**: "
                    + f"USERORGANIZATION={dc.get('TOTAL_USERORG', dc.get('total_userorg'))}, "
                    + f"USERORGRELATION={dc.get('TOTAL_USERORGREL', dc.get('total_userorgrel'))}, "
                    + f"COMPANYLIST={dc.get('TOTAL_COMPANYLIST', dc.get('total_companylist'))}, "
                    + f"PEOPLELIST={dc.get('TOTAL_PEOPLELIST', dc.get('total_peoplelist'))}"
                ))
            
            # ãƒ‡ãƒãƒƒã‚°2: ORGIDã‚µãƒ³ãƒ—ãƒ«æ¯”è¼ƒï¼ˆå„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç‹¬ç«‹ã—ã¦å–å¾—ï¼‰
            userorg_sample_query = f"""
            SELECT ORGID
            FROM {SF_SCHEMA}.USERORGANIZATION
            LIMIT 3
            """
            df_userorg_sample = query_sf(userorg_sample_query)
            
            userorgrel_sample_query = f"""
            SELECT ORGANIZATIONID
            FROM {SF_SCHEMA}.USERORGRELATION
            LIMIT 3
            """
            df_userorgrel_sample = query_sf(userorgrel_sample_query)
            
            debug_outputs.append(mo.md(
                "**USERORGANIZATION.ORGID ã‚µãƒ³ãƒ—ãƒ«**:\n"
                + "\n".join([f"- `{row.get('ORGID', row.get('orgid'))}`" 
                           for _, row in df_userorg_sample.iterrows()])
            ))
            debug_outputs.append(mo.md(
                "**USERORGRELATION.ORGANIZATIONID ã‚µãƒ³ãƒ—ãƒ«**:\n"
                + "\n".join([f"- `{row.get('ORGANIZATIONID', row.get('organizationid'))}`" 
                           for _, row in df_userorgrel_sample.iterrows()])
            ))
            
            # ãƒ‡ãƒãƒƒã‚°3: JOINçµæœç¢ºèªï¼ˆCASTç„¡ã—ã§ç›´æ¥æ¯”è¼ƒï¼‰
            join_test_query = f"""
            SELECT
                COUNT(DISTINCT u.ORGID) AS userorg_matched,
                COUNT(DISTINCT ur.ORGANIZATIONID) AS userorgrel_matched
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur
                ON u.ORGID = ur.ORGANIZATIONID
            """
            df_join_test = query_sf(join_test_query)
            if len(df_join_test) > 0:
                jt = df_join_test.iloc[0].to_dict()
                debug_outputs.append(mo.md(
                    f"**JOINç›´æ¥æ¯”è¼ƒï¼ˆCASTç„¡ã—ï¼‰**: userorg_matched={jt.get('USERORG_MATCHED', jt.get('userorg_matched'))}, "
                    + f"userorgrel_matched={jt.get('USERORGREL_MATCHED', jt.get('userorgrel_matched'))}"
                ))
            
            # ãƒ‡ãƒãƒƒã‚°4: ãƒ•ã‚£ãƒ«ã‚¿å¾ŒJOINçµæœ
            list_debug_query = f"""
            SELECT
                COUNT(DISTINCT CAST(u.ORGID AS STRING)) AS userorg_count,
                COUNT(DISTINCT CAST(ur.ORGANIZATIONID AS STRING)) AS userorgrel_org_count,
                COUNT(DISTINCT cl.ID) AS companylist_count,
                COUNT(DISTINCT pl.ID) AS peoplelist_count
            FROM {SF_SCHEMA}.USERORGANIZATION u
            LEFT JOIN {SF_SCHEMA}.USERORGRELATION ur
                ON CAST(u.ORGID AS STRING) = CAST(ur.ORGANIZATIONID AS STRING)
            LEFT JOIN {SF_SCHEMA}.COMPANYLIST cl
                ON cl.USERORGRELATIONID = ur.ID
            LEFT JOIN {SF_SCHEMA}.PEOPLELIST pl
                ON pl.USERORGRELATIONID = ur.ID
            WHERE CAST(u.ORGID AS STRING) IN ({list_orgids_sql})
            """
            df_list_debug = query_sf(list_debug_query)
            if len(df_list_debug) > 0:
                debug_row = df_list_debug.iloc[0].to_dict()
                debug_outputs.append(mo.md(
                    "**ãƒ•ã‚£ãƒ«ã‚¿å¾ŒJOINçµæœ**: "
                    + f"userorg={debug_row.get('USERORG_COUNT', debug_row.get('userorg_count'))}, "
                    + f"userorgrel={debug_row.get('USERORGREL_ORG_COUNT', debug_row.get('userorgrel_org_count'))}, "
                    + f"companylist={debug_row.get('COMPANYLIST_COUNT', debug_row.get('companylist_count'))}, "
                    + f"peoplelist={debug_row.get('PEOPLELIST_COUNT', debug_row.get('peoplelist_count'))}"
                ))

            # Step 1: org_rel ã ã‘å–å¾—ã—ã¦ãƒ‡ãƒãƒƒã‚°
            debug_outputs.append(mo.md("ğŸ”„ list_summary_queryå®Ÿè¡Œä¸­..."))
            
            org_rel_query = f"""
            SELECT CAST(u.ORGID AS STRING) AS ORGID, ur.ID AS USERORGRELATIONID
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur
                ON u.ORGID = ur.ORGANIZATIONID
            WHERE u.ORGID IN ({list_orgids_sql})
            """
            df_org_rel = query_sf(org_rel_query)
            debug_outputs.append(mo.md(f"âœ… org_relå–å¾—: {len(df_org_rel)}è¡Œ"))
            
            if len(df_org_rel) == 0:
                debug_outputs.append(mo.md("âš ï¸ org_relãŒ0è¡Œã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—"))
            else:
                # USERORGRELATIONIDã®ãƒªã‚¹ãƒˆä½œæˆ
                userorgrel_ids = df_org_rel["USERORGRELATIONID"].dropna().unique().tolist()
                debug_outputs.append(mo.md(f"âœ… USERORGRELATIONID: {len(userorgrel_ids)}ä»¶"))
                
                if userorgrel_ids:
                    userorgrel_ids_sql = ",".join([f"'{uid}'" for uid in userorgrel_ids[:500]])
                    
                    # CompanyListé›†è¨ˆ
                    companylist_query = f"""
                    SELECT 
                        USERORGRELATIONID,
                        COUNT(*) AS list_count
                    FROM {SF_SCHEMA}.COMPANYLIST
                    WHERE USERORGRELATIONID IN ({userorgrel_ids_sql})
                    GROUP BY USERORGRELATIONID
                    """
                    df_companylist_agg = query_sf(companylist_query)
                    debug_outputs.append(mo.md(f"âœ… CompanyListé›†è¨ˆ: {len(df_companylist_agg)}è¡Œ, åˆè¨ˆ={df_companylist_agg['LIST_COUNT'].sum() if len(df_companylist_agg) > 0 and 'LIST_COUNT' in df_companylist_agg.columns else 0}"))
                    
                    # PeopleListé›†è¨ˆ
                    peoplelist_query = f"""
                    SELECT 
                        USERORGRELATIONID,
                        COUNT(*) AS list_count
                    FROM {SF_SCHEMA}.PEOPLELIST
                    WHERE USERORGRELATIONID IN ({userorgrel_ids_sql})
                    GROUP BY USERORGRELATIONID
                    """
                    df_peoplelist_agg = query_sf(peoplelist_query)
                    debug_outputs.append(mo.md(f"âœ… PeopleListé›†è¨ˆ: {len(df_peoplelist_agg)}è¡Œ, åˆè¨ˆ={df_peoplelist_agg['LIST_COUNT'].sum() if len(df_peoplelist_agg) > 0 and 'LIST_COUNT' in df_peoplelist_agg.columns else 0}"))
                    
                    # org_rel ã¨é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸
                    df_list_summary = df_org_rel[["ORGID", "USERORGRELATIONID"]].drop_duplicates()
                    
                    if len(df_companylist_agg) > 0:
                        col_name = "LIST_COUNT" if "LIST_COUNT" in df_companylist_agg.columns else "list_count"
                        df_companylist_agg = df_companylist_agg.rename(columns={col_name: "companylist_count"})
                        uorid_col = "USERORGRELATIONID" if "USERORGRELATIONID" in df_companylist_agg.columns else "userorgrelationid"
                        df_companylist_agg = df_companylist_agg.rename(columns={uorid_col: "USERORGRELATIONID"})
                        df_list_summary = df_list_summary.merge(
                            df_companylist_agg[["USERORGRELATIONID", "companylist_count"]],
                            on="USERORGRELATIONID", how="left"
                        )
                    else:
                        df_list_summary["companylist_count"] = 0
                    
                    if len(df_peoplelist_agg) > 0:
                        col_name = "LIST_COUNT" if "LIST_COUNT" in df_peoplelist_agg.columns else "list_count"
                        df_peoplelist_agg = df_peoplelist_agg.rename(columns={col_name: "peoplelist_count"})
                        uorid_col = "USERORGRELATIONID" if "USERORGRELATIONID" in df_peoplelist_agg.columns else "userorgrelationid"
                        df_peoplelist_agg = df_peoplelist_agg.rename(columns={uorid_col: "USERORGRELATIONID"})
                        df_list_summary = df_list_summary.merge(
                            df_peoplelist_agg[["USERORGRELATIONID", "peoplelist_count"]],
                            on="USERORGRELATIONID", how="left"
                        )
                    else:
                        df_list_summary["peoplelist_count"] = 0
                    
                    df_list_summary["companylist_count"] = df_list_summary["companylist_count"].fillna(0).astype(int)
                    df_list_summary["peoplelist_count"] = df_list_summary["peoplelist_count"].fillna(0).astype(int)
                    
                    # ORGIDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦é›†è¨ˆ
                    df_list_summary = df_list_summary.groupby("ORGID").agg({
                        "companylist_count": "sum",
                        "peoplelist_count": "sum"
                    }).reset_index()
                    
                    debug_outputs.append(mo.md(f"âœ… df_list_summaryæ§‹ç¯‰å®Œäº†: {len(df_list_summary)}è¡Œ"))

            # CompanyListè©³ç´°ï¼ˆæ¥­ç¨®ãƒ»åœ°åŸŸã‚«ãƒ©ãƒ è¿½åŠ ï¼‰
            companylist_detail_query = f"""
            SELECT
                u.ORGID,
                cl.ID AS LIST_ID,
                cl.CREATEDAT AS LIST_CREATED_AT,
                bc.ID AS COMPANY_ID,
                bc.SHOGO AS COMPANY_NAME,
                bc.GYOSHUSHOID AS INDUSTRY_ID,
                bc.PREFID,
                bc.EMPCOUNT AS EMPLOYEE_COUNT
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
            JOIN {SF_SCHEMA}.COMPANYLIST cl ON cl.USERORGRELATIONID = ur.ID
            JOIN {SF_SCHEMA}._BEEGLECOMPANYTOCOMPANYLIST rel ON rel.B = cl.ID
            JOIN {SF_SCHEMA}.BEEGLECOMPANY bc ON rel.A = bc.ID
            WHERE u.ORGID IN ({list_orgids_sql})
            """
            df_companylist_detail = query_sf(companylist_detail_query)
            debug_outputs.append(mo.md(f"âœ… CompanyListè©³ç´°: {len(df_companylist_detail)}è¡Œ"))

            # PeopleListè©³ç´°ï¼ˆä¼æ¥­IDã‚‚è¿½åŠ ï¼‰
            peoplelist_detail_query = f"""
            SELECT
                u.ORGID,
                pl.ID AS LIST_ID,
                pl.CREATEDAT AS LIST_CREATED_AT,
                km.ID AS KEYMAN_ID,
                km.NAME AS KEYMAN_NAME
            FROM {SF_SCHEMA}.USERORGANIZATION u
            JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
            JOIN {SF_SCHEMA}.PEOPLELIST pl ON pl.USERORGRELATIONID = ur.ID
            JOIN {SF_SCHEMA}._KEYMANTOPEOPLELIST rel ON rel.B = pl.ID
            JOIN {SF_SCHEMA}.KEYMAN km ON rel.A = km.ID
            WHERE u.ORGID IN ({list_orgids_sql})
            """
            df_peoplelist_detail = query_sf(peoplelist_detail_query)
            debug_outputs.append(mo.md(f"âœ… PeopleListè©³ç´°: {len(df_peoplelist_detail)}è¡Œ"))

            if len(df_companylist_detail) > 0:
                company_counts = (
                    df_companylist_detail.groupby("ORGID")["COMPANY_ID"].nunique().reset_index()
                )
                company_counts.rename(
                    columns={"COMPANY_ID": "companylist_company_count"}, inplace=True
                )
                company_top_series = (
                    df_companylist_detail.groupby("ORGID")["COMPANY_NAME"]
                    .apply(lambda series: " / ".join(sorted(series.dropna().unique())[:5]))
                    .reset_index()
                    .rename(columns={"COMPANY_NAME": "companylist_top_companies"})
                )
                df_list_summary = df_list_summary.merge(company_counts, on="ORGID", how="left")
                df_list_summary = df_list_summary.merge(
                    company_top_series, on="ORGID", how="left"
                )

            if len(df_peoplelist_detail) > 0:
                people_counts = (
                    df_peoplelist_detail.groupby("ORGID")["KEYMAN_ID"].nunique().reset_index()
                )
                people_counts.rename(
                    columns={"KEYMAN_ID": "peoplelist_keyman_count"}, inplace=True
                )
                people_top_series = (
                    df_peoplelist_detail.groupby("ORGID")["KEYMAN_NAME"]
                    .apply(lambda series: " / ".join(sorted(series.dropna().astype(str).unique())[:5]))
                    .reset_index()
                    .rename(columns={"KEYMAN_NAME": "peoplelist_top_keymen"})
                )
                df_list_summary = df_list_summary.merge(people_counts, on="ORGID", how="left")
                df_list_summary = df_list_summary.merge(
                    people_top_series, on="ORGID", how="left"
                )

            for col_name in [
                "companylist_company_count",
                "peoplelist_keyman_count",
                "companylist_top_companies",
                "peoplelist_top_keymen",
            ]:
                if col_name not in df_list_summary.columns:
                    df_list_summary[col_name] = None

        except Exception as exc:
            import traceback
            tb = traceback.format_exc()
            debug_outputs.append(mo.md(f"**Liståˆ†æã‚¯ã‚¨ãƒªã‚¨ãƒ©ãƒ¼**: `{type(exc).__name__}: {exc}`\n\n```\n{tb}\n```"))
            df_list_summary = pd.DataFrame()
            df_companylist_detail = pd.DataFrame()
            df_peoplelist_detail = pd.DataFrame()

    if list_orgid_truncated:
        debug_outputs.append(mo.md("*ORGIDãŒå¤šã„ãŸã‚å…ˆé ­500ä»¶ã®ã¿ã§Liståˆ†æã—ã¦ã„ã¾ã™*"))
    
    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º
    debug_outputs.append(mo.md(f"**æœ€çµ‚çµæœ**: df_list_summary={len(df_list_summary)}è¡Œ"))
    mo.vstack(debug_outputs)
    return df_companylist_detail, df_list_summary, df_peoplelist_detail


@app.cell
def _(df_list_summary, mo):
    mo.ui.table(df_list_summary.head(20), pagination=True)
    return


@app.cell
def _(df_id_mapping, df_list_summary, mo):
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    debug_lines = ["**List Dropdown ãƒ‡ãƒãƒƒã‚°æƒ…å ±**:"]
    
    debug_lines.append(f"- df_id_mapping: {len(df_id_mapping)} è¡Œ")
    debug_lines.append(f"- df_list_summary: {len(df_list_summary)} è¡Œ")
    
    if len(df_id_mapping) > 0 and "ORGID" in df_id_mapping.columns:
        id_map_orgids = df_id_mapping["ORGID"].dropna()
        debug_lines.append(f"- df_id_mapping ORGIDå‹: {id_map_orgids.dtype}")
        debug_lines.append(f"- df_id_mapping ORGIDã‚µãƒ³ãƒ—ãƒ«: {id_map_orgids.head(3).tolist()}")
    
    if len(df_list_summary) > 0 and "ORGID" in df_list_summary.columns:
        list_sum_orgids = df_list_summary["ORGID"].dropna()
        debug_lines.append(f"- df_list_summary ORGIDå‹: {list_sum_orgids.dtype}")
        debug_lines.append(f"- df_list_summary ORGIDã‚µãƒ³ãƒ—ãƒ«: {list_sum_orgids.head(3).tolist()}")
        
        # ãƒªã‚¹ãƒˆãŒã‚ã‚‹è¡Œã®ã‚«ã‚¦ãƒ³ãƒˆ
        has_companylist = (df_list_summary["companylist_count"].fillna(0) > 0).sum()
        has_peoplelist = (df_list_summary["peoplelist_count"].fillna(0) > 0).sum()
        debug_lines.append(f"- companylist_count > 0: {has_companylist} è¡Œ")
        debug_lines.append(f"- peoplelist_count > 0: {has_peoplelist} è¡Œ")
        
        # companylist_count, peoplelist_count ã®åˆ†å¸ƒ
        if "companylist_count" in df_list_summary.columns:
            debug_lines.append(f"- companylist_count æœ€å¤§: {df_list_summary['companylist_count'].max()}")
        if "peoplelist_count" in df_list_summary.columns:
            debug_lines.append(f"- peoplelist_count æœ€å¤§: {df_list_summary['peoplelist_count'].max()}")
    
    mo.md("\n".join(debug_lines))
    
    list_options = {}
    if len(df_id_mapping) > 0 and len(df_list_summary) > 0:
        list_name_cols = [c for c in ["BQ_COMPANY_NAME", "ORG_NAME"] if c in df_id_mapping.columns]
        df_list_option_base = df_id_mapping.dropna(subset=["ORGID"]).copy()
        if list_name_cols:
            df_list_option_base["company_label"] = df_list_option_base[list_name_cols[0]].astype(str)
        else:
            df_list_option_base["company_label"] = df_list_option_base["ORGID"].astype(str)

        orgids_with_lists = df_list_summary[
            (df_list_summary["companylist_count"].fillna(0) > 0)
            | (df_list_summary["peoplelist_count"].fillna(0) > 0)
        ]["ORGID"].astype(str).unique().tolist()
        
        debug_lines2 = [f"- orgids_with_lists: {len(orgids_with_lists)} ä»¶"]
        if orgids_with_lists:
            debug_lines2.append(f"- orgids_with_listsã‚µãƒ³ãƒ—ãƒ«: {orgids_with_lists[:3]}")
        mo.md("\n".join(debug_lines2))

        for _, list_row in df_list_option_base.drop_duplicates(subset=["ORGID"]).iterrows():
            if str(list_row["ORGID"]) in orgids_with_lists:
                list_label = f"{list_row['company_label']} ({list_row['ORGID']})"
                list_options[list_label] = list_row["ORGID"]

    list_org_options = list_options
    list_default_value = next(iter(list_org_options.keys()), None) if list_org_options else None
    list_org_selector = mo.ui.dropdown(
        options=list_org_options,
        label="Liståˆ†æ: ä¼æ¥­ã‚’é¸æŠï¼ˆãƒªã‚¹ãƒˆ1ä»¶ä»¥ä¸Šï¼‰",
        value=list_default_value,
    )
    list_org_selector
    return list_org_options, list_org_selector


@app.cell
def _(
    df_companylist_detail,
    df_list_summary,
    df_peoplelist_detail,
    list_org_options,
    list_org_selector,
    pd,
):
    df_list_summary_selected = pd.DataFrame()
    df_companylist_detail_selected = pd.DataFrame()
    df_peoplelist_detail_selected = pd.DataFrame()

    selected_orgid = (
        list_org_options.get(list_org_selector.value) if list_org_selector.value else None
    )

    if selected_orgid:
        df_list_summary_selected = df_list_summary[
            df_list_summary["ORGID"] == selected_orgid
        ].copy()
        df_companylist_detail_selected = df_companylist_detail[
            df_companylist_detail["ORGID"] == selected_orgid
        ].copy()
        df_peoplelist_detail_selected = df_peoplelist_detail[
            df_peoplelist_detail["ORGID"] == selected_orgid
        ].copy()
    return (
        df_companylist_detail_selected,
        df_list_summary_selected,
        df_peoplelist_detail_selected,
    )


@app.cell
def _(
    df_companylist_detail_selected,
    df_list_summary_selected,
    df_peoplelist_detail_selected,
    list_org_selector,
    mo,
):
    _list_outputs = []
    
    if list_org_selector.value:
        _list_outputs.append(mo.md(f"### Liståˆ†æ: {list_org_selector.value}"))
        
        # ã‚¹ãƒ”ãƒŠãƒ¼ä»˜ãã§åˆ†æå®Ÿè¡Œ
        with mo.status.spinner(title="Liståˆ†æä¸­...") as _status:
            _status.update("CompanyListåˆ†æä¸­...")
            
            # CompanyListåˆ†æ
            if len(df_companylist_detail_selected) > 0:
                list_count = df_companylist_detail_selected["LIST_ID"].nunique()
                company_count = df_companylist_detail_selected["COMPANY_ID"].nunique()
                
                list_type_label = "(è¤‡æ•°ãƒªã‚¹ãƒˆé‹ç”¨)" if list_count > 1 else "(å˜ä¸€ãƒªã‚¹ãƒˆ)"
                _list_outputs.append(mo.md(f"""
**CompanyList ã‚µãƒãƒªãƒ¼**
- ãƒªã‚¹ãƒˆæ•°: **{list_count}ä»¶** {list_type_label}
- ç™»éŒ²ä¼æ¥­æ•°: **{company_count}ç¤¾**
"""))
                
                # æ¥­ç¨®åˆ†å¸ƒ
                if "INDUSTRY_ID" in df_companylist_detail_selected.columns:
                    industry_data = df_companylist_detail_selected["INDUSTRY_ID"].dropna()
                    if len(industry_data) > 0:
                        _list_outputs.append(mo.md("**æ¥­ç¨®åˆ†å¸ƒï¼ˆä¸Šä½5ä»¶ï¼‰**"))
                        industry_top = df_companylist_detail_selected.groupby("INDUSTRY_ID").size().nlargest(5).reset_index(name="ä»¶æ•°")
                        _list_outputs.append(mo.ui.table(industry_top, pagination=False))
                
                # åœ°åŸŸåˆ†å¸ƒ
                if "PREFID" in df_companylist_detail_selected.columns:
                    region_data = df_companylist_detail_selected["PREFID"].dropna()
                    if len(region_data) > 0:
                        _list_outputs.append(mo.md("**åœ°åŸŸåˆ†å¸ƒï¼ˆä¸Šä½5ä»¶ï¼‰**"))
                        region_top = df_companylist_detail_selected.groupby("PREFID").size().nlargest(5).reset_index(name="ä»¶æ•°")
                        _list_outputs.append(mo.ui.table(region_top, pagination=False))
                
                # å¾“æ¥­å“¡è¦æ¨¡åˆ†å¸ƒ
                if "EMPLOYEE_COUNT" in df_companylist_detail_selected.columns:
                    emp_data = df_companylist_detail_selected["EMPLOYEE_COUNT"].dropna()
                    if len(emp_data) > 0:
                        avg_emp = emp_data.mean()
                        max_emp = emp_data.max()
                        _list_outputs.append(mo.md(f"**ä¼æ¥­è¦æ¨¡**: å¹³å‡å¾“æ¥­å“¡æ•° {avg_emp:.0f}äºº, æœ€å¤§ {max_emp:.0f}äºº"))
            
            _status.update("PeopleListåˆ†æä¸­...")
            
            # PeopleListåˆ†æ
            if len(df_peoplelist_detail_selected) > 0:
                plist_count = df_peoplelist_detail_selected["LIST_ID"].nunique()
                keyman_count = df_peoplelist_detail_selected["KEYMAN_ID"].nunique()
                
                plist_type_label = "(è¤‡æ•°ãƒªã‚¹ãƒˆé‹ç”¨)" if plist_count > 1 else "(å˜ä¸€ãƒªã‚¹ãƒˆ)"
                _list_outputs.append(mo.md(f"""
**PeopleList ã‚µãƒãƒªãƒ¼**
- ãƒªã‚¹ãƒˆæ•°: **{plist_count}ä»¶** {plist_type_label}
- ç™»éŒ²äººç‰©æ•°: **{keyman_count}äºº**
"""))
                
        
        # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        if len(df_companylist_detail_selected) > 0:
            _list_outputs.append(mo.md(f"**CompanyListè©³ç´°** ({len(df_companylist_detail_selected)}ä»¶)"))
            _list_outputs.append(mo.ui.table(df_companylist_detail_selected.head(50), pagination=True))
        
        if len(df_peoplelist_detail_selected) > 0:
            _list_outputs.append(mo.md(f"**PeopleListè©³ç´°** ({len(df_peoplelist_detail_selected)}ä»¶)"))
            _list_outputs.append(mo.ui.table(df_peoplelist_detail_selected.head(50), pagination=True))
        
        if len(df_companylist_detail_selected) == 0 and len(df_peoplelist_detail_selected) == 0:
            _list_outputs.append(mo.md("*ã“ã®ä¼æ¥­ã«ã¯ãƒªã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“*"))
    else:
        _list_outputs.append(mo.md("*ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))
    
    mo.vstack(_list_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 7. ãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆãƒãƒ£ãƒ¼ãƒ³ + GA + ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆï¼‰
    """)
    return


@app.cell
def _(
    df_churn,
    df_churn_reason_latest,
    df_ga,
    df_id_mapping,
    df_list_summary,
    mo,
    np,
    pd,
    query_bq,
):
    df_merged = pd.DataFrame()

    if len(df_id_mapping) > 0 and len(df_ga) > 0:
        # 1. ãƒãƒ£ãƒ¼ãƒ³ï¼ˆTSVï¼‰+ BeegleCompany + USERORGANIZATIONï¼ˆCOMPNO ã§ç´ã¥ã‘ï¼‰
        bq_cols = [
            "COMPNO", "COMPANYID", "ORGID", "ORG_NAME", "BQ_COMPANY_NAME", "BQ_COMPANY_KANA",
            "CEO", "ESTABLISHMENT", "ADDRESS", "PHONE", "WEBSITE_URL", "MAIL",
            "INDUSTRY_ID", "EMPLOYEE_ID", "EMPLOYEE_COUNT", "REVENUE_ID", "CAPITAL_ID",
            "ISCLOSED", "BQ_CREATED_AT", "ORG_CREATED_AT"
        ]
        bq_cols_exist = [c for c in bq_cols if c in df_id_mapping.columns]
        df_merged = df_churn.merge(
            df_id_mapping[bq_cols_exist],
            on="COMPNO",
            how="left",
        )

        # 2. è§£ç´„ç†ç”±ï¼ˆCSVï¼‰ã‚’çµåˆ
        if len(df_churn_reason_latest) > 0 and "company_name_norm" in df_merged.columns:
            df_merged = df_merged.merge(
                df_churn_reason_latest,
                on="company_name_norm",
                how="left",
                suffixes=("", "_reason"),
            )

        # 3. GAæŒ‡æ¨™ã‚’çµåˆ
        df_merged = df_merged.merge(
            df_ga,
            left_on="ORGID",
            right_on="org_id",
            how="left",
        )

        # 4. Liståˆ†æï¼ˆSnowflakeï¼‰
        if len(df_list_summary) > 0:
            df_merged = df_merged.merge(
                df_list_summary,
                on="ORGID",
                how="left",
                suffixes=("", "_list"),
            )

        # 5. æ´¾ç”ŸæŒ‡æ¨™
        df_merged["sessions_per_user"] = df_merged["sessions"] / df_merged["users"].replace(0, np.nan)
        df_merged["page_views_per_user"] = df_merged["page_views"] / df_merged["users"].replace(0, np.nan)

        # 6. ãƒšãƒ¼ã‚¸åˆ©ç”¨ç‡
        total_pv = df_merged["page_views"].replace(0, np.nan)
        df_merged["list_rate"] = df_merged["pv_list"] / total_pv * 100
        df_merged["company_rate"] = df_merged["pv_company"] / total_pv * 100
        df_merged["download_rate"] = df_merged["pv_download"] / total_pv * 100
        df_merged["search_rate"] = df_merged["pv_search"] / total_pv * 100

        # 7. ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆã‚¹ã‚³ã‚¢å–å¾—ï¼ˆBigQuery first_party_scoreï¼‰
        # ORGIDã‚’corporate_idã¨ã—ã¦ä½¿ç”¨ï¼ˆè¦ç¢ºèªï¼‰
        orgids = df_merged["ORGID"].dropna().unique().tolist()
        if orgids:
            orgids_sql = ",".join([f"'{o}'" for o in orgids[:500]])  # ä¸Šé™500
            intent_query = f"""
            SELECT 
                CAST(first_party_corporate_id AS STRING) AS org_id,
                AVG(CASE WHEN intent_level = 3 THEN 1 ELSE 0 END) * 100 AS high_intent_rate,
                AVG(CASE WHEN intent_level = 2 THEN 1 ELSE 0 END) * 100 AS middle_intent_rate,
                COUNT(DISTINCT corporate_id) AS intent_company_count,
                MAX(change_date) AS latest_intent_change_date,
                MAX(CASE WHEN intent_level = 3 THEN 1 ELSE 0 END) AS intent_activation_flag,
                MAX(
                    CASE
                        WHEN change_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
                        THEN 1
                        ELSE 0
                    END
                ) AS intent_recent_change_flag
            FROM `gree-dionysus-infobox.production_infobox.first_party_score_company_latest`
            WHERE CAST(first_party_corporate_id AS STRING) IN ({orgids_sql})
            GROUP BY 1
            """
            try:
                df_intent = query_bq(intent_query)
                if len(df_intent) > 0:
                    df_merged = df_merged.merge(
                        df_intent,
                        left_on="ORGID",
                        right_on="org_id",
                        how="left",
                        suffixes=("", "_intent"),
                    )
            except Exception:
                pass

            sfa_intent_query = f"""
            SELECT 
                CAST(s.first_party_corporate_id AS STRING) AS org_id,
                MAX(
                    CASE
                        WHEN REGEXP_CONTAINS(c.original_category_name, r'(SFA|å–¶æ¥­æ”¯æ´|CRM|ã‚»ãƒ¼ãƒ«ã‚¹)') THEN 1
                        ELSE 0
                    END
                ) AS sfa_intent_flag,
                COUNT(DISTINCT CASE
                    WHEN REGEXP_CONTAINS(c.original_category_name, r'(SFA|å–¶æ¥­æ”¯æ´|CRM|ã‚»ãƒ¼ãƒ«ã‚¹)')
                    THEN s.corporate_id
                    ELSE NULL
                END) AS sfa_intent_company_count
            FROM `gree-dionysus-infobox.production_infobox.company_category_daily_v3` c
            JOIN `gree-dionysus-infobox.production_infobox.first_party_score_company_latest` s
              ON CAST(c.corporate_id AS INT64) = s.corporate_id
            WHERE c.view_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
              AND s.change_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 30 DAY)
              AND s.intent_level IN (2, 3)
              AND CAST(s.first_party_corporate_id AS STRING) IN ({orgids_sql})
            GROUP BY 1
            """
            try:
                df_sfa_intent = query_bq(sfa_intent_query)
                if len(df_sfa_intent) > 0:
                    df_merged = df_merged.merge(
                        df_sfa_intent,
                        left_on="ORGID",
                        right_on="org_id",
                        how="left",
                        suffixes=("", "_sfa"),
                    )
            except Exception:
                pass

        if "sfa_intent_flag" in df_merged.columns:
            df_merged["sfa_intent_flag"] = (
                df_merged["sfa_intent_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["sfa_intent_flag"] = 0

        if "sfa_intent_company_count" in df_merged.columns:
            df_merged["sfa_intent_company_count"] = (
                df_merged["sfa_intent_company_count"].fillna(0).astype(int)
            )
        else:
            df_merged["sfa_intent_company_count"] = 0

        if "intent_activation_flag" in df_merged.columns:
            df_merged["intent_activation_flag"] = (
                df_merged["intent_activation_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["intent_activation_flag"] = 0

        if "intent_recent_change_flag" in df_merged.columns:
            df_merged["intent_recent_change_flag"] = (
                df_merged["intent_recent_change_flag"].fillna(0).astype(int)
            )
        else:
            df_merged["intent_recent_change_flag"] = 0

        high_series = (
            df_merged["high_intent_rate"]
            if "high_intent_rate" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        middle_series = (
            df_merged["middle_intent_rate"]
            if "middle_intent_rate" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        sfa_series = (
            df_merged["sfa_intent_flag"]
            if "sfa_intent_flag" in df_merged.columns
            else pd.Series(0, index=df_merged.index)
        )
        df_merged["intent_interest_flag"] = (
            (high_series.fillna(0) > 0)
            | (middle_series.fillna(0) > 0)
            | (sfa_series.fillna(0) > 0)
        ).astype(int)

        df_merged["intent_recent_change_activation_flag"] = (
            (df_merged["intent_recent_change_flag"] > 0)
            & (df_merged["intent_activation_flag"] > 0)
        ).astype(int)

        mo.md(
            f"""
            **çµ±åˆçµæœ**:
            - çµ±åˆå¾Œè¡Œæ•°: {len(df_merged):,}
            - ORGIDçµåˆæˆåŠŸ: {df_merged['ORGID'].notna().sum():,}
            - GAçµåˆæˆåŠŸ: {df_merged['sessions'].notna().sum():,}
            """
        )
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿çµ±åˆã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒæƒã£ã¦ã„ã¾ã›ã‚“*")
    return (df_merged,)


@app.cell
def _(df_merged, mo):
    # ä¼šç¤¾åã‚«ãƒ©ãƒ ã‚’æ¢ã™
    company_col = next((c for c in ["BQ_COMPANY_NAME", "CompanyName", "COMPANY_NAME"] if c in df_merged.columns), None) if len(df_merged) > 0 else None
    display_cols = [
        company_col, "INDUSTRY_ID", "EMPLOYEE_COUNT", "ADDRESS",
        "status", "is_churned", "ORGID",
        "sessions", "page_views", "users",
        "list_rate", "download_rate", "search_rate",
        "sfa_intent_flag", "sfa_intent_company_count", "intent_interest_flag",
        "intent_activation_flag",
        "intent_recent_change_flag",
        "intent_recent_change_activation_flag",
        "latest_intent_change_date",
        "companylist_count", "peoplelist_count",
        "companylist_company_count", "peoplelist_keyman_count",
        "companylist_top_companies", "peoplelist_top_keymen",
        "loss_type", "loss_reason", "loss_detail",
    ]
    cols_exist = [c for c in display_cols if c and c in df_merged.columns] if len(df_merged) > 0 else []
    df_display = df_merged[cols_exist].head(30) if cols_exist else df_merged.head(30)
    mo.ui.table(df_display, pagination=True)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 8. ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆãƒãƒ£ãƒ¼ãƒ³å¯„ä¸åº¦ï¼‰
    """)
    return


@app.cell
def _(df_merged, mo, pd):
    df_importance = pd.DataFrame()

    if len(df_merged) > 0:
        try:
            from sklearn.linear_model import LogisticRegression
            from sklearn.preprocessing import StandardScaler

            # ç‰¹å¾´é‡é¸æŠ
            feature_cols = [
                "sessions", "page_views", "users",
                "sessions_per_user", "page_views_per_user",
                "pv_list", "pv_company", "pv_download", "pv_search",
                "list_rate", "download_rate", "search_rate",
            ]
            # ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆæŒ‡æ¨™ãŒã‚ã‚Œã°è¿½åŠ 
            if "high_intent_rate" in df_merged.columns:
                feature_cols.extend(["high_intent_rate", "middle_intent_rate", "intent_company_count"])

            feature_cols = [c for c in feature_cols if c in df_merged.columns]

            # æ¬ æå€¤ã‚’é™¤å¤–
            df_ml = df_merged[["is_churned"] + feature_cols].dropna()

            if len(df_ml) > 10:
                X = df_ml[feature_cols]
                y = df_ml["is_churned"]

                # æ¨™æº–åŒ–
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                # ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
                model_logit = LogisticRegression(max_iter=1000, random_state=42)
                model_logit.fit(X_scaled, y)

                importance_data = []
                for feat_col, coef in zip(feature_cols, model_logit.coef_[0]):
                    importance_data.append({
                        "ç‰¹å¾´é‡": feat_col,
                        "ä¿‚æ•°": round(coef, 4),
                        "åŠ¹æœæ–¹å‘": "è§£ç´„ä¿ƒé€²" if coef > 0 else "ç¶™ç¶šä¿ƒé€²",
                        "é‡è¦åº¦": round(abs(coef), 4),
                    })
                df_importance = pd.DataFrame(importance_data).sort_values("é‡è¦åº¦", ascending=False)

                mo.md(f"**å­¦ç¿’å®Œäº†**: ã‚µãƒ³ãƒ—ãƒ«æ•° {len(df_ml)}, ç‰¹å¾´é‡æ•° {len(feature_cols)}")
            else:
                mo.md("*ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™ï¼ˆ10ä»¶ä»¥ä¸Šå¿…è¦ï¼‰*")

        except ImportError as e:
            mo.md(f"**scikit-learnæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: `{e}`")
        except Exception as e:
            mo.md(f"**MLå®Ÿè¡Œã‚¨ãƒ©ãƒ¼**: `{e}`")
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿çµ±åˆãŒå®Œäº†ã™ã‚‹ã¾ã§ãŠå¾…ã¡ãã ã•ã„*")
    return (df_importance,)


@app.cell
def _(df_importance, mo):
    mo.md("### ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° å¯„ä¸åº¦")
    mo.ui.table(df_importance, pagination=False)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 9. LLMå±é™ºåº¦åˆ¤å®šï¼ˆGeminiï¼‰
    """)
    return


@app.cell
def _(df_merged, mo):
    llm_company_options = {}
    if len(df_merged) > 0:
        llm_name_cols = [c for c in ["BQ_COMPANY_NAME", "ORG_NAME", "CompanyName"] if c in df_merged.columns]
        df_llm_company_base = df_merged.dropna(subset=["ORGID"]).copy()
        if llm_name_cols:
            df_llm_company_base["company_label"] = df_llm_company_base[llm_name_cols[0]].astype(str)
        else:
            df_llm_company_base["company_label"] = df_llm_company_base["ORGID"].astype(str)

        for _, llm_company_row in df_llm_company_base.drop_duplicates(subset=["ORGID"]).iterrows():
            llm_label = f"{llm_company_row['company_label']} ({llm_company_row['ORGID']})"
            llm_company_options[llm_label] = llm_company_row["ORGID"]

    llm_company_default_value = next(iter(llm_company_options.keys()), None) if llm_company_options else None
    company_selector = mo.ui.dropdown(
        options=llm_company_options,
        label="ä¼šç¤¾ã‚’é¸æŠ",
        value=llm_company_default_value,
    )
    company_selector
    return company_selector, llm_company_options


@app.cell
def _(company_selector, df_merged, pd):
    df_company_detail = pd.DataFrame()

    # company_selector.value ã¯æ—¢ã«ORGIDï¼ˆdropdown ã® dict ã§ã¯ .value ãŒå€¤ã‚’ç›´æ¥è¿”ã™ï¼‰
    selected_company_orgid = company_selector.value

    if len(df_merged) > 0 and selected_company_orgid:
        df_company_detail = df_merged[
            df_merged["ORGID"] == selected_company_orgid
        ].copy()
    return (df_company_detail,)


@app.cell
def _(company_selector, df_company_detail, mo):
    _company_outputs = []
    
    if company_selector.value and len(df_company_detail) > 0:
        _company_outputs.append(mo.md(f"**é¸æŠä¼æ¥­**: {company_selector.value} ({len(df_company_detail)} ä»¶)"))
        _company_outputs.append(mo.ui.table(df_company_detail.head(10), pagination=True))
    else:
        _company_outputs.append(mo.md("*ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))
    
    return mo.vstack(_company_outputs)


@app.cell
def _(mo):
    run_llm_button = mo.ui.run_button(label="LLMå±é™ºåº¦åˆ¤å®šã‚’å®Ÿè¡Œ")
    run_llm_button
    return (run_llm_button,)


@app.cell
def _(
    GA_DATASET_ID,
    SF_SCHEMA,
    company_selector,
    df_company_detail,
    extract_json_block,
    genai,
    genai_error,
    genai_types,
    json,
    mo,
    os,
    query_bq,
    query_sf,
    run_llm_button,
):
    _llm_outputs = []
    
    # ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§é¸æŠã•ã‚ŒãŸä¼æ¥­ã‚’ä½¿ç”¨
    if len(df_company_detail) > 0:
        sample_company = df_company_detail.iloc[0:1].copy()
        row = sample_company.iloc[0]
        
        # è¤‡æ•°ã‚«ãƒ©ãƒ ã‹ã‚‰NaNã§ãªã„æœ€åˆã®å€¤ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ï¼‰
        import pandas as _pd_check
        company_name = (
            row.get("CompanyName") 
            if _pd_check.notna(row.get("CompanyName")) 
            else row.get("BQ_COMPANY_NAME") 
            if _pd_check.notna(row.get("BQ_COMPANY_NAME"))
            else row.get("ORG_NAME")
            if _pd_check.notna(row.get("ORG_NAME"))
            else "N/A"
        )
        orgid = row.get("ORGID") if _pd_check.notna(row.get("ORGID")) else "N/A"
        
        _llm_outputs.append(mo.md(f"### é¸æŠä¼æ¥­: {company_name}"))
        _llm_outputs.append(mo.md(f"ORGID: `{orgid}`"))
        
        # ãƒ‡ãƒãƒƒã‚°: ãƒœã‚¿ãƒ³ã¨genaiã®çŠ¶æ…‹
        _llm_outputs.append(mo.md(f"*Debug: button={run_llm_button.value}, genai={genai is not None}, genai_error={genai_error}*"))
        
        if run_llm_button.value and genai is not None:
            # ã‚¹ãƒ”ãƒŠãƒ¼ä»˜ãã§LLMå‡¦ç†å®Ÿè¡Œ
            with mo.status.spinner(title="åˆ†æãƒ‡ãƒ¼ã‚¿å–å¾— + LLMå‡¦ç†ä¸­...") as _status:
                try:
                    # === 1. GA 6ãƒ¶æœˆæ¨ç§»ã‚’å–å¾— ===
                    _status.update("GAåˆ©ç”¨æ¨ç§»ã‚’å–å¾—ä¸­ï¼ˆ6ãƒ¶æœˆï¼‰...")
                    ga_trend_query = f"""
                    WITH base AS (
                        SELECT
                            FORMAT_DATE('%Y-%m', PARSE_DATE('%Y%m%d', event_date)) AS month,
                            (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') AS org_id,
                            user_pseudo_id,
                            (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS session_id,
                            event_name,
                            (SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'page_location') AS page_location
                        FROM `{GA_DATASET_ID}.events_*`
                        WHERE _TABLE_SUFFIX >= FORMAT_DATE('%Y%m%d', DATE_SUB(CURRENT_DATE(), INTERVAL 6 MONTH))
                          AND (SELECT value.string_value FROM UNNEST(user_properties) WHERE key = 'org_id') = '{orgid}'
                    )
                    SELECT
                        month,
                        COUNT(DISTINCT user_pseudo_id) AS users,
                        COUNT(DISTINCT CONCAT(user_pseudo_id, '-', CAST(session_id AS STRING))) AS sessions,
                        COUNTIF(event_name = 'page_view') AS page_views,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/list') AND event_name = 'page_view') AS pv_list,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/company') AND event_name = 'page_view') AS pv_company,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/download|csv') AND event_name = 'page_view') AS pv_download,
                        COUNTIF(REGEXP_CONTAINS(page_location, r'/search') AND event_name = 'page_view') AS pv_search
                    FROM base
                    GROUP BY month
                    ORDER BY month
                    """
                    try:
                        df_ga_trend = query_bq(ga_trend_query)
                        ga_trend_json = df_ga_trend.to_json(orient="records", force_ascii=False) if len(df_ga_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… GAæ¨ç§»å–å¾—: {len(df_ga_trend)}è¡Œ"))
                    except Exception as ga_err:
                        df_ga_trend = None
                        ga_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ GAæ¨ç§»å–å¾—ã‚¨ãƒ©ãƒ¼: `{ga_err}`"))
                    
                    # === 2. Snowflake ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ã‚’å–å¾— ===
                    _status.update("Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ã‚’å–å¾—ä¸­...")
                    
                    # CompanyListä½œæˆæ¨ç§»
                    list_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', cl.CREATEDAT) AS MONTH,
                        COUNT(DISTINCT cl.ID) AS COMPANYLIST_CREATED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.COMPANYLIST cl ON cl.USERORGRELATIONID = ur.ID
                    WHERE u.ORGID = '{orgid}'
                      AND cl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_list_trend = query_sf(list_trend_query)
                        list_trend_json = df_list_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_list_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… CompanyListæ¨ç§»: {len(df_list_trend)}è¡Œ"))
                    except Exception as cl_err:
                        df_list_trend = None
                        list_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ CompanyListæ¨ç§»ã‚¨ãƒ©ãƒ¼: `{cl_err}`"))
                    
                    # PeopleListä½œæˆæ¨ç§»ï¼ˆã‚­ãƒ¼ãƒãƒ³ç™»éŒ²æ•°ï¼‰
                    peoplelist_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', pl.CREATEDAT) AS MONTH,
                        COUNT(DISTINCT pl.ID) AS PEOPLELIST_CREATED,
                        COUNT(DISTINCT k.ID) AS KEYMAN_REGISTERED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.PEOPLELIST pl ON pl.USERORGRELATIONID = ur.ID
                    LEFT JOIN {SF_SCHEMA}._KEYMANTOPEOPLELIST rel ON rel.B = pl.ID
                    LEFT JOIN {SF_SCHEMA}.KEYMAN k ON rel.A = k.ID
                    WHERE u.ORGID = '{orgid}'
                      AND pl.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_peoplelist_trend = query_sf(peoplelist_trend_query)
                        peoplelist_trend_json = df_peoplelist_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_peoplelist_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… PeopleListæ¨ç§»: {len(df_peoplelist_trend)}è¡Œ"))
                    except Exception as pl_err:
                        df_peoplelist_trend = None
                        peoplelist_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ PeopleListæ¨ç§»ã‚¨ãƒ©ãƒ¼: `{pl_err}`"))
                    
                    # Memoä½œæˆæ¨ç§»
                    memo_trend_query = f"""
                    SELECT
                        DATE_TRUNC('month', m.CREATEDAT) AS MONTH,
                        COUNT(*) AS MEMO_CREATED
                    FROM {SF_SCHEMA}.USERORGANIZATION u
                    JOIN {SF_SCHEMA}.USERORGRELATION ur ON u.ORGID = ur.ORGANIZATIONID
                    JOIN {SF_SCHEMA}.MEMO m ON m.USERORGRELATIONID = ur.ID
                    WHERE u.ORGID = '{orgid}'
                      AND m.CREATEDAT >= DATEADD('month', -6, CURRENT_DATE())
                    GROUP BY MONTH
                    ORDER BY MONTH
                    """
                    try:
                        df_memo_trend = query_sf(memo_trend_query)
                        memo_trend_json = df_memo_trend.to_json(orient="records", force_ascii=False, date_format="iso") if len(df_memo_trend) > 0 else "[]"
                        _llm_outputs.append(mo.md(f"âœ… ãƒ¡ãƒ¢æ¨ç§»: {len(df_memo_trend)}è¡Œ"))
                    except Exception as memo_err:
                        df_memo_trend = None
                        memo_trend_json = "[]"
                        _llm_outputs.append(mo.md(f"âš ï¸ ãƒ¡ãƒ¢æ¨ç§»ã‚¨ãƒ©ãƒ¼: `{memo_err}`"))
                    
                    # === 3. æ¨ç§»ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º ===
                    _llm_outputs.append(mo.md("### GAåˆ©ç”¨æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰"))
                    if df_ga_trend is not None and len(df_ga_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_ga_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*GAãƒ‡ãƒ¼ã‚¿ãªã—*"))
                    
                    _llm_outputs.append(mo.md("### Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»"))
                    _llm_outputs.append(mo.md("**CompanyListä½œæˆæ¨ç§»**"))
                    if df_list_trend is not None and len(df_list_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_list_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*CompanyListä½œæˆãªã—*"))
                    
                    _llm_outputs.append(mo.md("**PeopleListä½œæˆæ¨ç§»ï¼ˆã‚­ãƒ¼ãƒãƒ³ç™»éŒ²å«ã‚€ï¼‰**"))
                    if df_peoplelist_trend is not None and len(df_peoplelist_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_peoplelist_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*PeopleListä½œæˆãªã—*"))
                    
                    _llm_outputs.append(mo.md("**ãƒ¡ãƒ¢ä½œæˆæ¨ç§»**"))
                    if df_memo_trend is not None and len(df_memo_trend) > 0:
                        _llm_outputs.append(mo.ui.table(df_memo_trend, pagination=False))
                    else:
                        _llm_outputs.append(mo.md("*ãƒ¡ãƒ¢ä½œæˆãªã—*"))
                    
                    # === 4. LLMå‡¦ç† ===
                    _status.update("ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿ä¸­...")
                    
                    # .envã‹ã‚‰ç›´æ¥èª­ã¿è¾¼ã¿
                    import dotenv as _dotenv_mod
                    import pathlib as _pathlib_mod
                    _llm_env_path = _pathlib_mod.Path("/Users/kou1904/githubactions_fordata/work/aieda_agent/.env")
                    if _llm_env_path.exists():
                        _dotenv_mod.load_dotenv(_llm_env_path, override=True)
                    
                    # Gemini 3 Pro Preview
                    api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
                    if not api_key:
                        raise ValueError("GEMINI_API_KEY ã¾ãŸã¯ GOOGLE_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                    
                    _status.update("Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ä¸­...")
                    client = genai.Client(api_key=api_key)

                    # ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
                    company_data = sample_company.iloc[0].to_dict()
                    company_json = json.dumps({k: str(v) for k, v in company_data.items() if v is not None}, ensure_ascii=False, indent=2)

                    prompt = f"""
    ã‚ãªãŸã¯å–¶æ¥­æ”¯æ´SaaSã€ŒInfoBoxã€ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ã‚µã‚¯ã‚»ã‚¹æ‹…å½“ã§ã™ã€‚
    ä»¥ä¸‹ã®ä¼šç¤¾ãƒ‡ãƒ¼ã‚¿ã¨åˆ©ç”¨æ¨ç§»ã‚’åˆ†æã—ã€ãƒãƒ£ãƒ¼ãƒ³ï¼ˆè§£ç´„ï¼‰ãƒªã‚¹ã‚¯ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

    ## ä¼šç¤¾åŸºæœ¬ãƒ‡ãƒ¼ã‚¿
    ```json
    {company_json}
    ```

    ## GAåˆ©ç”¨æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰
    - users: ãƒ­ã‚°ã‚¤ãƒ³ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°
    - sessions: ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°
    - page_views: PVæ•°
    - pv_list: ãƒªã‚¹ãƒˆãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹
    - pv_company: ä¼æ¥­è©³ç´°ã‚¢ã‚¯ã‚»ã‚¹
    - pv_download: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¢ã‚¯ã‚»ã‚¹
    - pv_search: æ¤œç´¢ã‚¢ã‚¯ã‚»ã‚¹
    ```json
    {ga_trend_json}
    ```

    ## Snowflakeã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£æ¨ç§»ï¼ˆ6ãƒ¶æœˆï¼‰
    ### CompanyListä½œæˆæ•°ï¼ˆä¼æ¥­ãƒªã‚¹ãƒˆï¼‰
    ```json
    {list_trend_json}
    ```
    ### PeopleListä½œæˆæ•°ï¼ˆã‚­ãƒ¼ãƒãƒ³ãƒªã‚¹ãƒˆï¼‰
    - PEOPLELIST_CREATED: ä½œæˆã•ã‚ŒãŸPeopleListæ•°
    - KEYMAN_REGISTERED: ç™»éŒ²ã•ã‚ŒãŸã‚­ãƒ¼ãƒãƒ³æ•°
    ```json
    {peoplelist_trend_json}
    ```
    ### ãƒ¡ãƒ¢ä½œæˆæ•°
    ```json
    {memo_trend_json}
    ```

    ## åˆ¤å®šåŸºæº–
    - **Highï¼ˆ70-100ç‚¹ï¼‰**: è§£ç´„ãƒªã‚¹ã‚¯ãŒé«˜ã„
      - åˆ©ç”¨ç‡ãŒæ¸›å°‘å‚¾å‘
      - CompanyList/PeopleList/ãƒ¡ãƒ¢ä½œæˆãŒåœæ»
      - ç«¶åˆæ¤œè¨ã®å…†å€™
    - **Mediumï¼ˆ40-69ç‚¹ï¼‰**: æ³¨æ„ãŒå¿…è¦
      - åˆ©ç”¨é »åº¦ã«æ³¢ãŒã‚ã‚‹
      - ä¸€éƒ¨æ©Ÿèƒ½ã®ã¿æ´»ç”¨ï¼ˆä¾‹: CompanyListã¯ä½¿ã†ãŒPeopleListã¯æœªä½¿ç”¨ï¼‰
    - **Lowï¼ˆ0-39ç‚¹ï¼‰**: å®‰å®š
      - ç¶™ç¶šçš„ãªåˆ©ç”¨
      - è¤‡æ•°æ©Ÿèƒ½ã‚’æ´»ç”¨ï¼ˆCompanyList + PeopleList + ãƒ¡ãƒ¢ï¼‰
      - ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒç¶­æŒ/å¢—åŠ 

    ## å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
    ```json
    {{
      "risk_level": "High" | "Medium" | "Low",
      "risk_score": 0-100,
      "reason": "åˆ¤å®šç†ç”±ï¼ˆæ—¥æœ¬èªã€3æ–‡ä»¥å†…ï¼‰",
      "key_signals": ["ã‚·ã‚°ãƒŠãƒ«1", "ã‚·ã‚°ãƒŠãƒ«2"],
      "recommended_actions": ["ã‚¢ã‚¯ã‚·ãƒ§ãƒ³1", "ã‚¢ã‚¯ã‚·ãƒ§ãƒ³2"],
      "trend_analysis": "æ¨ç§»ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®åˆ†æï¼ˆæ—¥æœ¬èªã€2æ–‡ä»¥å†…ï¼‰"
    }}
    ```
    - JSONã®ã¿è¿”ã™ï¼ˆä½™è¨ˆãªèª¬æ˜ã‚„ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ä¸è¦ï¼‰
    """

                    _status.update("Gemini APIå‘¼ã³å‡ºã—ä¸­ï¼ˆ10-30ç§’ã‹ã‹ã‚Šã¾ã™ï¼‰...")
                    
                    response = client.models.generate_content(
                        model="gemini-3-pro-preview",
                        contents=prompt,
                        config=genai_types.GenerateContentConfig(
                            temperature=0.1,
                            max_output_tokens=2000,
                        ),
                    )

                    _status.update("å¿œç­”è§£æä¸­...")
                    llm_result = response.text
                    llm_json = extract_json_block(llm_result) or llm_result
                    
                except Exception as e:
                    _llm_outputs.append(mo.md(f"âŒ **LLMå®Ÿè¡Œã‚¨ãƒ©ãƒ¼**: `{e}`"))
                    llm_json = None
            
            # å‡¦ç†å®Œäº†å¾Œã®çµæœè¡¨ç¤º
            if llm_json:
                _llm_outputs.append(mo.md("âœ… **LLMå‡¦ç†å®Œäº†**"))
                _llm_outputs.append(mo.md("### LLMåˆ¤å®šçµæœ"))
                _llm_outputs.append(mo.md(f"```json\n{llm_json}\n```"))

                # JSONã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’å¯è¦–åŒ–
                try:
                    llm_parsed = json.loads(llm_json)
                    risk_level = llm_parsed.get("risk_level", "Unknown")
                    risk_score = llm_parsed.get("risk_score", 0)
                    reason = llm_parsed.get("reason", "")
                    key_signals = llm_parsed.get("key_signals", [])
                    actions = llm_parsed.get("recommended_actions", [])
                    trend_analysis = llm_parsed.get("trend_analysis", "")
                    
                    color = {"High": "red", "Medium": "orange", "Low": "green"}.get(risk_level, "gray")
                    
                    _llm_outputs.append(mo.md(f"""
### ãƒªã‚¹ã‚¯åˆ¤å®šã‚µãƒãƒªãƒ¼

| é …ç›® | å€¤ |
|------|-----|
| **ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«** | <span style="color:{color};font-weight:bold">{risk_level}</span> |
| **ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢** | {risk_score}/100 |
| **åˆ¤å®šç†ç”±** | {reason} |
| **æ¨ç§»åˆ†æ** | {trend_analysis} |
| **ä¸»è¦ã‚·ã‚°ãƒŠãƒ«** | {', '.join(key_signals) if key_signals else '-'} |
| **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³** | {', '.join(actions) if actions else '-'} |
"""))
                except Exception:
                    pass
                    
        elif genai_error:
            _llm_outputs.append(mo.md(f"**Geminiæœªè¨­å®š**: `{genai_error}`"))
        else:
            _llm_outputs.append(mo.md("*ã€ŒLLMå±é™ºåº¦åˆ¤å®šã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„*"))
    else:
        _llm_outputs.append(mo.md("*ä¸Šã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ä¼æ¥­ã‚’é¸æŠã—ã¦ãã ã•ã„*"))
    
    mo.vstack(_llm_outputs)
    return


@app.cell
def _(mo):
    mo.md("""
    ## 10. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å…¨ä½“ã‚µãƒãƒªãƒ¼
    """)
    return


@app.cell
def _(df_competitor_intent, df_importance, df_merged, mo):
    summary_parts = []

    if len(df_merged) > 0:
        churned = df_merged["is_churned"].sum()
        active = len(df_merged) - churned
        summary_parts.append(f"- **ä¼æ¥­æ•°**: è§£ç´„ {churned}, å¥‘ç´„ä¸­ {active}")

        if "sessions" in df_merged.columns:
            avg_sessions = df_merged[df_merged["is_churned"] == 0]["sessions"].mean()
            avg_sessions_churned = df_merged[df_merged["is_churned"] == 1]["sessions"].mean()
            summary_parts.append(f"- **å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³**: å¥‘ç´„ä¸­ {avg_sessions:.1f}, è§£ç´„ {avg_sessions_churned:.1f}")

    if len(df_importance) > 0:
        top_factors = df_importance.head(3)["ç‰¹å¾´é‡"].tolist()
        summary_parts.append(f"- **ãƒãƒ£ãƒ¼ãƒ³å¯„ä¸åº¦TOP3**: {', '.join(top_factors)}")

    if len(df_competitor_intent) > 0:
        high_intent = df_competitor_intent[df_competitor_intent["level_name"] == "High"]["company_count"].sum()
        summary_parts.append(f"- **ç«¶åˆã‚«ãƒ†ã‚´ãƒª High ã‚¤ãƒ³ãƒ†ãƒ³ãƒˆ**: {high_intent:,} ç¤¾")

    if summary_parts:
        mo.md("### ã‚µãƒãƒªãƒ¼\n" + "\n".join(summary_parts))
    else:
        mo.md("*ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„*")
    summary_parts
    return


@app.cell
def _():
    return


if __name__ == "__main__":
    app.run()

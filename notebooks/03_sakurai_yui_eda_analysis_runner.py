"""æ«»äº•å„ªè¡£ EDA åˆ†æå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å…¨ã¦ã®EDAåˆ†æã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¡¨ç¤ºãƒ»ä¿å­˜ã—ã¾ã™ã€‚
"""

import sys
import os
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime
import re
from collections import Counter
import json

# ç’°å¢ƒè¨­å®š
if "GOOGLE_APPLICATION_CREDENTIALS" in os.environ:
    gac_path = os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
    if not os.path.exists(gac_path):
        del os.environ["GOOGLE_APPLICATION_CREDENTIALS"]

root_dir = Path(__file__).parent.parent
if str(root_dir / "src") not in sys.path:
    sys.path.insert(0, str(root_dir / "src"))

from ai_data_lab.connectors.bigquery import BigQueryConnector

# BigQuery è¨­å®š
PROJECT_ID = "yoake-dev-analysis"
DATASET_ID = "dev_yoake_posts"
TABLE_ID = "æ«»äº•å„ªè¡£"

print("=" * 80)
print("ğŸ€ æ«»äº•å„ªè¡£ æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ EDA åˆ†æå®Ÿè¡Œ")
print("=" * 80)
print()

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("ğŸ“¥ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
connector = BigQueryConnector(project_id=PROJECT_ID)

base_query = f"""
WITH deduplicated AS (
  SELECT
    keyword,
    talentId,
    post.xPostId as post_id,
    TIMESTAMP_SECONDS(post.xPostCreatedAt) as created_at,
    post.xPostUrl as post_url,
    REGEXP_EXTRACT(
      post.xPostUrl,
      r'^https://x\\.com/([^/]+)/status'
    ) as post_user_handle,
    post.xPostContent as content,
    post.xPostQuotedCount as quoted_count,
    post.xPostRepostedCount as repost_count,
    post.xPostRepliedCount as reply_count,
    post.xPostLikedCount as like_count,
    ARRAY_LENGTH(post.xPostMediaList) as media_count,
    user.xPostUserId as user_id,
    user.xPostUserName as user_name,
    user.xPostUserBadge as user_badge,
    user.xProfileImageUrl as user_profile_image,
    ROW_NUMBER() OVER (PARTITION BY post.xPostId ORDER BY _PARTITIONTIME DESC) as row_num
  FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
  WHERE _PARTITIONTIME IS NOT NULL
)
SELECT
    keyword,
    talentId,
    post_id,
    created_at,
    post_url,
    post_user_handle,
    content,
    quoted_count,
    repost_count,
    reply_count,
    like_count,
    media_count,
    user_id,
    user_name,
    user_badge,
    user_profile_image
FROM deduplicated
WHERE
  row_num = 1
  AND post_user_handle NOT IN (
    'FRUITS_ZIPPER',
    'amane_fz1026',
    'suzuka_fz1124',
    'yui_fz0221',
    'luna_fz0703',
    'manafy_fz0422',
    'karen_fz0328',
    'noel_fz1229',
    'CUTIE_STREET_',
    'aika_cs1126',
    'risa_cs1108',
    'ayano_cs0526',
    'emiru_cs0422',
    'kana_cs1111',
    'haruka_cs0129',
    'miyu_cs0913',
    'nagisa_cs0628',
    'candy_tune_',
    'mizuki_ct0221',
    'rino_ct1224',
    'nachico_ct1001',
    'natsu_ct0317',
    'kotomi_ct0525',
    'shizuka_ct0530',
    'bibian_ct1203',
    'SWEET_STEADY',
    'rise_ss0731',
    'ayu_ss0107',
    'sakina_ss0229',
    'nagisa_ss1029',
    'natsuka_ss0719',
    'mayumi_ss1227',
    'yui_ss0109',
    'nogizaka46',
    'takanenofficial',
    'nao_kizuki',
    'hina_hinahata',
    'Mikuru_hositani',
    'erisahigasiyama',
    'momonamatsumoto',
    'MomokoHashimoto',
    'su_suzumi_',
    'himeri_momiyama',
    'saara_hazuki',
    'Equal_LOVE_12',
    'otani_emiri',
    'hana_oba',
    'otoshima_risa',
    'saitou_kiara',
    'sasaki_maika',
    'takamatsuhitomi',
    'shoko_takiwaki',
    'noguchi_iori',
    'morohashi_sana',
    'yamamoto_anna_'
  )
"""

df = connector.query(base_query)
print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ Ã— {len(df.columns)} åˆ—\n")

# ãƒ‡ãƒ¼ã‚¿åŠ å·¥
print("ğŸ”§ ãƒ‡ãƒ¼ã‚¿åŠ å·¥ä¸­...")
df['date'] = pd.to_datetime(df['created_at']).dt.date
df['hour'] = pd.to_datetime(df['created_at']).dt.hour
df['day_of_week'] = pd.to_datetime(df['created_at']).dt.dayofweek
df['weekday_name'] = pd.to_datetime(df['created_at']).dt.day_name()
df['has_media'] = df['media_count'] > 0
df['total_engagement'] = df['like_count'] + df['repost_count'] + df['reply_count'] + df['quoted_count']
df['content_length'] = df['content'].fillna('').str.len()
df['has_url'] = df['content'].fillna('').str.contains('http')
print("âœ… ãƒ‡ãƒ¼ã‚¿åŠ å·¥å®Œäº†\n")

# åˆ†æçµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
results = {}

print("=" * 80)
print("ğŸ“Š 1. åŸºæœ¬çµ±è¨ˆ")
print("=" * 80)
results['basic_stats'] = {
    'ç·æŠ•ç¨¿æ•°': len(df),
    'ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°': int(df['user_id'].nunique()),
    'ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ•ç¨¿IDæ•°': int(df['post_id'].nunique()),
    'æœŸé–“_é–‹å§‹': str(df['created_at'].min()),
    'æœŸé–“_çµ‚äº†': str(df['created_at'].max()),
    'æ—¥æ•°': int((df['created_at'].max() - df['created_at'].min()).days),
}
for key, value in results['basic_stats'].items():
    print(f"  {key}: {value}")
print()

print("=" * 80)
print("ğŸ“… 2. æŠ•ç¨¿ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
print("=" * 80)

# æ—¥åˆ¥æŠ•ç¨¿æ•°
daily_posts = df.groupby('date').size().reset_index(name='post_count')
results['daily_posts'] = {
    'å¹³å‡æŠ•ç¨¿æ•°_æ—¥': float(daily_posts['post_count'].mean()),
    'ä¸­å¤®å€¤æŠ•ç¨¿æ•°_æ—¥': float(daily_posts['post_count'].median()),
    'æœ€å¤§æŠ•ç¨¿æ•°_æ—¥': int(daily_posts['post_count'].max()),
    'æœ€å°æŠ•ç¨¿æ•°_æ—¥': int(daily_posts['post_count'].min()),
}
print("\nğŸ“ˆ æ—¥åˆ¥æŠ•ç¨¿æ•°:")
for key, value in results['daily_posts'].items():
    print(f"  {key}: {value}")

# æ›œæ—¥åˆ¥æŠ•ç¨¿æ•°
weekday_posts = df.groupby('weekday_name').size().reset_index(name='post_count')
weekday_posts = weekday_posts.sort_values('post_count', ascending=False)
results['weekday_posts'] = weekday_posts.to_dict('records')
print("\nğŸ“† æ›œæ—¥åˆ¥æŠ•ç¨¿æ•° (TOP 3):")
for i, row in weekday_posts.head(3).iterrows():
    print(f"  {row['weekday_name']}: {row['post_count']:,} ä»¶")

# æ™‚é–“å¸¯åˆ¥æŠ•ç¨¿æ•°
hourly_posts = df.groupby('hour').size().reset_index(name='post_count')
hourly_posts = hourly_posts.sort_values('post_count', ascending=False)
results['hourly_posts'] = hourly_posts.to_dict('records')
print("\nğŸ• æ™‚é–“å¸¯åˆ¥æŠ•ç¨¿æ•° (TOP 5):")
for i, row in hourly_posts.head(5).iterrows():
    print(f"  {row['hour']:02d}æ™‚: {row['post_count']:,} ä»¶")
print()

print("=" * 80)
print("ğŸ’¬ 3. ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ")
print("=" * 80)

engagement_cols = ['like_count', 'repost_count', 'reply_count', 'quoted_count', 'total_engagement']
engagement_stats = df[engagement_cols].describe()
results['engagement_stats'] = {
    'ã„ã„ã­_å¹³å‡': float(df['like_count'].mean()),
    'ã„ã„ã­_ä¸­å¤®å€¤': float(df['like_count'].median()),
    'ã„ã„ã­_æœ€å¤§': int(df['like_count'].max()),
    'ãƒªãƒã‚¹ãƒˆ_å¹³å‡': float(df['repost_count'].mean()),
    'ãƒªãƒã‚¹ãƒˆ_ä¸­å¤®å€¤': float(df['repost_count'].median()),
    'è¿”ä¿¡_å¹³å‡': float(df['reply_count'].mean()),
    'è¿”ä¿¡_ä¸­å¤®å€¤': float(df['reply_count'].median()),
    'å¼•ç”¨_å¹³å‡': float(df['quoted_count'].mean()),
    'å¼•ç”¨_ä¸­å¤®å€¤': float(df['quoted_count'].median()),
    'ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ_å¹³å‡': float(df['total_engagement'].mean()),
    'ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ_ä¸­å¤®å€¤': float(df['total_engagement'].median()),
    'ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ_æœ€å¤§': int(df['total_engagement'].max()),
}

print("\nğŸ“Š ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆçµ±è¨ˆ:")
for key, value in results['engagement_stats'].items():
    if isinstance(value, float):
        print(f"  {key}: {value:.2f}")
    else:
        print(f"  {key}: {value:,}")

# ç›¸é–¢è¡Œåˆ—
engagement_corr = df[['like_count', 'repost_count', 'reply_count', 'quoted_count']].corr()
print("\nğŸ”— ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆç›¸é–¢:")
print(engagement_corr.to_string())

# ä¸Šä½ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿
top_engagement = df.nlargest(10, 'total_engagement')[
    ['created_at', 'user_name', 'content', 'like_count', 'repost_count', 'reply_count', 'quoted_count', 'total_engagement', 'has_media']
]
results['top_engagement_posts'] = top_engagement.to_dict('records')
print("\nğŸ† TOP 10 ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŠ•ç¨¿:")
for i, row in top_engagement.iterrows():
    # content ãŒ None/NaN ã®å ´åˆã«ã‚‚å®‰å…¨ã«ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”Ÿæˆ
    if pd.isna(row["content"]):
        content = ""
    else:
        content = str(row["content"])
    content_preview = content[:50] + "..." if len(content) > 50 else content
    print(f"  {i+1}. {row['user_name']}: {content_preview}")
    print(f"     ã„ã„ã­: {row['like_count']}, ãƒªãƒã‚¹ãƒˆ: {row['repost_count']}, è¿”ä¿¡: {row['reply_count']}, ç·è¨ˆ: {row['total_engagement']}")
print()

print("=" * 80)
print("ğŸ¬ 4. ãƒ¡ãƒ‡ã‚£ã‚¢åˆ†æ")
print("=" * 80)

posts_with_media = df['has_media'].sum()
posts_without_media = (~df['has_media']).sum()

media_comparison = df.groupby('has_media').agg({
    'post_id': 'count',
    'like_count': 'mean',
    'repost_count': 'mean',
    'reply_count': 'mean',
    'quoted_count': 'mean',
    'total_engagement': 'mean'
}).round(2)

media_eng_with = df[df['has_media']]['total_engagement'].mean()
media_eng_without = df[~df['has_media']]['total_engagement'].mean()
media_ratio = media_eng_with / media_eng_without if media_eng_without != 0 else float('nan')

results['media_stats'] = {
    'ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚ŠæŠ•ç¨¿æ•°': int(posts_with_media),
    'ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Šå‰²åˆ_%': float(posts_with_media / len(df) * 100),
    'ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—æŠ•ç¨¿æ•°': int(posts_without_media),
    'ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—å‰²åˆ_%': float(posts_without_media / len(df) * 100),
    'ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Š_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ': float(media_eng_with),
    'ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ': float(media_eng_without),
    'ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå€ç‡': float(media_ratio),
}

print("\nğŸ“Š ãƒ¡ãƒ‡ã‚£ã‚¢çµ±è¨ˆ:")
print(f"  ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Š: {posts_with_media:,} ä»¶ ({posts_with_media/len(df)*100:.1f}%)")
print(f"  ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—: {posts_without_media:,} ä»¶ ({posts_without_media/len(df)*100:.1f}%)")
print(f"\n  ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Šå¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {results['media_stats']['ãƒ¡ãƒ‡ã‚£ã‚¢ã‚ã‚Š_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ']:.2f}")
print(f"  ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {results['media_stats']['ãƒ¡ãƒ‡ã‚£ã‚¢ãªã—_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ']:.2f}")
print(f"  ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå€ç‡: {results['media_stats']['ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå€ç‡']:.2f}x")

print("\nğŸ¬ ãƒ¡ãƒ‡ã‚£ã‚¢æœ‰ç„¡åˆ¥ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆè©³ç´°:")
print(media_comparison.to_string())
print()

print("=" * 80)
print("ğŸ‘¥ 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ†æ")
print("=" * 80)

user_stats = df.groupby('user_id').agg({
    'post_id': 'count',
    'user_name': 'first',
    'user_badge': 'first',
    'like_count': 'sum',
    'repost_count': 'sum',
    'reply_count': 'sum',
    'quoted_count': 'sum',
    'total_engagement': 'sum'
}).reset_index()

user_stats.columns = ['user_id', 'post_count', 'user_name', 'user_badge', 'total_likes', 'total_reposts', 'total_replies', 'total_quotes', 'total_engagement']
user_stats = user_stats.sort_values('post_count', ascending=False)

badge_users = user_stats[user_stats['user_badge'] == True]
non_badge_users = user_stats[user_stats['user_badge'] != True]

results['user_stats'] = {
    'ç·ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°': int(len(user_stats)),
    'ãƒãƒƒã‚¸ä»˜ããƒ¦ãƒ¼ã‚¶ãƒ¼æ•°': int(len(badge_users)),
    'ãƒãƒƒã‚¸ä»˜ãå‰²åˆ_%': float(len(badge_users) / len(user_stats) * 100),
    'å¹³å‡æŠ•ç¨¿æ•°_ãƒ¦ãƒ¼ã‚¶ãƒ¼': float(user_stats['post_count'].mean()),
    'ä¸­å¤®å€¤æŠ•ç¨¿æ•°_ãƒ¦ãƒ¼ã‚¶ãƒ¼': float(user_stats['post_count'].median()),
    'æœ€å¤§æŠ•ç¨¿æ•°': int(user_stats['post_count'].max()),
    'æœ€å¤šæŠ•ç¨¿ãƒ¦ãƒ¼ã‚¶ãƒ¼': str(user_stats.iloc[0]['user_name']),
    'ãƒãƒƒã‚¸ã‚ã‚Š_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ_æŠ•ç¨¿': float(badge_users['total_engagement'].sum() / badge_users['post_count'].sum()) if len(badge_users) > 0 else 0,
    'ãƒãƒƒã‚¸ãªã—_å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ_æŠ•ç¨¿': float(non_badge_users['total_engagement'].sum() / non_badge_users['post_count'].sum()) if len(non_badge_users) > 0 else 0,
}

print("\nğŸ“Š ãƒ¦ãƒ¼ã‚¶ãƒ¼çµ±è¨ˆ:")
for key, value in results['user_stats'].items():
    if isinstance(value, float):
        print(f"  {key}: {value:.2f}")
    else:
        print(f"  {key}: {value}")

print("\nğŸ† TOP 10 æŠ•ç¨¿æ•°ãƒ¦ãƒ¼ã‚¶ãƒ¼:")
top_users = user_stats.head(10)
for i, row in top_users.iterrows():
    badge = "âœ“" if row['user_badge'] else ""
    print(f"  {row['user_name']}{badge}: {row['post_count']} ä»¶ (ç·ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {row['total_engagement']:,})")
print()

print("=" * 80)
print("ğŸ“ 6. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ†æ")
print("=" * 80)

results['content_stats'] = {
    'å¹³å‡æ–‡å­—æ•°': float(df['content_length'].mean()),
    'ä¸­å¤®å€¤æ–‡å­—æ•°': float(df['content_length'].median()),
    'æœ€å¤§æ–‡å­—æ•°': int(df['content_length'].max()),
    'æœ€å°æ–‡å­—æ•°': int(df['content_length'].min()),
    'URLå«æœ‰ç‡_%': float(df['has_url'].sum() / len(df) * 100),
}

print("\nğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·çµ±è¨ˆ:")
for key, value in results['content_stats'].items():
    if isinstance(value, float):
        print(f"  {key}: {value:.2f}")
    else:
        print(f"  {key}: {value}")

# ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°æŠ½å‡º
all_content = ' '.join(df['content'].fillna(''))
hashtags = re.findall(r'#\w+', all_content)
hashtag_freq = Counter(hashtags).most_common(20)
results['top_hashtags'] = [{'hashtag': h, 'count': c} for h, c in hashtag_freq]

print("\nğŸ·ï¸ TOP 20 ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°:")
for i, (tag, count) in enumerate(hashtag_freq, 1):
    print(f"  {i:2d}. {tag}: {count:,} å›")

# ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º
mentions = re.findall(r'@\w+', all_content)
mention_freq = Counter(mentions).most_common(20)
results['top_mentions'] = [{'mention': m, 'count': c} for m, c in mention_freq]

print("\nğŸ‘¤ TOP 20 ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³:")
for i, (mention, count) in enumerate(mention_freq, 1):
    print(f"  {i:2d}. {mention}: {count:,} å›")
print()

# çµæœã‚’JSONã§ä¿å­˜
print("=" * 80)
print("ğŸ’¾ çµæœä¿å­˜ä¸­...")
output_file = root_dir / "reports" / "sakurai_yui_eda_results.json"
output_file.parent.mkdir(parents=True, exist_ok=True)

with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2, default=str)

print(f"âœ… çµæœã‚’ä¿å­˜: {output_file}")
print()

print("=" * 80)
print("âœ… EDA åˆ†æå®Œäº†ï¼")
print("=" * 80)


import marimo

__generated_with = "0.17.8"
app = marimo.App(width="full")


@app.cell
def _():
    import marimo as mo
    return (mo,)


@app.cell
def _(mo):
    mo.md("""
    # ğŸ­ ã‚¢ã‚¤ãƒ‰ãƒ«æŠ•ç¨¿ã®æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ

    Xï¼ˆTwitterï¼‰æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡ºã—ã€ã‚¢ã‚¤ãƒ‰ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ä¾¡å€¤è¦³ã‚„æ„Ÿæƒ…æ§‹é€ ã®é•ã„ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚

    ## ğŸ“š å‚è€ƒ
    ã€Œãªãœä»¤å’Œã®ã‚¢ã‚¤ãƒ‰ãƒ«ã¯ã€è‡ªå·±è‚¯å®šæ„Ÿã€ã‚’æ­Œã†ã®ã‹ã€ï¼ˆå¾’ç„¶ç ”ç©¶å®¤ï¼‰ã®æ‰‹æ³•ã‚’æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã«å¿œç”¨

    ## ğŸ¯ åˆ†æã®æµã‚Œ
    1. **ãƒ‡ãƒ¼ã‚¿å–å¾—**: BigQueryã‹ã‚‰ç‰¹å®šæœŸé–“ã®æŠ•ç¨¿ã‚’å–å¾—
    2. **æ„Ÿæƒ…æŠ½å‡º**: Gemini APIã§æŠ•ç¨¿ã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡º
    3. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰**: æ„Ÿæƒ…ã®å…±èµ·é–¢ä¿‚ã‚’å¯è¦–åŒ–
    4. **æ¯”è¼ƒåˆ†æ**: ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®æ„Ÿæƒ…æ§‹é€ ã®é•ã„ã‚’åˆ†æ
    """)
    return


@app.cell
def _():
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    import seaborn as sns
    import networkx as nx
    from pyvis.network import Network
    from google import genai
    from google.genai import types
    from wordcloud import WordCloud
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    import json
    import os
    import sys
    from pathlib import Path
    from datetime import datetime, timedelta
    from collections import defaultdict, Counter
    import time
    from janome.tokenizer import Tokenizer
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
    project_root = Path.cwd().parent if Path.cwd().name == "notebooks" else Path.cwd()
    if str(project_root / "src") not in sys.path:
        sys.path.insert(0, str(project_root / "src"))
    
    from ai_data_lab.connectors.bigquery import BigQueryConnector
    
    # é«˜è§£åƒåº¦ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
    plt.rcParams['figure.dpi'] = 300
    plt.rcParams['savefig.dpi'] = 300
    sns.set_context("notebook", font_scale=1.2)
    sns.set_style("whitegrid")
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
    font_candidates = ['Hiragino Sans', 'Hiragino Kaku Gothic ProN', 'AppleGothic']
    for font in font_candidates:
        try:
            fm.findfont(font, fallback_to_default=False)
            plt.rcParams['font.family'] = font
            break
        except:
            continue
    
    # GOOGLE_APPLICATION_CREDENTIALS ãƒã‚§ãƒƒã‚¯
    if "GOOGLE_APPLICATION_CREDENTIALS" in os.environ:
        gac_path = os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
        if not os.path.exists(gac_path):
            print(f"âš ï¸ Credential file not found at {gac_path}. Removing env var to use ADC.")
            del os.environ["GOOGLE_APPLICATION_CREDENTIALS"]
    
    # Gemini APIè¨­å®šï¼ˆæ–°SDK: google-genaiï¼‰
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿: export GEMINI_API_KEY="your-api-key"
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "")
    if GEMINI_API_KEY:
        genai_client = genai.Client(api_key=GEMINI_API_KEY)
    else:
        genai_client = None
        print("âš ï¸ GEMINI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚APIæŠ½å‡ºæ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
    
    # æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼
    tokenizer = Tokenizer()
    
    return (
        BigQueryConnector,
        Counter,
        Network,
        WordCloud,
        datetime,
        defaultdict,
        genai_client,
        json,
        mo,
        nx,
        pd,
        plt,
        project_root,
        sns,
        time,
        timedelta,
        tokenizer,
        types,
    )


@app.cell
def _():
    # å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé™¤å¤–ãƒªã‚¹ãƒˆ
    EXCLUDED_HANDLES = [
        'FRUITS_ZIPPER', 'amane_fz1026', 'suzuka_fz1124', 'yui_fz0221',
        'luna_fz0703', 'manafy_fz0422', 'karen_fz0328', 'noel_fz1229',
        'CUTIE_STREET_', 'aika_cs1126', 'risa_cs1108', 'ayano_cs0526',
        'emiru_cs0422', 'kana_cs1111', 'haruka_cs0129', 'miyu_cs0913',
        'nagisa_cs0628', 'candy_tune_', 'mizuki_ct0221', 'rino_ct1224',
        'nachico_ct1001', 'natsu_ct0317', 'kotomi_ct0525', 'shizuka_ct0530',
        'bibian_ct1203', 'SWEET_STEADY', 'rise_ss0731', 'ayu_ss0107',
        'sakina_ss0229', 'nagisa_ss1029', 'natsuka_ss0719', 'mayumi_ss1227',
        'yui_ss0109', 'nogizaka46', 'takanenofficial', 'nao_kizuki',
        'hina_hinahata', 'Mikuru_hositani', 'erisahigasiyama', 'momonamatsumoto',
        'MomokoHashimoto', 'su_suzumi_', 'himeri_momiyama', 'saara_hazuki',
        'Equal_LOVE_12', 'otani_emiri', 'hana_oba', 'otoshima_risa',
        'saitou_kiara', 'sasaki_maika', 'takamatsuhitomi', 'shoko_takiwaki',
        'noguchi_iori', 'morohashi_sana', 'yamamoto_anna_'
    ]

    EXCLUDED_HANDLES_STR = ", ".join([f"'{h}'" for h in EXCLUDED_HANDLES])

    # æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒªå®šç¾©
    EMOTION_CATEGORIES = [
        "å¸Œæœ›", "ä¸å®‰", "æ„›æƒ…", "å–œã³", "æ‚²ã—ã¿", 
        "æ±ºæ„", "é€£å¸¯æ„Ÿ", "å­¤ç‹¬", "è‡ªå·±è‚¯å®š", "æ„Ÿè¬",
        "å¿œæ´", "æ†§ã‚Œ", "åˆ‡ãªã•", "èˆˆå¥®", "å¹³ç©"
    ]

    # æ„Ÿæƒ…ã®æ¥µæ€§ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ï¼‰
    EMOTION_POLARITY = {
        "å¸Œæœ›": 1, "æ„›æƒ…": 1, "å–œã³": 1, "æ±ºæ„": 1, "é€£å¸¯æ„Ÿ": 1,
        "è‡ªå·±è‚¯å®š": 1, "æ„Ÿè¬": 1, "å¿œæ´": 1, "æ†§ã‚Œ": 1, "èˆˆå¥®": 1, "å¹³ç©": 1,
        "ä¸å®‰": -1, "æ‚²ã—ã¿": -1, "å­¤ç‹¬": -1, "åˆ‡ãªã•": -1
    }

    # æ„Ÿæƒ…ã®è‰²è¨­å®š
    EMOTION_COLORS = {
        "å¸Œæœ›": "#FFD700", "ä¸å®‰": "#4169E1", "æ„›æƒ…": "#FF69B4", 
        "å–œã³": "#FFA500", "æ‚²ã—ã¿": "#6495ED", "æ±ºæ„": "#DC143C",
        "é€£å¸¯æ„Ÿ": "#32CD32", "å­¤ç‹¬": "#483D8B", "è‡ªå·±è‚¯å®š": "#FF1493",
        "æ„Ÿè¬": "#FFB6C1", "å¿œæ´": "#00CED1", "æ†§ã‚Œ": "#DDA0DD",
        "åˆ‡ãªã•": "#9370DB", "èˆˆå¥®": "#FF4500", "å¹³ç©": "#90EE90"
    }

    return (
        EMOTION_CATEGORIES,
        EMOTION_COLORS,
        EMOTION_POLARITY,
        EXCLUDED_HANDLES_STR,
    )


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ“¥ ãƒ‡ãƒ¼ã‚¿å–å¾—è¨­å®š
    """)
    return


@app.cell
def _(datetime, mo, timedelta):
    # æ—¥ä»˜é¸æŠUI
    default_date = datetime.now() - timedelta(days=7)

    date_selector = mo.ui.date(
        value=default_date.strftime('%Y-%m-%d'),
        label="åˆ†æå¯¾è±¡æ—¥: "
    )

    # æœŸé–“é¸æŠ
    period_selector = mo.ui.slider(
        start=1,
        stop=30,
        value=7,
        label="åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰: "
    )

    # ã‚µãƒ³ãƒ—ãƒ«æ•°é¸æŠ
    sample_size_selector = mo.ui.slider(
        start=100,
        stop=1000,
        value=300,
        step=100,
        label="ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ã‚µãƒ³ãƒ—ãƒ«æŠ•ç¨¿æ•°: "
    )

    mo.vstack([
        date_selector,
        period_selector,
        sample_size_selector
    ])
    return date_selector, period_selector, sample_size_selector


@app.cell
def _(
    BigQueryConnector,
    EXCLUDED_HANDLES_STR,
    date_selector,
    mo,
    pd,
    period_selector,
    sample_size_selector,
    timedelta,
):
    # BigQueryãƒ‡ãƒ¼ã‚¿å–å¾—

    if date_selector.value is None:
        mo.stop(True, mo.md("âš ï¸ æ—¥ä»˜ã‚’é¸æŠã—ã¦ãã ã•ã„"))

    selected_date = pd.to_datetime(date_selector.value)
    start_date = selected_date - timedelta(days=period_selector.value - 1)
    end_date = selected_date

    bq = BigQueryConnector(project_id="yoake-dev-analysis")
    DATASET_ID = "dev_yoake_posts"

    mo.md(f"""
    ### ğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...
    - **æœŸé–“**: {start_date.strftime('%Y-%m-%d')} ã€œ {end_date.strftime('%Y-%m-%d')}
    - **ã‚µãƒ³ãƒ—ãƒ«æ•°**: å„ã‚°ãƒ«ãƒ¼ãƒ—æœ€å¤§ {sample_size_selector.value} ä»¶
    """)

    # 1. TOP5ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®š
    query_top_groups = f"""
    WITH deduplicated AS (
        SELECT
            _TABLE_SUFFIX as idol_name,
            post.xPostId as xPostId,
            REGEXP_EXTRACT(post.xPostUrl, r'^https://x\\.com/([^/]+)/status') as handle,
            ROW_NUMBER() OVER (PARTITION BY post.xPostId ORDER BY _PARTITIONTIME DESC) as row_num
        FROM `{bq.project_id}.{DATASET_ID}.*`
        WHERE _TABLE_SUFFIX IS NOT NULL 
            AND _PARTITIONTIME IS NOT NULL
            AND DATE(TIMESTAMP_SECONDS(post.xPostCreatedAt)) BETWEEN '{start_date.strftime('%Y-%m-%d')}' AND '{end_date.strftime('%Y-%m-%d')}'
    ),
    base AS (
        SELECT * FROM deduplicated WHERE row_num = 1
    )
    SELECT
        idol_name,
        COUNT(DISTINCT xPostId) as post_count
    FROM base
    WHERE handle NOT IN ({EXCLUDED_HANDLES_STR})
    GROUP BY idol_name
    ORDER BY post_count DESC
    LIMIT 5
    """

    try:
        df_top_groups = bq.query(query_top_groups)
        top_groups = df_top_groups['idol_name'].tolist()

        mo.md(f"""
        âœ… **TOP 5 ã‚°ãƒ«ãƒ¼ãƒ—**: {', '.join(top_groups)}
        """)
    except Exception as e:
        mo.stop(True, mo.md(f"âŒ Query Error: {e}"))

    return DATASET_ID, bq, end_date, start_date, top_groups


@app.cell
def _(
    DATASET_ID,
    EXCLUDED_HANDLES_STR,
    bq,
    end_date,
    mo,
    pd,
    sample_size_selector,
    start_date,
    top_groups,
):
    # 2. å„ã‚°ãƒ«ãƒ¼ãƒ—ã®æŠ•ç¨¿ã‚’å–å¾—
    all_posts = []

    for group_fetch in top_groups:
        query_posts = f"""
        WITH deduplicated AS (
            SELECT
                '{group_fetch}' as idol_name,
                post.xPostId as xPostId,
                post.xPostContent as content,
                TIMESTAMP_SECONDS(post.xPostCreatedAt) as created_at,
                post.xPostLikedCount + post.xPostRepostedCount + post.xPostRepliedCount + post.xPostQuotedCount as total_engagement,
                user.xPostUserName as user_name,
                REGEXP_EXTRACT(post.xPostUrl, r'^https://x\\.com/([^/]+)/status') as handle,
                ROW_NUMBER() OVER (PARTITION BY post.xPostId ORDER BY _PARTITIONTIME DESC) as row_num
            FROM `{bq.project_id}.{DATASET_ID}.{group_fetch}`
            WHERE _PARTITIONTIME IS NOT NULL
                AND DATE(TIMESTAMP_SECONDS(post.xPostCreatedAt)) BETWEEN '{start_date.strftime('%Y-%m-%d')}' AND '{end_date.strftime('%Y-%m-%d')}'
                AND LENGTH(post.xPostContent) > 10
        ),
        base AS (
            SELECT * FROM deduplicated WHERE row_num = 1
        ),
        ranked AS (
            SELECT *,
                ROW_NUMBER() OVER (ORDER BY RAND()) as random_rank
            FROM base
            WHERE handle NOT IN ({EXCLUDED_HANDLES_STR})
        )
        SELECT *
        FROM ranked
        WHERE random_rank <= {sample_size_selector.value}
        """

        try:
            df_group_posts = bq.query(query_posts)
            all_posts.append(df_group_posts)
            print(f"âœ… {group_fetch}: {len(df_group_posts)} ä»¶å–å¾—")
        except Exception as e:
            print(f"âŒ {group_fetch} ã‚¨ãƒ©ãƒ¼: {e}")

    # å…¨æŠ•ç¨¿ã‚’çµåˆ
    df_all_posts = pd.concat(all_posts, ignore_index=True)

    mo.md(f"""
    ### âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†
    - **ç·æŠ•ç¨¿æ•°**: {len(df_all_posts):,} ä»¶
    - **ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥å†…è¨³**:
    {df_all_posts.groupby('idol_name').size().to_frame('æŠ•ç¨¿æ•°').to_markdown()}
    """)

    return (df_all_posts,)


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ¤– æ„Ÿæƒ…æŠ½å‡ºï¼ˆGemini APIï¼‰

    å„æŠ•ç¨¿ã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡ºã—ã¾ã™ã€‚å‡¦ç†ã«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
    """)
    return


@app.cell
def _(mo):
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆ
    use_cache_only = mo.ui.switch(value=True, label="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ä½¿ç”¨ï¼ˆAPIæŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    use_cache_only
    return (use_cache_only,)


@app.cell
def _(EMOTION_CATEGORIES, genai_client, json, mo, pd, project_root, time, use_cache_only):
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    cache_file = project_root / "data" / "emotion_cache.csv"
    cache_file.parent.mkdir(exist_ok=True)
    
    # ========================================
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã¿ä½¿ç”¨ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    # ========================================
    if use_cache_only.value:
        if not cache_file.exists():
            mo.stop(True, mo.md("âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: `data/emotion_cache.csv`"))
        
        df_emotions = pd.read_csv(cache_file)
        
        mo.md(f"""
        ### âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ
        
        - **æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ä»¶æ•°**: {len(df_emotions):,} ä»¶
        - **ãƒ¦ãƒ‹ãƒ¼ã‚¯æŠ•ç¨¿æ•°**: {df_emotions['xPostId'].nunique():,} ä»¶
        - **ã‚°ãƒ«ãƒ¼ãƒ—æ•°**: {df_emotions['idol_name'].nunique()} ã‚°ãƒ«ãƒ¼ãƒ—
        
        â€» æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹å ´åˆã¯ã€ä¸Šã®ã‚¹ã‚¤ãƒƒãƒã‚’OFFã«ã—ã¦ãã ã•ã„
        """)
    
    # ========================================
    # APIæŠ½å‡ºãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¹ã‚¤ãƒƒãƒOFFæ™‚ï¼‰
    # ========================================
    else:
        # æ„Ÿæƒ…æŠ½å‡ºé–¢æ•°ï¼ˆæ–°SDK: google-genai + gemini-2.5-flashï¼‰
        def extract_emotions_gemini(text, client):
            """Gemini APIã‚’ä½¿ç”¨ã—ã¦æŠ•ç¨¿ã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡º"""
            emotions_str = ", ".join([f'"{e}"' for e in EMOTION_CATEGORIES])
            prompt = f"""
            ä»¥ä¸‹ã®æŠ•ç¨¿ã‹ã‚‰æ„Ÿæƒ…ã‚’æœ€å¤§3ã¤ã¾ã§æŠ½å‡ºã—ã€ãã®å¼·åº¦(1-5)ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
            æ„Ÿæƒ…ã¯ä»¥ä¸‹ã®ãƒªã‚¹ãƒˆã‹ã‚‰é¸ã‚“ã§ãã ã•ã„: {emotions_str}

            æŠ•ç¨¿: {text}

            JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å›ç­”ã—ã¦ãã ã•ã„:
            [
                {{"emotion": "æ„Ÿæƒ…å", "strength": å¼·åº¦, "evidence": "æ ¹æ‹ ã¨ãªã‚‹éƒ¨åˆ†"}}
            ]

            æŠ•ç¨¿ãŒçŸ­ã™ãã‚‹å ´åˆã‚„æ„Ÿæƒ…ãŒèª­ã¿å–ã‚Œãªã„å ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆ[]ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
            """
            try:
                response = client.models.generate_content(
                    model='gemini-2.5-flash',
                    contents=prompt
                )
                response_text = response.text.strip()
                if response_text.startswith('```'):
                    lines = response_text.split('\n')
                    json_lines = [l for l in lines if not l.startswith('```')]
                    response_text = '\n'.join(json_lines)
                result = json.loads(response_text)
                return result
            except Exception as e:
                print(f"Error: {e}")
                return []
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®èª­ã¿è¾¼ã¿
        if cache_file.exists():
            df_cache = pd.read_csv(cache_file)
            cached_ids = set(df_cache['xPostId'].astype(str))
        else:
            df_cache = pd.DataFrame()
            cached_ids = set()
        
        # â€» df_all_posts ãŒå¿…è¦ãªå ´åˆã¯ã€BigQueryå–å¾—ã‚»ãƒ«ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„
        mo.stop(True, mo.md("""
        âš ï¸ **APIæŠ½å‡ºãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®æ‰‹é †ãŒå¿…è¦ã§ã™ï¼š**
        
        1. BigQueryãƒ‡ãƒ¼ã‚¿å–å¾—ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ `df_all_posts` ã‚’å–å¾—
        2. ã“ã®ã‚»ãƒ«ã‚’å†å®Ÿè¡Œ
        
        ã¾ãŸã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã—ã¦ãã ã•ã„ï¼ˆã‚¹ã‚¤ãƒƒãƒONï¼‰
        """))
        
        df_emotions = df_cache
    
    # æ„Ÿæƒ…æŠ½å‡ºçµæœã®ã‚µãƒãƒªãƒ¼
    emotion_summary = df_emotions.groupby(['idol_name', 'emotion']).size().unstack(fill_value=0)

    mo.md(f"""
    ### ğŸ­ æ„Ÿæƒ…æŠ½å‡ºçµæœ

    {emotion_summary.to_markdown()}
    """)

    return (df_emotions,)


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ•¸ï¸ æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰

    æ„Ÿæƒ…ã®å…±èµ·é–¢ä¿‚ã‹ã‚‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(
    Counter,
    EMOTION_COLORS,
    EMOTION_POLARITY,
    defaultdict,
    df_emotions,
    mo,
    nx,
):
    # æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰
    def build_emotion_network(df_group_emotions):
        """ã‚°ãƒ«ãƒ¼ãƒ—ã®æ„Ÿæƒ…ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰"""

        # æ„Ÿæƒ…ã®å…±èµ·ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        co_occurrence = defaultdict(int)
        emotion_counts = Counter()

        # æŠ•ç¨¿ã”ã¨ã«æ„Ÿæƒ…ã®çµ„ã¿åˆã‚ã›ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        for post_id in df_group_emotions['xPostId'].unique():
            post_emotions = df_group_emotions[df_group_emotions['xPostId'] == post_id]['emotion'].tolist()
            emotion_counts.update(post_emotions)

            # åŒã˜æŠ•ç¨¿å†…ã®æ„Ÿæƒ…ãƒšã‚¢ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            for i in range(len(post_emotions)):
                for j in range(i+1, len(post_emotions)):
                    pair = tuple(sorted([post_emotions[i], post_emotions[j]]))
                    co_occurrence[pair] += 1

        # NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
        G = nx.Graph()

        # ãƒãƒ¼ãƒ‰ï¼ˆæ„Ÿæƒ…ï¼‰ã‚’è¿½åŠ 
        for emotion, count in emotion_counts.items():
            G.add_node(emotion, 
                      size=count,
                      color=EMOTION_COLORS.get(emotion, '#808080'),
                      polarity=EMOTION_POLARITY.get(emotion, 0))

        # ã‚¨ãƒƒã‚¸ï¼ˆå…±èµ·é–¢ä¿‚ï¼‰ã‚’è¿½åŠ 
        for (e1, e2), weight in co_occurrence.items():
            if weight > 1:  # é–¾å€¤ã‚’è¨­å®š
                G.add_edge(e1, e2, weight=weight)

        return G, emotion_counts, co_occurrence

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰
    group_networks = {}

    for group_net in df_emotions['idol_name'].unique():
        df_group_net = df_emotions[df_emotions['idol_name'] == group_net]
        G_net, counts_net, co_occur_net = build_emotion_network(df_group_net)
        group_networks[group_net] = {
            'graph': G_net,
            'emotion_counts': counts_net,
            'co_occurrence': co_occur_net
        }

    mo.md(f"""
    ### âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰å®Œäº†

    å„ã‚°ãƒ«ãƒ¼ãƒ—ã®æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸï¼š
    - **ãƒãƒ¼ãƒ‰æ•°ï¼ˆæ„Ÿæƒ…ã®ç¨®é¡ï¼‰**: {', '.join([f"{g}: {len(data['graph'].nodes)}" for g, data in group_networks.items()])}
    - **ã‚¨ãƒƒã‚¸æ•°ï¼ˆå…±èµ·é–¢ä¿‚ï¼‰**: {', '.join([f"{g}: {len(data['graph'].edges)}" for g, data in group_networks.items()])}
    """)

    return (group_networks,)


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ“Š å¯è¦–åŒ–
    """)
    return


@app.cell
def _(group_networks, mo):
    # Pyvisã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¯è¦–åŒ–

    # ã‚°ãƒ«ãƒ¼ãƒ—é¸æŠUI
    group_selector = mo.ui.dropdown(
        options=list(group_networks.keys()),
        value=list(group_networks.keys())[0],
        label="ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é¸æŠ: "
    )

    group_selector
    return (group_selector,)


@app.cell
def _(Network, group_networks, group_selector, mo, project_root):
    # é¸æŠã•ã‚ŒãŸã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å¯è¦–åŒ–
    selected_group = group_selector.value

    if selected_group:
        network_data_sel = group_networks[selected_group]
        G_sel = network_data_sel['graph']

        # Pyvisãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä½œæˆ
        net = Network(height="600px", width="100%", bgcolor="#222222", font_color="white")
        net.from_nx(G_sel)

        # ãƒãƒ¼ãƒ‰ã®ã‚µã‚¤ã‚ºã¨è‰²ã‚’èª¿æ•´
        for node_sel in net.nodes:
            node_id_sel = node_sel['id']
            if node_id_sel in G_sel.nodes:
                node_sel['size'] = G_sel.nodes[node_id_sel]['size'] * 2
                node_sel['color'] = G_sel.nodes[node_id_sel]['color']
                node_sel['title'] = f"{node_id_sel}<br>å‡ºç¾å›æ•°: {G_sel.nodes[node_id_sel]['size']}"

        # ã‚¨ãƒƒã‚¸ã®å¤ªã•ã‚’èª¿æ•´
        for edge_sel in net.edges:
            # weightã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
            edge_weight = edge_sel.get('weight', 1)
            edge_sel['width'] = edge_weight * 0.5

        # ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
        net.set_options("""
        {
            "physics": {
                "enabled": true,
                "solver": "forceAtlas2Based",
                "forceAtlas2Based": {
                    "gravitationalConstant": -50,
                    "centralGravity": 0.01,
                    "springLength": 100,
                    "springConstant": 0.08
                }
            },
            "interaction": {
                "hover": true,
                "tooltipDelay": 100
            }
        }
        """)

        # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
        output_dir = project_root / "reports" / "visualizations"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"emotion_network_{selected_group}.html"
        net.save_graph(str(output_file))

        mo.md(f"""
        ### ğŸ•¸ï¸ {selected_group} ã®æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

        ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:
        - ğŸ“ ä¿å­˜å…ˆ: `{output_file.relative_to(project_root)}`
        - ğŸ¨ ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚º: æ„Ÿæƒ…ã®å‡ºç¾é »åº¦
        - ğŸ”— ã‚¨ãƒƒã‚¸ã®å¤ªã•: å…±èµ·é »åº¦

        â€» ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ãã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«æ“ä½œã§ãã¾ã™
        """)

    return


@app.cell
def _(group_networks, nx, plt, project_root):
    # é™çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å›³ï¼ˆå…¨ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒï¼‰
    fig_static, axes_static = plt.subplots(2, 3, figsize=(18, 12))
    axes_static = axes_static.flatten()

    for idx_static, (gname_static, ndata_static) in enumerate(group_networks.items()):
        if idx_static >= 6:
            break

        ax_static = axes_static[idx_static]
        G_static = ndata_static['graph']

        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨ˆç®—
        pos_static = nx.spring_layout(G_static, k=1, iterations=50)

        # ãƒãƒ¼ãƒ‰ã‚µã‚¤ã‚ºã®æ­£è¦åŒ–
        node_sizes_static = [G_static.nodes[n]['size'] * 50 for n in G_static.nodes()]
        node_colors_static = [G_static.nodes[n]['color'] for n in G_static.nodes()]

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æç”»
        nx.draw_networkx_nodes(G_static, pos_static, node_size=node_sizes_static, node_color=node_colors_static, ax=ax_static)
        nx.draw_networkx_labels(G_static, pos_static, font_size=8, font_family='Hiragino Sans', ax=ax_static)

        # ã‚¨ãƒƒã‚¸ã®æç”»
        edge_widths_static = [G_static[u][v].get('weight', 1) * 0.5 for u, v in G_static.edges()]
        nx.draw_networkx_edges(G_static, pos_static, width=edge_widths_static, alpha=0.5, ax=ax_static)

        ax_static.set_title(gname_static, fontsize=14, fontweight='bold')
        ax_static.axis('off')

    # æœªä½¿ç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for idx_unused_static in range(len(group_networks), 6):
        axes_static[idx_unused_static].axis('off')

    plt.suptitle('ã‚¢ã‚¤ãƒ‰ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¯”è¼ƒ', fontsize=20, y=0.98)
    plt.tight_layout()

    # ä¿å­˜
    save_path_static = project_root / "reports" / "visualizations" / "emotion_networks_comparison.png"
    plt.savefig(save_path_static, bbox_inches='tight')
    plt.show()
    return


@app.cell
def _(EMOTION_CATEGORIES, group_networks, mo, pd, plt, project_root, sns):
    # æ„Ÿæƒ…åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—

    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    emotion_matrix = []
    group_names_heat = []

    for gname_heat, ndata_heat in group_networks.items():
        group_names_heat.append(gname_heat)
        counts_heat = ndata_heat['emotion_counts']
        emotion_row_heat = {emotion: counts_heat.get(emotion, 0) for emotion in EMOTION_CATEGORIES}
        emotion_matrix.append(emotion_row_heat)

    df_heatmap = pd.DataFrame(emotion_matrix, index=group_names_heat)

    # æ­£è¦åŒ–ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®åˆè¨ˆã§å‰²ã‚‹ï¼‰
    df_heatmap_norm = df_heatmap.div(df_heatmap.sum(axis=1), axis=0) * 100

    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æç”»
    plt.figure(figsize=(12, 8))
    sns.heatmap(df_heatmap_norm, 
                annot=True, 
                fmt='.1f', 
                cmap='YlOrRd',
                cbar_kws={'label': 'æ„Ÿæƒ…ã®å‰²åˆ (%)'},
                xticklabels=EMOTION_CATEGORIES,
                yticklabels=group_names_heat)

    plt.title('ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ æ„Ÿæƒ…åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', fontsize=16, pad=20)
    plt.xlabel('æ„Ÿæƒ…ã‚«ãƒ†ã‚´ãƒª', fontsize=12)
    plt.ylabel('ã‚¢ã‚¤ãƒ‰ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—', fontsize=12)
    plt.xticks(rotation=45, ha='right')

    plt.tight_layout()
    save_path_heatmap = project_root / "reports" / "visualizations" / "emotion_distribution_heatmap.png"
    plt.savefig(save_path_heatmap)
    plt.show()

    # ç‰¹å¾´çš„ãªæ„Ÿæƒ…ã‚’æŠ½å‡º
    top_emotions_per_group = {}
    for gname_top in group_names_heat:
        top_3_heat = df_heatmap_norm.loc[gname_top].nlargest(3)
        top_emotions_per_group[gname_top] = list(top_3_heat.index)

    mo.md(f"""
    ### ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ ç‰¹å¾´çš„ãªæ„Ÿæƒ…TOP3

    {pd.DataFrame(top_emotions_per_group).T.to_markdown()}
    """)

    return df_heatmap_norm, top_emotions_per_group


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ“ˆ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†ææŒ‡æ¨™

    å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’æ•°å€¤çš„ã«åˆ†æã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(group_networks, mo, nx, pd):
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†ææŒ‡æ¨™ã®è¨ˆç®—
    network_metrics = []

    for gname_metrics, ndata_metrics in group_networks.items():
        G_metrics = ndata_metrics['graph']

        if len(G_metrics.nodes()) > 0 and len(G_metrics.edges()) > 0:
            # åŸºæœ¬æŒ‡æ¨™
            metrics_dict = {
                'ã‚°ãƒ«ãƒ¼ãƒ—': gname_metrics,
                'ãƒãƒ¼ãƒ‰æ•°': len(G_metrics.nodes()),
                'ã‚¨ãƒƒã‚¸æ•°': len(G_metrics.edges()),
                'å¯†åº¦': nx.density(G_metrics),
                'å¹³å‡æ¬¡æ•°': sum(dict(G_metrics.degree()).values()) / len(G_metrics.nodes()),
            }

            # ä¸­å¿ƒæ€§æŒ‡æ¨™
            if len(G_metrics.nodes()) > 1:
                degree_centrality_metrics = nx.degree_centrality(G_metrics)
                betweenness_centrality_metrics = nx.betweenness_centrality(G_metrics)

                # æœ€ã‚‚ä¸­å¿ƒçš„ãªæ„Ÿæƒ…
                top_degree_metrics = max(degree_centrality_metrics, key=degree_centrality_metrics.get)
                top_betweenness_metrics = max(betweenness_centrality_metrics, key=betweenness_centrality_metrics.get)

                metrics_dict['æœ€é«˜æ¬¡æ•°ä¸­å¿ƒæ€§'] = f"{top_degree_metrics} ({degree_centrality_metrics[top_degree_metrics]:.3f})"
                metrics_dict['æœ€é«˜åª’ä»‹ä¸­å¿ƒæ€§'] = f"{top_betweenness_metrics} ({betweenness_centrality_metrics[top_betweenness_metrics]:.3f})"

            # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°
            if len(G_metrics.edges()) > 0:
                metrics_dict['ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°'] = nx.average_clustering(G_metrics)

            network_metrics.append(metrics_dict)

    df_metrics = pd.DataFrame(network_metrics)

    mo.md(f"""
    ### ğŸ“Š ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ åˆ†æ

    {df_metrics.to_markdown(index=False)}

    **æŒ‡æ¨™ã®èª¬æ˜**:
    - **å¯†åº¦**: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµåˆåº¦ï¼ˆ0-1ã€é«˜ã„ã»ã©å¯†ï¼‰
    - **å¹³å‡æ¬¡æ•°**: å„æ„Ÿæƒ…ãŒå¹³å‡ä½•å€‹ã®ä»–ã®æ„Ÿæƒ…ã¨å…±èµ·ã™ã‚‹ã‹
    - **æ¬¡æ•°ä¸­å¿ƒæ€§**: å¤šãã®æ„Ÿæƒ…ã¨å…±èµ·ã™ã‚‹ä¸­å¿ƒçš„ãªæ„Ÿæƒ…
    - **åª’ä»‹ä¸­å¿ƒæ€§**: æ„Ÿæƒ…é–“ã®æ©‹æ¸¡ã—å½¹ã¨ãªã‚‹æ„Ÿæƒ…
    - **ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°**: æ„Ÿæƒ…ã®å±€æ‰€çš„ãªã¾ã¨ã¾ã‚Šåº¦
    """)

    return (df_metrics,)


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ¨ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ

    å„ã‚°ãƒ«ãƒ¼ãƒ—ã§ç‰¹å¾´çš„ãªæ„Ÿæƒ…ã‚’ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã§å¯è¦–åŒ–ã—ã¾ã™ã€‚
    """)
    return


@app.cell
def _(Counter, WordCloud, df_emotions, plt, project_root, tokenizer):
    # æŠ•ç¨¿ãƒˆãƒ”ãƒƒã‚¯ï¼ˆåè©ï¼‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆ
    
    # ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼ˆé™¤å¤–ã™ã‚‹å˜èªï¼‰
    stopwords_wc = {
        'ã“ã¨', 'ã‚‚ã®', 'ãŸã‚', 'ã‚ˆã†', 'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'ã“ã‚Œ', 'ãã‚Œ', 'ã‚ã‚Œ',
        'ã“ã“', 'ãã“', 'ã©ã“', 'ç§', 'åƒ•', 'ä¿º', 'è‡ªåˆ†', 'ä»Šæ—¥', 'æ˜æ—¥', 'æ˜¨æ—¥',
        'ä»Š', 'å¾Œ', 'å‰', 'ä¸­', 'ä¸Š', 'ä¸‹', 'æ–¹', 'äºº', 'æ™‚', 'æ—¥', 'å¹´', 'æœˆ',
        'ã®', 'ã«', 'ã¯', 'ã‚’', 'ãŒ', 'ã¨', 'ã§', 'ã‚‚', 'ãª', 'ã‚ˆ', 'ã­', 'ã‹',
        'RT', 'http', 'https', 'co', 't', 'amp', 'ç¬‘', 'w', 'ww', 'www',
        'æ„Ÿã˜', 'æ°—æŒã¡', 'æ°—', 'æ‰€', 'è¾º', 'ã¨ã“', 'ã¨ã“ã‚', 'çš„', 'ç³»', 'é¢¨',
    }
    
    def extract_nouns_wc(text_wc):
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡º"""
        if not text_wc or not isinstance(text_wc, str):
            return []
        nouns_wc = []
        for token_wc in tokenizer.tokenize(text_wc):
            # åè©ã®ã¿æŠ½å‡ºï¼ˆå›ºæœ‰åè©ã€ä¸€èˆ¬åè©ï¼‰
            if token_wc.part_of_speech.startswith('åè©'):
                word_wc = token_wc.surface
                # 1æ–‡å­—ã€ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã€æ•°å­—ã®ã¿ã¯é™¤å¤–
                if len(word_wc) > 1 and word_wc not in stopwords_wc and not word_wc.isdigit():
                    nouns_wc.append(word_wc)
        return nouns_wc
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡º
    group_topics = {}
    
    for group_wc in df_emotions['idol_name'].unique():
        # è©²å½“ã‚°ãƒ«ãƒ¼ãƒ—ã®evidenceãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        group_evidences = df_emotions[df_emotions['idol_name'] == group_wc]['evidence'].dropna().tolist()
        
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ã‚’æŠ½å‡º
        all_nouns_wc = []
        for evidence_wc in group_evidences:
            all_nouns_wc.extend(extract_nouns_wc(str(evidence_wc)))
        
        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        noun_counts_wc = Counter(all_nouns_wc)
        group_topics[group_wc] = noun_counts_wc
    
    # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰æç”»
    fig_wc, axes_wc = plt.subplots(2, 3, figsize=(18, 12))
    axes_wc = axes_wc.flatten()
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ï¼ˆMacç”¨ï¼‰
    font_path_wc = "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc"
    
    for idx_wc, (gname_wc, topic_counts_wc) in enumerate(group_topics.items()):
        if idx_wc >= 6:
            break
        
        ax_wc = axes_wc[idx_wc]
        
        # ä¸Šä½100å˜èªã§ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆ
        if topic_counts_wc:
            top_topics = dict(topic_counts_wc.most_common(100))
            wordcloud_wc = WordCloud(
                font_path=font_path_wc,
                width=400,
                height=300,
                background_color='white',
                colormap='plasma',
                relative_scaling=0.5,
                min_font_size=8,
                max_words=80
            ).generate_from_frequencies(top_topics)
            
            ax_wc.imshow(wordcloud_wc, interpolation='bilinear')
            ax_wc.set_title(gname_wc, fontsize=14, fontweight='bold')
            ax_wc.axis('off')
    
    # æœªä½¿ç”¨ã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    for idx_unused_wc in range(len(group_topics), 6):
        axes_wc[idx_unused_wc].axis('off')
    
    plt.suptitle('ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ æŠ•ç¨¿ãƒˆãƒ”ãƒƒã‚¯ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰', fontsize=20, y=0.98)
    plt.tight_layout()
    
    save_path_wc = project_root / "reports" / "visualizations" / "topic_wordclouds.png"
    plt.savefig(save_path_wc, bbox_inches='tight')
    plt.show()

    return (group_topics,)


@app.cell
def _(mo):
    mo.md("""
    ## ğŸ“ åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """)
    return


@app.cell
def _(
    df_emotions,
    df_heatmap_norm,
    df_metrics,
    end_date,
    mo,
    project_root,
    start_date,
    top_emotions_per_group,
):
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

    # å…¨ä½“çš„ãªå‚¾å‘ã‚’åˆ†æ
    overall_emotion_counts = df_emotions['emotion'].value_counts()
    top_overall_emotions = overall_emotion_counts.head(5).index.tolist()

    # è‡ªå·±è‚¯å®šæ„Ÿã®å‰²åˆã‚’è¨ˆç®—
    self_affirmation_ratio = {}
    for grp_report in df_heatmap_norm.index:
        if 'è‡ªå·±è‚¯å®š' in df_heatmap_norm.columns:
            self_affirmation_ratio[grp_report] = df_heatmap_norm.loc[grp_report, 'è‡ªå·±è‚¯å®š']

    # ãƒ¬ãƒãƒ¼ãƒˆMarkdownä½œæˆ
    report_content = f"""# ğŸ­ ã‚¢ã‚¤ãƒ‰ãƒ«æŠ•ç¨¿ã®æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

    **åˆ†ææœŸé–“**: {start_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ã€œ {end_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}

    ## ğŸ“Š 1. å…¨ä½“å‚¾å‘

    ### æœ€ã‚‚å¤šã„æ„Ÿæƒ…TOP5
    {', '.join([f"**{e}**" for e in top_overall_emotions])}

    ### ç·æŠ•ç¨¿æ•°ã¨æ„Ÿæƒ…æŠ½å‡ºçµæœ
    - åˆ†ææŠ•ç¨¿æ•°: {len(df_emotions['xPostId'].unique()):,} ä»¶
    - æŠ½å‡ºã•ã‚ŒãŸæ„Ÿæƒ…ç·æ•°: {len(df_emotions):,} ä»¶
    - å¹³å‡æ„Ÿæƒ…æ•°/æŠ•ç¨¿: {len(df_emotions) / len(df_emotions['xPostId'].unique()):.2f}

    ## ğŸ† 2. ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥åˆ†æ

    ### ç‰¹å¾´çš„ãªæ„Ÿæƒ…ãƒ‘ã‚¿ãƒ¼ãƒ³

    """

    # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ã‚’è¿½åŠ 
    for grp_pattern, top_emotions_pattern in top_emotions_per_group.items():
        self_aff_report = self_affirmation_ratio.get(grp_pattern, 0)

        # ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å¾´ã‚’åˆ¤å®š
        if 'è‡ªå·±è‚¯å®š' in top_emotions_pattern[:2]:
            pattern_type = "è‡ªå·±è‚¯å®šå‹ï¼ˆä»¤å’Œã®ã‚»ã‚«ã‚¤ç³»ï¼‰"
        elif 'é€£å¸¯æ„Ÿ' in top_emotions_pattern[:3] or 'å¿œæ´' in top_emotions_pattern[:3]:
            pattern_type = "é€£å¸¯ãƒ»å¿œæ´å‹ï¼ˆä¼çµ±çš„ã‚°ãƒ«ãƒ¼ãƒ—ã‚¢ã‚¤ãƒ‰ãƒ«ï¼‰"
        elif 'æ„›æƒ…' in top_emotions_pattern[:2]:
            pattern_type = "æ„›æƒ…è¡¨ç¾å‹ï¼ˆæ¨ã—æ´»ä¸­å¿ƒï¼‰"
        else:
            pattern_type = "è¤‡åˆå‹"

        report_content += f"""
    ### {grp_pattern}
    - **ãƒ‘ã‚¿ãƒ¼ãƒ³**: {pattern_type}
    - **ç‰¹å¾´çš„ãªæ„Ÿæƒ…**: {', '.join(top_emotions_pattern)}
    - **è‡ªå·±è‚¯å®šæ„Ÿã®å‰²åˆ**: {self_aff_report:.1f}%

    """

    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æçµæœã‚’è¿½åŠ 
    report_content += """
    ## ğŸ•¸ï¸ 3. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ åˆ†æ

    ### å¯†åº¦ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    """

    # å¯†åº¦é †ã«ã‚½ãƒ¼ãƒˆ
    df_metrics_sorted = df_metrics.sort_values('å¯†åº¦', ascending=False)

    for _, row_report in df_metrics_sorted.iterrows():
        report_content += f"""
    - **{row_report['ã‚°ãƒ«ãƒ¼ãƒ—']}**: å¯†åº¦ {row_report['å¯†åº¦']:.3f}, ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•° {row_report.get('ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¿‚æ•°', 0):.3f}
    """

    # è€ƒå¯Ÿã‚’è¿½åŠ 
    report_content += f"""

    ## ğŸ’­ 4. è€ƒå¯Ÿ

    ### ä»¤å’Œã‚¢ã‚¤ãƒ‰ãƒ«ã®ç‰¹å¾´
    1. **è‡ªå·±è‚¯å®šæ„Ÿã®å°é ­**: ç‰¹ã«æ–°ã—ã„ã‚°ãƒ«ãƒ¼ãƒ—ã§ã€Œè‡ªå·±è‚¯å®šã€ãŒä¸Šä½ã«æ¥ã‚‹å‚¾å‘
    2. **å€‹äººçš„ãªé–¢ä¿‚æ€§**: ã€Œã‚ãŸã—ã¨å›ã€çš„ãªè¦ªå¯†ãªæ„Ÿæƒ…è¡¨ç¾ãŒå¢—åŠ 
    3. **å¤šæ§˜ãªæ„Ÿæƒ…æ§‹é€ **: ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ç•°ãªã‚‹æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³

    ### SNSæ™‚ä»£ã®å½±éŸ¿
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãªåŒæ–¹å‘ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    - ãƒ•ã‚¡ãƒ³ã¨ã®è·é›¢æ„Ÿã®å¤‰åŒ–
    - å€‹äººã®å¹¸ç¦ã‚„å……å®Ÿæ„Ÿã‚’é‡è¦–ã™ã‚‹ä¾¡å€¤è¦³

    ## ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«

    - æ„Ÿæƒ…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼‰: `reports/visualizations/emotion_network_*.html`
    - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ¯”è¼ƒå›³: `reports/visualizations/emotion_networks_comparison.png`
    - æ„Ÿæƒ…åˆ†å¸ƒãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—: `reports/visualizations/emotion_distribution_heatmap.png`
    - ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰: `reports/visualizations/emotion_wordclouds.png`

    ---
    *Generated by Emotion Network Analysis System*
    """

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_path = project_root / "reports" / "emotion_network_analysis.md"
    report_path.write_text(report_content, encoding='utf-8')

    mo.md(f"""
    ### âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†

    ğŸ“ ä¿å­˜å…ˆ: `{report_path.relative_to(project_root)}`

    {report_content}
    """)

    return


@app.cell
def _(mo):
    mo.md("""
    ---
    ## ğŸ¯ ã¾ã¨ã‚

    ã“ã®åˆ†æã«ã‚ˆã‚Šã€ã‚¢ã‚¤ãƒ‰ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®æ„Ÿæƒ…æ§‹é€ ã®é•ã„ãŒå¯è¦–åŒ–ã•ã‚Œã¾ã—ãŸã€‚
    ç‰¹ã«ã€Œè‡ªå·±è‚¯å®šæ„Ÿã€ã‚’ä¸­å¿ƒã¨ã—ãŸä»¤å’Œå‹ã®æ„Ÿæƒ…ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã€
    ã€Œé€£å¸¯æ„Ÿã€ã€Œå¸Œæœ›ã€ã‚’ä¸­å¿ƒã¨ã—ãŸä¼çµ±çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…±å­˜ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

    ä»Šå¾Œã®åˆ†æã®ç™ºå±•å¯èƒ½æ€§ï¼š
    - æ™‚ç³»åˆ—ã§ã®æ„Ÿæƒ…å¤‰åŒ–ã®è¿½è·¡
    - ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆãƒ©ã‚¤ãƒ–ã€ãƒªãƒªãƒ¼ã‚¹ç­‰ï¼‰ã¨æ„Ÿæƒ…ã®ç›¸é–¢åˆ†æ
    - ãƒ•ã‚¡ãƒ³å±¤ã®é•ã„ã«ã‚ˆã‚‹æ„Ÿæƒ…ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒ
    """)
    return


if __name__ == "__main__":
    app.run()
